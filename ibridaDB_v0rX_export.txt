<codebase_context>
  <dirtree root="/home/caleb/repo/ibridaDB">
ibridaDB (10.9k/113.0k)
||-- CHANGELOG.md (45 lines/614 tokens)
||-- CLAUDE.md (287/2.8k)
||-- TODO.md (60/716)
||-- dbTools (5.7k/55.7k)
||   |-- FLOW.md (32/262)
||   |-- README.md (152/1.6k)
||   |-- dem (254/2.6k)
||   |   |-- download_merit.sh (85/1.1k)
||   |   \-- download_merit_parallel.sh (169/1.5k)
||   |-- export (3.2k/29.0k)
||   |   \-- v0 (3.2k/29.0k)
||   |       |-- common (1.5k/15.7k)
||   |       |   |-- clade_defns.sh (153/1.8k)
||   |       |   |-- clade_helpers.sh (321/3.6k)
||   |       |   |-- cladistic.sh (314/3.0k)
||   |       |   |-- functions.sh (64/441)
||   |       |   |-- main.sh (206/2.0k)
||   |       |   |-- region_defns.sh (70/774)
||   |       |   \-- regional_base.sh (401/3.9k)
||   |       \-- r1 (1.6k/13.2k)
||   |           |-- wrapper.sh (89/647)
||   |           |-- wrapper_amphibia_all_exc_nonrg_sp.sh (92/722)
||   |           |-- wrapper_amphibia_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/799)
||   |           |-- wrapper_amphibia_all_miniTest.sh (98/776)
||   |           |-- wrapper_angiospermae_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/779)
||   |           |-- wrapper_aves_all_exc_nonrg_sp.sh (96/759)
||   |           |-- wrapper_aves_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/771)
||   |           |-- wrapper_aves_reuse_all_sp.sh (98/791)
||   |           |-- wrapper_mammalia_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/775)
||   |           |-- wrapper_pta.sh (89/663)
||   |           |-- wrapper_pta_all_exc_nonrg_sp.sh (92/726)
||   |           |-- wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh (95/771)
||   |           |-- wrapper_pta_all_exc_nonrg_sp_inc_oor_fas_elev.sh (93/741)
||   |           |-- wrapper_pta_all_exc_nonrg_sp_inc_oor_fas_elev_mini.sh (93/742)
||   |           |-- wrapper_pta_non_rg.sh (93/737)
||   |           |-- wrapper_reptilia.sh (62/517)
||   |           |-- wrapper_reptilia_all_exc_nonrg_sp.sh (96/763)
||   |           \-- wrapper_reptilia_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/775)
||   |-- ingest (609/4.7k)
||   |   \-- v0 (609/4.7k)
||   |       |-- common (507/4.0k)
||   |       |   |-- functions.sh (42/273)
||   |       |   |-- geom.sh (50/366)
||   |       |   |-- main.sh (263/2.1k)
||   |       |   \-- vers_origin.sh (152/1.2k)
||   |       \-- r1 (102/723)
||   |           |-- structure.sql (49/283)
||   |           \-- wrapper.sh (53/440)
||   |-- schema.md (45/906)
||   \-- taxa (1.4k/16.5k)
||       |-- ColDP_raw_samples.txt (205/4.8k)
||       |-- __init__.py (1/0)
||       |-- analysis_utils.py (67/888)
||       |-- analyze_diff.py (244/2.6k)
||       |-- diffs (217/1.7k)
||       |   \-- May2024 (217/1.7k)
||       |       |-- L40_analysis.txt (94/776)
||       |       |-- L50_analysis.txt (107/885)
||       |       \-- L60_analysis.txt (16/116)
||       |-- expand (261/2.5k)
||       |   \-- expand_taxa.sh (261/2.5k)
||       |-- models (352/3.3k)
||       |   |-- __init__.py (19/116)
||       |   |-- coldp_models.py (106/945)
||       |   |-- expanded_taxa.py (112/1.0k)
||       |   \-- expanded_taxa_cmn.py (115/1.1k)
||       |-- reference.md (41/374)
||       \-- tools (23/179)
||           \-- extract_ColDP_samples.sh (23/179)
||-- dev (488/7.6k)
||   \-- ancestor_aware.md (488/7.6k)
||-- docker (21/198)
||   \-- stausee (21/198)
||       |-- entrypoint.sh (5/35)
||       \-- notes.md (16/163)
||-- docs (1.8k/19.9k)
||   |-- README.md (290/3.1k)
||   |-- coldp_integration.md (329/2.7k)
||   |-- export.md (251/3.3k)
||   |-- ingest.md (196/2.0k)
||   |-- roadmap.md (3/37)
||   |-- sample_data (156/1.8k)
||   |   |-- README.md (126/1.4k)
||   |   \-- generate_lca_sample.sql (30/357)
||   \-- schemas.md (670/6.8k)
||-- models (420/4.2k)
||   |-- __init__.py (21/126)
||   |-- base.py (3/17)
||   |-- coldp_models.py (144/1.2k)
||   |-- expanded_taxa.py (139/1.7k)
||   \-- expanded_taxa_cmn.py (113/1.1k)
||-- requirements.txt (19/171)
||-- scripts (2.0k/20.7k)
||   |-- __init__.py (1/4)
||   |-- add_immediate_ancestors.py (307/2.8k)
||   \-- ingest_coldp (1.7k/17.9k)
||       |-- __init__.py (2/13)
||       |-- load_tables.py (196/1.7k)
||       |-- map_taxa.py (461/4.9k)
||       |-- map_taxa_parallel.py (664/6.9k)
||       |-- populate_common_names.py (121/1.4k)
||       |-- snips.txt (4/58)
||       |-- wrapper_ingest_coldp.sh (120/1.2k)
||       \-- wrapper_ingest_coldp_parallel.sh (150/1.5k)
|\-- todo-scrap.py (15/178)
  </dirtree>
  <files>
    <file path="CHANGELOG.md">
# Catalog of Life Integration - CHANGELOG

This document tracks completed tasks and changes related to the Catalog of Life (ColDP) data integration project.

## [Unreleased] - 2025-05-24

### Added
- Created top-level `models/` directory to separate Python code from bash processing flows
- Created `scripts/ingest_coldp/` directory for ColDP integration scripts
- Moved existing model files from `dbTools/taxa/models/` to top-level `models/`
- Added base.py with SQLAlchemy Base class declaration
- Created initial script versions for:
  - `load_tables.py` - Loads ColDP TSV files into PostgreSQL tables
  - `map_taxa.py` - Maps iNaturalist taxa to ColDP taxa identifiers
  - `populate_common_names.py` - Updates commonName fields in expanded_taxa
  - `wrapper_ingest_coldp.sh` - Orchestrates the entire integration process
- Implemented fuzzy matching with the rapidfuzz library for improved taxon matching
- Added homonym resolution logic using taxonomic ancestors
- Added command-line options to control the fuzzy matching process
- Added TODO.md file to track remaining work
- Added CHANGELOG.md file to document changes
- Created `map_taxa_parallel.py` - Parallelized version of map_taxa.py for 10-12x speedup
- Created `wrapper_ingest_coldp_parallel.sh` - Parallelized wrapper with NUM_PROCESSES environment variable

### Changed
- Updated model import paths to use the new top-level models directory
- Enhanced ColdpNameUsage model to include taxonomic hierarchy fields
- Enhanced wrapper script with configuration options to enable/disable steps and fuzzy matching
- Improved mapping algorithm to prioritize accepted names from Catalog of Life
- Implemented batch processing for fuzzy matching to handle large datasets

### Fixed
- Fixed `populate_common_names.py` to handle NULL values in the `preferred` field of ColDP vernacular names
  - The ColDP data has `preferred` set to NULL for all English vernacular names
  - Updated WHERE clause from `cvn.preferred = TRUE` to `(cvn.preferred = TRUE OR cvn.preferred IS NULL)`
  - This allows the script to successfully populate common names even when the preferred flag isn't explicitly set
- Fixed memory exhaustion in `map_taxa_parallel.py` during fuzzy matching
  - Replaced full DataFrame copying to worker processes with lightweight lookup dictionaries
  - Changed from copying 5.2M ColDP records to each worker to sharing essential data only
  - Reduced batch size from 2000 to 1000 to manage memory usage
  - Implemented `resolve_homonyms_lightweight()` function that works with dictionaries instead of DataFrames
  - This reduced memory usage from ~99GB to manageable levels even with multiple parallel workers

## [Future Release]
This section will be populated as tasks are completed and ready for release.
    </file>
    <file path="CLAUDE.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

ibridaDB is a modular, reproducible database system designed to ingest, process, and export biodiversity observations from iNaturalist open data dumps. It leverages PostgreSQL with PostGIS to store and query geospatial data with specialized pipelines for:

- **Data Ingestion:** Importing CSV dumps, calculating geospatial geometries, updating metadata
- **Elevation Integration:** Enriching observations with elevation data from MERIT DEM tiles
- **Data Export:** Filtering observations by region and taxonomic clade, performing advanced ancestor searches, and exporting curated CSV files
- **Taxonomy Enrichment:** Integrating taxonomic data from Catalog of Life Data Package (ColDP) to add common names and additional taxonomic information

## Database Connection Information

The database runs in a Docker container. Here's how to connect to it:

```bash
# Database connection details
DB_USER=postgres
DB_PASSWORD=ooglyboogly69
DB_HOST=localhost
DB_PORT=5432
DB_NAME=ibrida-v0-r1

# Connect using psql through Docker
docker exec -it ibridaDB psql -U postgres -d ibrida-v0-r1

# Run SQL commands from outside
docker exec ibridaDB psql -U postgres -d ibrida-v0-r1 -c "SELECT COUNT(*) FROM observations"

# Backup a table
docker exec ibridaDB pg_dump -U postgres -d ibrida-v0-r1 -t observations > observations_backup.sql
```

## Key Concepts

- **Versioning:** The system uses dual versioning:
  - `VERSION_VALUE` (e.g., "v0"): Database structure/schema version
  - `RELEASE_VALUE` (e.g., "r1"): Data release identifier
  
- **Region & Clade Filtering:** The export pipeline allows filtering based on:
  - Geographic regions (defined by bounding boxes)
  - Taxonomic clades or metaclades
  - Minimum observation thresholds

## ColDP Integration

The Catalog of Life Data Package (ColDP) integration is a newer component that enriches the existing iNaturalist taxonomy with standardized taxonomy and common names from the Catalog of Life.

### ColDP Pipeline Structure

- **Scripts directory:** `/home/caleb/repo/ibridaDB/scripts/ingest_coldp/`
  - `load_tables.py` - Imports raw TSV files from ColDP into staging tables
  - `map_taxa.py` - Maps iNaturalist taxa to Catalog of Life taxa using exact and fuzzy matching
  - `map_taxa_parallel.py` - Parallelized version of map_taxa.py for faster processing
  - `populate_common_names.py` - Updates the expanded_taxa table with common names from ColDP
  - `wrapper_ingest_coldp.sh` - Orchestrates the entire ColDP integration process
  - `wrapper_ingest_coldp_parallel.sh` - Parallelized version of the wrapper

- **Model definitions:** `/home/caleb/repo/ibridaDB/models/coldp_models.py`
  - Contains SQLAlchemy ORM models for the ColDP tables
  - Includes the following tables:
    - `coldp_name_usage_staging` - Scientific names and taxonomic hierarchy
    - `coldp_vernacular_name` - Common names in different languages
    - `coldp_distribution` - Geographic distribution information
    - `coldp_media` - Media resources like images and sounds
    - `coldp_reference` - Bibliographic references
    - `coldp_type_material` - Type specimen information

- **Mapping table:** `inat_to_coldp_taxon_map`
  - Links iNaturalist taxa (via taxonID) to Catalog of Life taxa (via ID)
  - Stores match confidence and matching method

### Mapping Workflow

1. **Data Loading:** Import all ColDP TSV files into staging tables
2. **Exact Matching:** First attempt to match taxa based on scientific name and rank
3. **Name-Only Matching:** Try matching just on scientific name for remaining taxa
4. **Fuzzy Matching:** For remaining unmatched taxa, use fuzzy string matching
5. **Homonym Resolution:** When multiple fuzzy matches exist, use taxonomic hierarchy to resolve
6. **Common Name Population:** Update expanded_taxa.commonName and LXX_commonName fields

### Known Issues and Solutions

- **Schema Issues:** Several fields in the ColDP tables were originally defined with varchar lengths that are too small for actual data:
  1. **Taxon ID Fields:** The `taxonID` fields in various tables (like `coldp_vernacular_name.taxonID`) were defined as varchar(10) but some IDs are up to 64 characters (e.g., 'H-EzEkwxHee94KsK0nR3H0').
  2. **Reference Fields:** Fields in the `coldp_reference` table contain very long values that won't fit in varchar fields of any reasonable length.
  
  **Solution:** The wrapper script now:
  1. Drops all ColDP tables (but not expanded_taxa) before each run so they're recreated with the correct schema
  2. Uses SQLAlchemy models with:
     - `varchar(64)` for most ID fields in non-reference tables
     - `Text` type for all fields in the reference table (except the primary key) to avoid any length constraints
     - `varchar(255)` for the reference table's primary key
  3. Validates the schema to catch any field length issues before loading data

- **Performance Issue:** Fuzzy matching is computationally intensive. Use the parallelized version to speed up processing by 10-12x.

### Running ColDP Integration

```bash
# Standard sequential process
./scripts/ingest_coldp/wrapper_ingest_coldp.sh

# Parallelized process (much faster)
NUM_PROCESSES=12 ./scripts/ingest_coldp/wrapper_ingest_coldp_parallel.sh

# Skip certain steps if needed
DO_LOAD_TABLES=false DO_MAP_TAXA=true DO_POPULATE_COMMON_NAMES=false ./scripts/ingest_coldp/wrapper_ingest_coldp_parallel.sh
```

## Common Commands

### Docker Setup Commands

```bash
# Build the custom Docker image (with raster2pgsql support)
cd docker
docker build -t frontierkodiak/ibridadb:latest .

# Start a Docker container (using the appropriate compose file)
cd docker/stausee
docker-compose up -d
```

### Ingestion Commands

```bash
# Make scripts executable
chmod +x dbTools/ingest/v0/common/*.sh
chmod +x dbTools/ingest/v0/r1/wrapper.sh

# Run ingestion for release r1 (with elevation data)
ENABLE_ELEVATION=true dbTools/ingest/v0/r1/wrapper.sh

# Add elevation data to an existing database
cd dbTools/ingest/v0/utils/elevation
./wrapper.sh
```

### Elevation Data Download

```bash
# Download MERIT DEM tiles (sequential)
dbTools/dem/download_merit.sh

# Download MERIT DEM tiles (parallel)
dbTools/dem/download_merit_parallel.sh 4  # Using 4 parallel downloads
```

### Export Commands

```bash
# Make scripts executable
chmod +x dbTools/export/v0/common/*.sh
chmod +x dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh

# Run export for amphibians (with elevation data)
dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
```

## Architecture and Key Directories

### Ingestion Pipeline (`dbTools/ingest/v0/`)

- `common/` - Core ingestion logic:
  - `main.sh` - Orchestrates the entire ingestion process
  - `geom.sh` - Computes geospatial geometries
  - `vers_origin.sh` - Updates version and origin metadata

- `r0/`, `r1/`, etc. - Release-specific wrapper scripts and structures
  - `wrapper.sh` - Sets environment variables for a specific release
  - `structure.sql` - Database schema definition

- `utils/elevation/` - Elevation integration tools
  - `main.sh` - Orchestrates elevation data processing
  - `create_elevation_table.sh` - Creates the elevation_raster table
  - `load_dem.sh` - Loads MERIT DEM data into the database
  - `update_elevation.sh` - Updates observations with elevation values

### Export Pipeline (`dbTools/export/v0/`)

- `common/` - Core export logic:
  - `main.sh` - Orchestrates the export process
  - `regional_base.sh` - Creates region-filtered tables
  - `cladistic.sh` - Performs taxonomic filtering and CSV export
  - `clade_defns.sh` - Defines taxonomic clades and metaclades
  - `functions.sh` - Shared utility functions

- `r0/`, `r1/`, etc. - Release-specific export wrappers
  - Various wrapper scripts for different export configurations

### DEM Tools (`dbTools/dem/`)

- `download_merit.sh` - Downloads MERIT DEM tiles (sequential)
- `download_merit_parallel.sh` - Downloads MERIT DEM tiles (parallel)

### Documentation (`docs/`)

- `README.md` - High-level overview
- `export.md` - Export pipeline documentation
- `ingest.md` - Ingestion pipeline documentation
- `schemas.md` - Database schema reference
- `coldp_integration.md` - ColDP integration documentation

## Critical Environment Variables

### For Ingestion:

- `DB_USER` - PostgreSQL user name
- `VERSION_VALUE` - Database version identifier
- `RELEASE_VALUE` - Data release identifier
- `ORIGIN_VALUE` - Data source identifier
- `DB_NAME` - Name of the database to create
- `SOURCE` - Source data identifier
- `METADATA_PATH` - Path to CSV files
- `ENABLE_ELEVATION` - Toggle elevation data integration
- `DEM_DIR` - Path to DEM data

### For Export:

- Same database configuration variables as ingestion
- `REGION_TAG` - Identifies the geographic region
- `MIN_OBS` - Minimum number of observations per species
- `CLADE`/`METACLADE` - Taxonomic filter
- `MAX_RN` - Maximum observations per species in export
- `INCLUDE_OUT_OF_REGION_OBS` - Toggle inclusion of observations outside region
- `INCLUDE_ELEVATION_EXPORT` - Toggle inclusion of elevation data in exports
- `EXPORT_GROUP` - Name identifier for the export job

### For ColDP Integration:

- Database configuration variables (DB_USER, DB_PASSWORD, etc.)
- `ENABLE_FUZZY_MATCH` - Enable fuzzy matching (default: true)
- `FUZZY_THRESHOLD` - Minimum score for fuzzy matches (default: 90)
- `NUM_PROCESSES` - Number of parallel processes for fuzzy matching
- `COLDP_DIR` - Path to the ColDP data directory
- `DO_LOAD_TABLES` - Whether to load ColDP tables
- `DO_MAP_TAXA` - Whether to map taxa
- `DO_POPULATE_COMMON_NAMES` - Whether to populate common names

## Testing

As this is primarily a data processing system, there are no formal unit tests. Instead, verify that:

1. The database is created with all required tables and indexes
2. CSV files are successfully imported into tables
3. The `geom` column is correctly computed and indexed
4. If elevation was enabled, the `observations.elevation_meters` column is populated
5. Export files contain the expected number of observations and follow the required schema

## Common Workflows

1. **Basic ingestion workflow:**
   - Configure wrapper script with appropriate environment variables
   - Run ingestion wrapper
   - Verify database creation and data loading

2. **Adding elevation to existing database:**
   - Configure elevation wrapper script
   - Run elevation wrapper
   - Verify population of elevation_meters column

3. **Export workflow:**
   - Create or modify export wrapper with desired filtering parameters
   - Run export wrapper
   - Check resulting CSV files and export summary in the output directory

4. **Creating a new release:**
   - Create new release directories (`r2/` within ingest and export)
   - Copy and update wrapper scripts with new release info
   - Run the updated wrappers

5. **ColDP integration workflow:**
   - Ensure model schema is correct (especially field lengths)
   - Run parallelized wrapper script
   - Verify mappings and common name population

## Notes and Considerations

- This system is designed to be reproducible - the same inputs should always produce the same database
- The export pipeline supports ancestor-aware logic and partial-labeled data (see documentation)
- When using elevation data, ensure you're using the custom Docker image with `raster2pgsql` support
- Export generates detailed summaries alongside CSV files
- Config follows the single responsibility principle - wrappers should focus on one specific task
- The ColDP fuzzy matching process is highly CPU-intensive but can be parallelized effectively
    </file>
    <file path="TODO.md">
# Catalog of Life Integration - TODO List

## Directory Structure and Setup
- [x] Create top-level `models/` directory
- [x] Create `scripts/ingest_coldp/` directory
- [x] Move model files from `dbTools/taxa/models/` to top-level `models/`
- [x] Create base SQLAlchemy models

## Phase 1: Load ColDP Data
- [x] Create `load_tables.py` to import ColDP data
- [x] Implement table creation logic for all ColDP entities
- [x] Implement TSV parsing and database loading logic

## Phase 2: Mapping and Populating
- [x] Create `map_taxa.py` with exact name+rank matching
- [x] Create `map_taxa.py` with exact name-only matching
- [x] Implement fuzzy matching logic in `map_taxa.py`
  - [x] Integrate the rapidfuzz library for Levenshtein distance calculations
  - [x] Tune the match threshold to minimize false positives
  - [x] Add robust homonym resolution by checking ancestor ranks
  - [x] Add debug mode for monitoring fuzzy match quality
- [x] Update `ColdpNameUsage` model to include taxonomic hierarchy fields
- [x] Create `populate_common_names.py` for direct taxa common names
- [x] Implement logic for populating ancestor taxa common names
- [ ] Verify SQL update statements work with the actual database schema

## Phase 3: Wrapper Script and Documentation
- [x] Create `wrapper_ingest_coldp.sh` script
- [x] Add error handling for missing data files and database errors
- [x] Add options to bypass specific steps when re-running
- [ ] Update `docs/schemas.md` with ColDP table definitions
- [ ] Create new `docs/coldp_integration.md` documentation

## Testing and Validation
- [ ] Test load_tables.py with sample ColDP data
- [ ] Test map_taxa.py with a small subset of taxa to verify matching quality
- [ ] Test fuzzy matching algorithm with different threshold values
- [ ] Test populate_common_names.py to ensure proper SQL updates
- [ ] Run full end-to-end test with wrapper_ingest_coldp.sh
- [ ] Verify mapping quality with a sample of 100 species
- [ ] Check for missing common names in popular taxa

## SDK Development Support
- [ ] Create script to dump all database table schemas to text file
  - Use PostgreSQL information_schema or psql \d+ commands
  - Include all table structures, constraints, indexes, and relationships
  - Output format suitable for SDK code generation
  - Consider creating SQL DDL export and JSON schema formats
  - Include documentation about each table's purpose
- [ ] Document database access patterns and common queries
- [ ] Create sample queries for SDK development

## Future Work
- [ ] Further optimize fuzzy matching algorithm performance for large datasets
- [ ] Develop an iterative/multi-pass fuzzy matching process for improved quality
- [ ] Add support for more ColDP entities (SpeciesInteraction, etc.)
- [ ] Integrate with Alembic migrations for schema management
- [ ] Add incremental update logic for new ColDP releases
- [ ] Create visualization/monitoring tools for match quality
- [ ] Add automatic validation and crosscheck against taxonomic authorities
    </file>
    <file path="requirements.txt">
async-timeout==4.0.3
cffi==1.16.0
cryptography==42.0.8
greenlet==3.0.3
numpy==1.26.4
pandas==2.2.2
psycopg2==2.9.9
psycopg2-binary==2.9.9
pycparser==2.22
PyMySQL==1.1.1
python-dateutil==2.9.0.post0
pytz==2024.1
redis==5.0.4
six==1.16.0
SQLAlchemy==2.0.30
tqdm==4.66.4
typing_extensions==4.11.0
tzdata==2024.1

    </file>
    <file path="todo-scrap.py">
# BUG: Memory leak detected in cache clearing function
# CLARIFY: Need to specify exact behavior for edge cases in user authentication
# HACK: Temporary workaround for API rate limiting
# FIXME: Database connection timeout needs to be increased
# TODO: Implement password reset functionality
# NOTE: JWT tokens expire after 24 hours
# [ ] Add input validation for email addresses
# [x] Implement basic user registration
# FUTURE: Add support for OAuth 2.0 providers
# REVIEW: Security measures for file upload system
# REVIEWED: Authentication middleware passes all test cases
# MIGRATE: Update from Django 3.2 to 4.2
# REFACTOR: Split utility functions into separate modules
# OPTIMIZE: Cache frequently accessed user preferences
# DEPRECATED: Old API endpoints will be removed in v2.0
    </file>
    <dir path="dbTools">
      <file path="dbTools/FLOW.md">
```mermaid
flowchart TB

    subgraph Ingest["Database Initialization (ingest/)"]
        i_wrap["Ingest Wrapper<br/>(e.g. r0/wrapper.sh)"]
        i_main["Ingest Main<br/>(common/main.sh)"]
        i_other["Other Common Scripts"]
        db["(ibridaDB PostgreSQL)"]
        i_wrap --> i_main
        i_main --> i_other
        i_other --> db
    end

    subgraph Export["Data Export (export/)"]
        e_wrap["Export Wrapper<br/>(e.g. r1/my_wrapper.sh)"]
        e_main["Export Main<br/>(common/main.sh)"]
        rbase["regional_base.sh<br/>Species + Ancestors"]
        clad["cladistic.sh<br/>RG_FILTER_MODE + partial-labeled"]
        csv_out["CSV + Summary Files"]
        e_wrap --> e_main
        e_main --> rbase
        rbase --> clad
        clad --> csv_out
    end

    i_other --> db
    db --> e_wrap

    style Ingest fill:#f9f,stroke:#333,stroke-width:2px
    style Export fill:#bbf,stroke:#333,stroke-width:2px

```
      </file>
      <file path="dbTools/README.md">
# ibrida Database Reproduction Guide

## Overview
This guide documents the end-to-end process for **reproducing** and **exporting** from the ibrida database, which is derived from iNaturalist open data dumps. The database uses a versioning system with two components:
- **Version (v#)**: Indicates structural changes (schema revisions) to the database.
- **Release (r#)**: Indicates distinct data releases from iNaturalist under the same schema version.

For example:
- **v0r0**: June 2024 iNat data release
- **v0r1**: December 2024 iNat data release (adds `anomaly_score` column to `observations`)

## System Architecture
The pipeline is split into two phases:
1. **Database Initialization** (`ingest/`)
2. **Data Export** (`export/`)

Each phase has:
- Common scripts for shared logic
- Release- or job-specific *wrapper scripts* that set environment variables for that particular run

## 1. Database Initialization (ingest/)
### Directory Structure

```
dbTools/ingest/v0/
├── common/
│   ├── geom.sh         # Geometry calculations
│   ├── vers_origin.sh  # Version/origin updates
│   └── main.sh         # Core ingestion logic
├── r0/
│   ├── wrapper.sh      # June 2024 release
│   └── structure.sql   # schema for r0
└── r1/
    ├── wrapper.sh      # December 2024 release
    └── structure.sql   # schema for r1 (adds anomaly_score)
```

### Running the Ingestion Process
1. **Make scripts executable**:
    ```bash
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/*.sh
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r0/wrapper.sh
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
    ```
2. **Run**:
    ```bash
    # For June 2024 (r0)
    /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r0/wrapper.sh

    # For December 2024 (r1)
    /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
    ```

## 2. Data Export (export/)
The export pipeline allows flexible subsetting of the DB by region, minimum threshold, clade, etc. For additional detail, see [export.md](export.md).

### Directory Structure
```
dbTools/export/v0/
├── common/
│   ├── main.sh            # Orchestrates creation or skipping of base tables; final summary
│   ├── regional_base.sh   # Region-based table creation, ancestor-aware logic
│   ├── cladistic.sh       # Taxonomic filtering, partial-rank wiping, CSV export
│   └── functions.sh       # Shared shell functions
├── r0/
│   └── wrapper.sh         # Example job wrapper for June 2024 export
├── r1/
│   └── wrapper.sh         # Example job wrapper for December 2024 export
└── export.md              # Detailed usage documentation (v1)
```

### Export Workflow
1. **User creates/edits a wrapper script** (e.g., `r1/my_special_wrapper.sh`) to set:
   - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`
   - Optional toggles like `INCLUDE_OUT_OF_REGION_OBS`, `RG_FILTER_MODE`, `ANCESTOR_ROOT_RANKLEVEL`, `MIN_OCCURRENCES_PER_RANK`
   - A unique `EXPORT_GROUP` name
2. **Run** that wrapper. The pipeline will:
   1. **(regional_base.sh)** Build base tables of in-threshold species + ancestors, optionally bounding to region or not, depending on `INCLUDE_OUT_OF_REGION_OBS`.
   2. **(cladistic.sh)** Filter final observations by clade or metaclade, optionally wipe partial ranks, and do a random-sample CSV export.
   3. **(main.sh)** Write a summary file enumerating environment variables, row counts, timing, etc.

3. **Check** `/datasets/ibrida-data/exports` for final CSV output (organized by `VERSION_VALUE` / `RELEASE_VALUE` / any job-specific subdirectory).

### Drafting a New Wrapper
It is **recommended** to create a separate wrapper script for each new export job. For instance:
```bash
#!/bin/bash

export WRAPPER_PATH="$0"

export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_test"

export INCLUDE_OUT_OF_REGION_OBS=false
export RG_FILTER_MODE="ALL"
export ANCESTOR_ROOT_RANKLEVEL=40
export MIN_OCCURRENCES_PER_RANK=30

# other optional vars, e.g. PROCESS_OTHER, SKIP_REGIONAL_BASE, etc.

export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/myamphibia_job"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

source "${BASE_DIR}/common/functions.sh"

/home/caleb/repo/ibridaDB/dbTools/export/v0/common/main.sh
```
Then `chmod +x` this file and run it to generate a new job.

### Example Outputs
The final CSV and summary are placed in a subdirectory (e.g. `v0/r1/myamphibia_job`). A typical summary file `amphibia_test_export_summary.txt` includes:
- Region: NAfull
- MIN_OBS: 50
- RG_FILTER_MODE: ALL
- Observations: 10,402
- Unique Taxa: 927
- Timings for each step

### Further Reading
- **[export.md](export/v0/export.md)** for a deeper parameter reference (v1).
- **clade_defns.sh** for built-in definitions of macroclades, clades, and metaclades.

## Overall Flow
Below is a schematic of the entire ingest→export pipeline. For details on the ingest side, see [Ingestion docs](#database-initialization-ingest):
```
Ingest (ingest/v0/) --> Database --> Export (export/v0/)
```
In the export sub-phase, each new wrapper script can define a distinct job. Summaries and CSVs are stored in `HOST_EXPORT_BASE_PATH` for easy retrieval and analysis.

## Notes on Schema
- **v0r1** adds the `anomaly_score numeric(15,6)` column to `observations`.
- The export scripts automatically check if that column is present based on `RELEASE_VALUE`.
- If partial-labeled data is desired (coarse ranks for rare species), see the advanced features in `regional_base.sh` (ancestor logic) and `cladistic.sh` (partial-rank wiping logic).

**Notes**:
- The ingest side is unchanged for v0→v0r1 except for adding columns and data updates.
- The export side is significantly more flexible now, supporting ancestor‐aware logic and partial-labeled data.  
- Each new export job typically has its own wrapper script referencing the relevant `VERSION_VALUE`, `RELEASE_VALUE`, region, and clade parameters.
      </file>
      <file path="dbTools/schema.md">
```markdown
### Observations
Column | Description
-------|------------
observation_uuid | A unique identifier associated with each observation also available at iNaturalist.org via URLs constructed like this https://www.inaturalist.org/observations/c075c500-b566-44aa-847c-95da8fb8b3c9
observer_id | The identifier of the associated iNaturalist user who recorded the observation
latitude | The latitude where the organism was encountered
longitude | The longitude where the organism was encountered
positional_accuracy | The uncertainty in meters around the latitude and longitude
taxon_id | The identifier of the associated axon the observation has been identified as
quality_grade | `Casual` observations are missing certain data components (e.g. latitude) or may have flags associated with them not shown here (e.g. `location appears incorrect`). Observations flagged as not wild are also considered Casual. All other observations are either `Needs ID` or `Research Grade`. Generally, Research Grade observations have more than one agreeing identifications at the species level, or if there are disagreements at least ⅔ of the identifications are in agreement a the species level
observed_on | The date at which the observation took place
<NOTE> New column added in v0/r1 'anomaly_score' </NOTE>

### Observers
Column | Description
-------|------------
observer_id | A unique identifier associated with each observer also available on https://www.inaturalist.org via URLs constructed like this: https://www.inaturalist.org/users/1
login | A unique login associated with each observer
name | Personal name of the observer, if provided

### Photos
Column | Description
-------|------------
photo_uuid | A unique identifier associated with each photo. Note that photo_uuid can be non-unique across different observations.
photo_id | A photo identifier used on iNaturalist and available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/photos/113756411
observation_uuid | The identifier of the associated observation
observer_id | The identifier of the associated observer who took the photo
extension | The image file format, e.g. `jpeg`
license | All photos in the dataset have open licenses (e.g. Creative Commons) and unlicensed (CC0 / public domain)
width | The width of the photo in pixels
height | The height of the photo in pixels
position | When observations have multiple photos the user can set the position in which the photos should appear. Lower numbers are meant to appear first
>The issue is that some observations include more than one photo, and photos associated with observations that have >1 photo share a photo_id and photo_uuid, which I did not expect. These additional photos (which have their own rows in the 'photos' table) are denoted by the 'position' field, where position ==0 indicates that the photo is the primary photo for the record. If an observation only has one photo, then the associated 'photos' record will have position == 0. Therefore. I'm pretty sure that a composite key of photo_id ++ photo_uuid ++ position will function as a primary key. 

### Taxa
Column | Description
-------|------------
taxon_id | A unique identifier associated with each node in the iNaturalist taxonomy hierarchy. Also available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/taxa/47219
ancestry | The taxon_ids of ancestry of the taxon ordered from the root of the tree to the taxon concatenated together with `\`
rank_level | A number associated with the rank. Taxon rank_levels must be less than the rank level of their parent. For example, a taxon with rank genus and rank_level 20 cannot descend from a taxon of rank species and rank_level 10
rank | A constrained set of labels associated with nodes on the hierarchy. These include the standard Linnaean ranks: Kingdom, Phylum, Class, Order, Family, Genus, Species, and a number of internodes such as Subfamily
name | The scientific name for the taxon
active | When the taxonomy changes, generally taxa aren’t deleted on iNaturalist to avoid breaking links. Instead taxa are made inactive and observations are moved to new active nodes. Occasionally, observations linger on inactive taxa which are no longer active parts of the iNaturalist taxonomy
```
      </file>
      <dir path="dbTools/dem">
        <file path="dbTools/dem/download_merit.sh">
#!/bin/bash

# Configuration
USERNAME="hydrography"
PASSWORD="rivernetwork"
BASE_URL="http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_Hydro/distribute/v1.0"
DEST_DIR="/datasets/dem/merit"
LOG_FILE="${DEST_DIR}/download.log"

# Create destination directory
mkdir -p "${DEST_DIR}"

# Initialize log file
echo "Starting MERIT DEM download at $(date)" > "${LOG_FILE}"

# Function to download a single file with retry logic
download_file() {
    local filename="$1"
    local attempts=3
    local wait_time=30

    for ((i=1; i<=attempts; i++)); do
        echo "Downloading ${filename} (attempt ${i}/${attempts})..." | tee -a "${LOG_FILE}"
        
        if wget --quiet --user="${USERNAME}" \
                --password="${PASSWORD}" \
                --no-check-certificate \
                -P "${DEST_DIR}" \
                "${BASE_URL}/${filename}"; then
            echo "Successfully downloaded ${filename}" | tee -a "${LOG_FILE}"
            return 0
        else
            echo "Failed to download ${filename} on attempt ${i}" | tee -a "${LOG_FILE}"
            if [ $i -lt $attempts ]; then
                echo "Waiting ${wait_time} seconds before retry..." | tee -a "${LOG_FILE}"
                sleep ${wait_time}
            fi
        fi
    done
    
    echo "Failed to download ${filename} after ${attempts} attempts" | tee -a "${LOG_FILE}"
    return 1
}

# List of all valid files to download (excluding "no data" regions)
declare -a files=(
    # N60-N90
    "elv_n60w180.tar" "elv_n60w150.tar" "elv_n60w120.tar" "elv_n60w090.tar" "elv_n60w060.tar" "elv_n60w030.tar"
    "elv_n60e000.tar" "elv_n60e030.tar" "elv_n60e060.tar" "elv_n60e090.tar" "elv_n60e120.tar" "elv_n60e150.tar"
    # N30-N60
    "elv_n30w180.tar" "elv_n30w150.tar" "elv_n30w120.tar" "elv_n30w090.tar" "elv_n30w060.tar" "elv_n30w030.tar"
    "elv_n30e000.tar" "elv_n30e030.tar" "elv_n30e060.tar" "elv_n30e090.tar" "elv_n30e120.tar" "elv_n30e150.tar"
    # N00-N30
    "elv_n00w180.tar" "elv_n00w120.tar" "elv_n00w090.tar" "elv_n00w060.tar" "elv_n00w030.tar"
    "elv_n00e000.tar" "elv_n00e030.tar" "elv_n00e060.tar" "elv_n00e090.tar" "elv_n00e120.tar" "elv_n00e150.tar"
    # S30-N00
    "elv_s30w180.tar" "elv_s30w150.tar" "elv_s30w120.tar" "elv_s30w090.tar" "elv_s30w060.tar" "elv_s30w030.tar"
    "elv_s30e000.tar" "elv_s30e030.tar" "elv_s30e060.tar" "elv_s30e090.tar" "elv_s30e120.tar" "elv_s30e150.tar"
    # S60-S30
    "elv_s60w180.tar" "elv_s60w090.tar" "elv_s60w060.tar" "elv_s60w030.tar"
    "elv_s60e000.tar" "elv_s60e030.tar" "elv_s60e060.tar" "elv_s60e090.tar" "elv_s60e120.tar" "elv_s60e150.tar"
)

# Download all files
failed_files=()
for file in "${files[@]}"; do
    if ! download_file "${file}"; then
        failed_files+=("${file}")
    fi
    # Small delay between downloads
    sleep 2
done

# Report results
echo -e "\nDownload Summary:" | tee -a "${LOG_FILE}"
echo "Total files attempted: ${#files[@]}" | tee -a "${LOG_FILE}"
echo "Failed downloads: ${#failed_files[@]}" | tee -a "${LOG_FILE}"

if [ ${#failed_files[@]} -gt 0 ]; then
    echo "Failed files:" | tee -a "${LOG_FILE}"
    printf '%s\n' "${failed_files[@]}" | tee -a "${LOG_FILE}"
    exit 1
fi

echo "All downloads completed successfully at $(date)" | tee -a "${LOG_FILE}"
        </file>
        <file path="dbTools/dem/download_merit_parallel.sh" line_interval="25">
#!/bin/bash
#
# download_merit_parallel.sh
#
# Downloads MERIT DEM files in parallel with controlled concurrency. 
# Retains retry logic for robustness.

# --- Configuration ---

# Credentials
USERNAME="hydrography"
PASSWORD="rivernetwork"

# Base URL for the dataset
BASE_URL="http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_Hydro/distribute/v1.0"

# Destination directory for downloaded files
DEST_DIR="/datasets/dem/merit"

# Main log file
LOG_FILE="${DEST_DIR}/download.log"

# Default number of parallel processes (override by passing an argument)
NUM_PARALLEL=4
#|LN|25|
if [ -n "$1" ]; then
  NUM_PARALLEL="$1"
fi

# Maximum number of retry attempts for each file
MAX_ATTEMPTS=3

# Seconds to wait between retries
RETRY_WAIT=180

# Seconds to wait between spawn of parallel tasks (to avoid hammering the server)
# You can set this to 0 if you prefer no delay at all.
SPAWN_DELAY=45


# --- Setup ---

# Create destination directory
mkdir -p "${DEST_DIR}"

# Initialize log file
echo "Starting MERIT DEM download at $(date)" > "${LOG_FILE}"
echo "Using concurrency level: ${NUM_PARALLEL}" | tee -a "${LOG_FILE}"

# Temporary file to track failures across parallel processes
#|LN|50|
FAIL_FILE=$(mktemp)
touch "$FAIL_FILE"


# --- Helper Functions ---

# Thread-safe logging function for general messages.
# In extreme concurrency, lines may still interleave. For robust locking,
# consider using 'flock'. We keep it simple here.
log_message() {
  local msg="$1"
  echo "$msg" | tee -a "$LOG_FILE"
}

# Download a single file with retry logic
# Usage: download_file "filename.tar"
download_file() {
    local filename="$1"
    local attempt

    for ((attempt=1; attempt<=MAX_ATTEMPTS; attempt++)); do
        log_message "Downloading ${filename} (attempt ${attempt}/${MAX_ATTEMPTS})..."
        
        if wget --quiet --user="${USERNAME}" \
                --password="${PASSWORD}" \
#|LN|75|
                --no-check-certificate \
                -P "${DEST_DIR}" \
                "${BASE_URL}/${filename}"; then
            log_message "Successfully downloaded ${filename}"
            return 0
        else
            log_message "Failed to download ${filename} on attempt ${attempt}"
            if [ $attempt -lt $MAX_ATTEMPTS ]; then
                log_message "Waiting ${RETRY_WAIT} seconds before retry..."
                sleep "${RETRY_WAIT}"
            fi
        fi
    done
    
    # If we reached here, all attempts failed
    echo "${filename}" >> "${FAIL_FILE}"
    log_message "Failed to download ${filename} after ${MAX_ATTEMPTS} attempts"
    return 1
}


# --- File List ---

# List of all valid files to download (excluding "no data" regions)
files=(
#|LN|100|
    # # N60-N90
    "elv_n60w180.tar" "elv_n60w150.tar" "elv_n60w120.tar" "elv_n60w090.tar" "elv_n60w060.tar" "elv_n60w030.tar"
    "elv_n60e000.tar" "elv_n60e030.tar" "elv_n60e060.tar" "elv_n60e090.tar" "elv_n60e120.tar" "elv_n60e150.tar"
    # # N30-N60
    "elv_n30w180.tar" "elv_n30w150.tar" "elv_n30w120.tar" "elv_n30w090.tar" "elv_n30w060.tar" "elv_n30w030.tar"
    "elv_n30e000.tar" "elv_n30e030.tar" "elv_n30e060.tar" "elv_n30e090.tar" "elv_n30e120.tar" "elv_n30e150.tar"
    # N00-N30
    "elv_n00w180.tar" "elv_n00w120.tar" "elv_n00w090.tar" "elv_n00w060.tar" "elv_n00w030.tar"
    "elv_n00e000.tar" "elv_n00e030.tar" "elv_n00e060.tar" "elv_n00e090.tar" "elv_n00e120.tar" "elv_n00e150.tar"
    # S30-N00
    "elv_s30w180.tar" "elv_s30w150.tar" "elv_s30w120.tar" "elv_s30w090.tar" "elv_s30w060.tar" "elv_s30w030.tar"
    "elv_s30e000.tar" "elv_s30e030.tar" "elv_s30e060.tar" "elv_s30e090.tar" "elv_s30e120.tar" "elv_s30e150.tar"
    # S60-S30
    "elv_s60w180.tar" "elv_s60w090.tar" "elv_s60w060.tar" "elv_s60w030.tar"
    "elv_s60e000.tar" "elv_s60e030.tar" "elv_s60e060.tar" "elv_s60e090.tar" "elv_s60e120.tar" "elv_s60e150.tar"
)

total_files=${#files[@]}
log_message "Total files to download: ${total_files}"

# --- Parallel Download Loop ---

pids=()
count=0

#|LN|125|
for file in "${files[@]}"; do
    # Start the download in a background subshell
    (
      download_file "${file}"
    ) &
    pids+=($!)
    
    # Throttle concurrency
    ((count++))
    if [ "$((count % NUM_PARALLEL))" -eq 0 ]; then
        # Wait for at least one job to finish before spawning new ones
        wait -n
    fi

    # Optional small delay to prevent saturating the remote server
    sleep "${SPAWN_DELAY}"
done

# Wait for all remaining jobs to finish
wait

# --- Summarize ---

# Gather any failed files from the temp failure file
mapfile -t failed_files < "$FAIL_FILE"
#|LN|150|
rm -f "$FAIL_FILE"

num_failed=${#failed_files[@]}
log_message ""
log_message "Download Summary:"
log_message "Total files attempted: ${total_files}"
log_message "Failed downloads: ${num_failed}"

if [ ${num_failed} -gt 0 ]; then
    log_message "Failed files:"
    for f in "${failed_files[@]}"; do
        log_message "  - $f"
    done
    log_message "Check the log file for more details: ${LOG_FILE}"
    exit 1
fi

log_message "All downloads completed successfully at $(date)"
exit 0
        </file>
      </dir>
      <dir path="dbTools/export">
        <dir path="dbTools/export/v0">
          <dir path="dbTools/export/v0/common">
            <file path="dbTools/export/v0/common/clade_defns.sh" line_interval="25">
#!/bin/bash
# ------------------------------------------------------------------------------
# clade_defns.sh
# ------------------------------------------------------------------------------
# This file defines the integer-based filtering expressions for macroclades,
# clades, and metaclades, referencing columns in "expanded_taxa".
#
# Usage:
#   source clade_defns.sh
#   Then pick a macroclade (MACROCLADE="..."), or a clade (CLADE="..."),
#   or a metaclade (METACLADE="...") in your environment, and the
#   cladistic.sh script will build a condition from one of the arrays below.
#
# Example:
#   MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'
#   CLADES["insecta"]='("L50_taxonID" = 47158)'
#   METACLADES["primary_terrestrial_arthropoda"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'
#
# Be sure to substitute the correct taxonIDs for your local database!
# ------------------------------------------------------------------------------
#
# Sections in this file:
#   1) Macroclade Definitions
#   2) Clade Definitions
#|LN|25|
#   3) Metaclade Definitions
#   4) get_clade_condition() helper
#
# NOTE: We do NOT remove any existing definitions or comments.

# ---[ 1) Macroclade Definitions ]---------------------------------------------
# Typically for kingdom-level (L70) or phylum-level (L60) anchors.

declare -A MACROCLADES

# 1) Arthropoda => phylum at L60 = 47120
MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'

# 2) Chordata => phylum at L60 = 2
MACROCLADES["chordata"]='("L60_taxonID" = 2)'

# 3) Plantae => kingdom at L70 = 47126
MACROCLADES["plantae"]='("L70_taxonID" = 47126)'

# 4) Fungi => kingdom at L70 = 47170
MACROCLADES["fungi"]='("L70_taxonID" = 47170)'

# (Optional) If you consider Actinopterygii, Mammalia, Reptilia, etc.
# to be "macroclades," you may define them here instead of in CLADES.
# For instance:
#|LN|50|
#   MACROCLADES["mammalia"]='("L50_taxonID" = 40151)'


# ---[ 2) Clade Definitions ]--------------------------------------------------
# Typically for class-level (L50), order-level (L40), or narrower taxonomic groups.
# single-root, so functionally equivalent to METACLADES.

declare -A CLADES

# -- Plant Clades (Subphylum and Class levels) --
# -- Plant Subphylum (L57) --
CLADES["angiospermae"]='("L57_taxonID" = 47125)' # flowering plants

# -- Plant Classes (L50) --
CLADES["liliopsida"]='("L50_taxonID" = 47163)'    # monocots
CLADES["magnoliopsida"]='("L50_taxonID" = 47124)' # dicots

# -- Class-level (L50) Examples --
CLADES["actinopterygii"]='("L50_taxonID" = 47178)'
CLADES["amphibia"]='("L50_taxonID" = 20978)'
CLADES["arachnida"]='("L50_taxonID" = 47119)'
CLADES["aves"]='("L50_taxonID" = 3)'
CLADES["insecta"]='("L50_taxonID" = 47158)'
CLADES["mammalia"]='("L50_taxonID" = 40151)'
CLADES["reptilia"]='("L50_taxonID" = 26036)'
#|LN|75|

# -- Order-level (L40) Examples --
CLADES["testudines"]='("L40_taxonID" = 39532)'
CLADES["crocodylia"]='("L40_taxonID" = 26039)'
CLADES["coleoptera"]='("L40_taxonID" = 47208)'
CLADES["lepidoptera"]='("L40_taxonID" = 47157)'
CLADES["hymenoptera"]='("L40_taxonID" = 47201)'
CLADES["hemiptera"]='("L40_taxonID" = 47744)'
CLADES["orthoptera"]='("L40_taxonID" = 47651)'
CLADES["odonata"]='("L40_taxonID" = 47792)'
CLADES["diptera"]='("L40_taxonID" = 47822)'

# -- Additional Named Groups (Suborders, Clade Subsets, etc.) --
# Pterygota => The DB shows two taxonIDs (184884, 418641) plus
# another entry with L40_taxonID=48796. We combine them with OR:
CLADES["pterygota"]='("taxonID" = 184884 OR "taxonID" = 418641 OR "L40_taxonID" = 48796)'

# Phasmatodea => Not found in your query results. If/when you know its ID,
# you can fill it in here:
# CLADES["phasmatodea"]='("L40_taxonID" = ???)'

# Subclades within Hymenoptera (all share L40_taxonID=47201).
# Typically, referencing the top-level order is "hymenoptera"
# while these might be more specific anchor taxa:
CLADES["aculeata"]='("taxonID" = 326777)'
#|LN|100|
CLADES["apoidea"]='("taxonID" = 47222)'
CLADES["formicidae"]='("taxonID" = 47336)'
CLADES["vespoidea"]='("taxonID" = 48740)'
CLADES["vespidae"]='("taxonID" = 52747)'


# ---[ 3) Metaclade Definitions ]----------------------------------------------
# Multi-root or cross-macroclade definitions. Compose bigger groups using OR.

declare -A METACLADES

# Example 1: primary_terrestrial_arthropoda (pta) => Insecta OR Arachnida.
METACLADES["pta"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'

# Example 2: flying_vertebrates => Birds (aves) OR Bats (chiroptera)
# METACLADES["flying_vertebrates"]='("L50_taxonID" = 3 OR "L40_taxonID" = 7721)'

# Example 3: nonavian_reptiles => reptilia minus birds.
# METACLADES["nonavian_reptiles"]='("L50_taxonID" = 26036 AND "L50_taxonID" != 3)'


# ---[ 4) get_clade_condition() Helper ]-----------------------------------------
# Picks the correct expression given environment variables (METACLADE, CLADE,
# MACROCLADE). This is used by cladistic.sh to filter rows.

#|LN|125|
function get_clade_condition() {
  local condition

  # 1) If METACLADE is set (and found in METACLADES), return that
  if [[ -n "${METACLADE}" && -n "${METACLADES[${METACLADE}]}" ]]; then
    condition="${METACLADES[${METACLADE}]}"
    echo "${condition}"
    return
  fi

  # 2) Else if CLADE is set
  if [[ -n "${CLADE}" && -n "${CLADES[${CLADE}]}" ]]; then
    condition="${CLADES[${CLADE}]}"
    echo "${condition}"
    return
  fi

  # 3) Else if MACROCLADE is set
  if [[ -n "${MACROCLADE}" && -n "${MACROCLADES[${MACROCLADE}]}" ]]; then
    condition="${MACROCLADES[${MACROCLADE}]}"
    echo "${condition}"
    return
  fi

  # 4) Fallback: no recognized key => no filter
#|LN|150|
  echo "TRUE"
}

export -f get_clade_condition
            </file>
            <file path="dbTools/export/v0/common/clade_helpers.sh" line_interval="25">
#!/bin/bash
# ------------------------------------------------------------------------------
# clade_helpers.sh
# ------------------------------------------------------------------------------
# This file contains helper functions for multi-root/metaclade logic,
# rank boundary calculations, and advanced taxon-ancestry checks.
#
# Proposed usage:
#   1) "parse_clade_expression()" to parse user-provided condition strings
#      (e.g. "L50_taxonID=123 OR L40_taxonID=9999") into structured data.
#   2) "check_root_independence()" to verify that each root is truly disjoint
#      (none is an ancestor of another).
#   3) "get_major_rank_floor()" to compute the next-lower major-rank boundary
#      if user does not want to include minor ranks. Typically used if the root
#      is e.g. 57 => 50. If user includes minor ranks, we skip the rounding.
#
# NOTE: We do not forcibly integrate with existing "get_clade_condition()"
# in clade_defns.sh. Instead, you can call parse_clade_expression() if you
# want to do deeper multi-root logic.
#
# Implementation details:
#   - We store a reference map from "L<number>_taxonID" to the numeric rank
#     (e.g. "L50_taxonID" => 50). If the user requests minor ranks, we do not
#     round them down to the multiple of 10.
#|LN|25|
#   - We rely on "expanded_taxa" for ancestry checks. The "check_root_independence()"
#     function is conceptual: it gathers each root's entire ancestry (e.g. ~30
#     columns from L5..L70) and ensures no overlap among root sets.
#
# ------------------------------------------------------------------------------
#
# Exports:
#   - parse_clade_expression()
#   - check_root_independence()
#   - get_major_rank_floor()
#

# -------------------------------------------------------------
# A) Internal reference: Maps "L50_taxonID" => 50, "L40_taxonID" => 40, etc.
# -------------------------------------------------------------
declare -A RANKLEVEL_MAP=(
  ["L5_taxonID"]="5"
  ["L10_taxonID"]="10"
  ["L11_taxonID"]="11"
  ["L12_taxonID"]="12"
  ["L13_taxonID"]="13"
  ["L15_taxonID"]="15"
  ["L20_taxonID"]="20"
  ["L24_taxonID"]="24"
  ["L25_taxonID"]="25"
#|LN|50|
  ["L26_taxonID"]="26"
  ["L27_taxonID"]="27"
  ["L30_taxonID"]="30"
  ["L32_taxonID"]="32"
  ["L33_taxonID"]="33"
  ["L33_5_taxonID"]="33.5"
  ["L34_taxonID"]="34"
  ["L34_5_taxonID"]="34.5"
  ["L35_taxonID"]="35"
  ["L37_taxonID"]="37"
  ["L40_taxonID"]="40"
  ["L43_taxonID"]="43"
  ["L44_taxonID"]="44"
  ["L45_taxonID"]="45"
  ["L47_taxonID"]="47"
  ["L50_taxonID"]="50"
  ["L53_taxonID"]="53"
  ["L57_taxonID"]="57"
  ["L60_taxonID"]="60"
  ["L67_taxonID"]="67"
  ["L70_taxonID"]="70"
  # stateofmatter => 100, if we had that in expanded_taxa
)

# --------------------------------------------------------------------------
#|LN|75|
# parse_clade_expression()
# --------------------------------------------------------------------------
# Parses a SQL-like expression containing L{XX}_taxonID conditions into an array 
# of "rank=taxonID" pairs.
#
# Expected usage:
#   - We typically pass the result of get_clade_condition(), which looks like:
#     ("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)
#   - The caller captures the results in an array:
#     roots=( $(parse_clade_expression "$clade_condition") )
#
# Processing steps:
#   1) Removes parentheses and double quotes
#   2) Splits on " OR " to handle multiple conditions
#   3) For each condition:
#      - Splits on '=' to get the LHS and RHS
#      - Extracts the rank number from L{XX}_taxonID pattern
#      - Pairs the rank with the taxonID
#
# Return format:
#   Space-separated strings in the form "rank=taxonID", e.g.:
#   "50=47158" "50=47119"
#
# Examples:
#   Input:  "L50_taxonID" = 47158
#|LN|100|
#   Output: 50=47158
#
#   Input:  ("L50_taxonID" = 47158 OR "L40_taxonID" = 9999)
#   Output: 50=47158 40=9999
#
# Notes:
#   - Case-insensitive: l50_taxonid and L50_taxonID are equivalent
#   - Spaces around '=' are optional
#   - Ignores any conditions not matching L{XX}_taxonID pattern
#   - Requires numeric taxonID values
# --------------------------------------------------------------------------
function parse_clade_expression() {
  local expr="$1"

  # 1) Remove parentheses and double quotes
  local cleaned_expr
  cleaned_expr="$(echo "$expr" | tr -d '()"')"
  echo "DEBUG [2]: After removing parentheses/quotes: '$cleaned_expr'" >&2

  # 2) Split on " OR " properly using sed
  local or_parts
  or_parts="$(echo "$cleaned_expr" | sed 's/ OR /\n/g')"
  
  local results=()
  
#|LN|125|
  while IFS= read -r part; do
    # Trim spaces and split on =
    local lhs rhs
    IFS='=' read -r lhs rhs <<< "$(echo "$part" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')"
    
    # Remove any remaining spaces
    lhs="$(echo "$lhs" | sed 's/[[:space:]]//g')"
    rhs="$(echo "$rhs" | sed 's/[[:space:]]//g')"
    
    # Extract the numeric part from LXX_taxonID
    if [[ $lhs =~ L([0-9]+)_taxonID ]]; then
      local rank="${BASH_REMATCH[1]}"
      results+=( "${rank}=${rhs}" )
    fi
  done <<< "$or_parts"

  echo "${results[@]}"
}

function check_root_independence() {
  # --------------------------------------------------------------------------
  # check_root_independence()
  #
  # PURPOSE:
  #   Ensures that each root in a multi-root scenario is truly independent,
#|LN|150|
  #   i.e., no root is an ancestor or descendant of another when viewed at
  #   or below the highest rank boundary. For example, if you have two roots
  #   at rank=50 (Insecta, Arachnida), they do share a phylum at rank=60
  #   (Arthropoda), but that is above their rank boundary, so it should NOT
  #   trigger a conflict.
  #
  # IMPLEMENTATION STEPS:
  #   1) Parse the root array (each item = "rank=taxonID", e.g. "50=47158").
  #   2) Find the globalMaxRank = max(r_i for each root).
  #   3) For each root, fetch its single row from expanded_taxa (which includes
  #      columns L5..L70). Then cross-join or left-join each potential ancestor
  #      ID to get that ancestor's rankLevel from expanded_taxa.
  #      Keep only those whose rankLevel <= globalMaxRank.
  #   4) Build a set of taxonIDs for that root (space-separated).
  #   5) Compare each pair of root sets for intersection. If they share a taxonID
  #      that is rankLevel <= globalMaxRank, we treat it as an overlap => return 1.
  #
  #   If no overlap is found among the rank <= globalMaxRank ancestors, return 0.
  #
  # USAGE:
  #   check_root_independence <db_name> <rootArray...>
  #   e.g. check_root_independence "myDB" "50=47158" "50=47119"
  #
  # RETURNS:
  #   0 if no overlap found, 1 if overlap is detected or root is not found.
#|LN|175|
  # --------------------------------------------------------------------------

  local dbName="$1"
  shift
  local roots=("$@")  # e.g. ("50=47158" "50=47119")

  # If there's 0 or 1 root, there's nothing to compare => trivially independent
  if [ "${#roots[@]}" -le 1 ]; then
    return 0
  fi

  # 1) Determine the global max rank among all root definitions
  local globalMaxRank=0
  for r in "${roots[@]}"; do
    local rr="${r%%=*}"
    if (( rr > globalMaxRank )); then
      globalMaxRank="$rr"
    fi
  done

  declare -A rootSets  # will map index => "list of ancestor taxonIDs"

  for i in "${!roots[@]}"; do
    local pair="${roots[$i]}"
    local rank="${pair%%=*}"
#|LN|200|
    local tid="${pair##*=}"

    # We'll do an expanded cross-lateral approach to gather the root's entire
    # L5..L70 columns, then retrieve each ancestor's rankLevel, ignoring any
    # with rankLevel > globalMaxRank.
    #
    # Because we only do ONE row for the root (plus ~30 columns), a single
    # CROSS JOIN to expanded_taxa for each ancestor ID is feasible.

    local sql="
COPY (
  WITH one_root AS (
    SELECT
      e.\"taxonID\" AS sp_id,
      e.\"L5_taxonID\", e.\"L10_taxonID\", e.\"L11_taxonID\", e.\"L12_taxonID\",
      e.\"L13_taxonID\", e.\"L15_taxonID\", e.\"L20_taxonID\", e.\"L24_taxonID\",
      e.\"L25_taxonID\", e.\"L26_taxonID\", e.\"L27_taxonID\", e.\"L30_taxonID\",
      e.\"L32_taxonID\", e.\"L33_taxonID\", e.\"L33_5_taxonID\", e.\"L34_taxonID\",
      e.\"L34_5_taxonID\", e.\"L35_taxonID\", e.\"L37_taxonID\", e.\"L40_taxonID\",
      e.\"L43_taxonID\", e.\"L44_taxonID\", e.\"L45_taxonID\", e.\"L47_taxonID\",
      e.\"L50_taxonID\", e.\"L53_taxonID\", e.\"L57_taxonID\", e.\"L60_taxonID\",
      e.\"L67_taxonID\", e.\"L70_taxonID\"
    FROM expanded_taxa e
    WHERE e.\"taxonID\" = ${tid}
  ),
#|LN|225|
  potential_ancestors AS (
    SELECT sp_id as taxon_id FROM one_root
    UNION ALL
    SELECT anc.\"taxonID\"
    FROM one_root o
    CROSS JOIN LATERAL (VALUES
      (o.\"L5_taxonID\"),(o.\"L10_taxonID\"),(o.\"L11_taxonID\"),(o.\"L12_taxonID\"),
      (o.\"L13_taxonID\"),(o.\"L15_taxonID\"),(o.\"L20_taxonID\"),(o.\"L24_taxonID\"),
      (o.\"L25_taxonID\"),(o.\"L26_taxonID\"),(o.\"L27_taxonID\"),(o.\"L30_taxonID\"),
      (o.\"L32_taxonID\"),(o.\"L33_taxonID\"),(o.\"L33_5_taxonID\"),(o.\"L34_taxonID\"),
      (o.\"L34_5_taxonID\"),(o.\"L35_taxonID\"),(o.\"L37_taxonID\"),(o.\"L40_taxonID\"),
      (o.\"L43_taxonID\"),(o.\"L44_taxonID\"),(o.\"L45_taxonID\"),(o.\"L47_taxonID\"),
      (o.\"L50_taxonID\"),(o.\"L53_taxonID\"),(o.\"L57_taxonID\"),(o.\"L60_taxonID\"),
      (o.\"L67_taxonID\"),(o.\"L70_taxonID\")
    ) x(ancestor_id)
    JOIN expanded_taxa anc ON anc.\"taxonID\" = x.ancestor_id
    WHERE anc.\"rankLevel\" <= ${globalMaxRank}
  )
  SELECT array_agg(potential_ancestors.taxon_id) AS allowed_ancestors
  FROM potential_ancestors
) TO STDOUT WITH CSV HEADER;
"
    local query_result
    query_result="$(execute_sql "$sql")"

#|LN|250|
    # If the query returns only a header line, it might indicate no row found
    # for that root. We can check for 'allowed_ancestors' in the last line.
    local data_line
    data_line="$(echo "$query_result" | tail -n1)"
    if [[ "$data_line" == *"allowed_ancestors"* ]]; then
      echo "ERROR: check_root_independence: No row found or no ancestors for taxonID=${tid}" >&2
      return 1
    fi

    # data_line might look like: {47158,47157,47120,...}
    # We'll remove braces and parse
    local trimmed="$(echo "$data_line" | tr -d '{}')"
    # e.g. 47158,47157,47120
    # We'll split on commas
    IFS=',' read -ra ancestors <<< "$trimmed"

    # Now store them in space-separated form
    rootSets["$i"]="${ancestors[*]}"
  done

  # 3) Compare each pair of sets for intersection
  for ((i=0; i<${#roots[@]}; i++)); do
    for ((j=i+1; j<${#roots[@]}; j++)); do
      local set1=" ${rootSets[$i]} "
      for t2 in ${rootSets[$j]}; do
#|LN|275|
        # If the token t2 appears in set1 => overlap
        # (We assume space-bounded match to avoid partial string hits)
        if [[ "$set1" =~ " $t2 " ]]; then
          echo "ERROR: Overlap detected between root #$i (${roots[$i]}) \
and root #$j (${roots[$j]}) on taxonID=${t2}" >&2
          return 1
        fi
      done
    done
  done

  return 0
}

# -------------------------------------------------------------
# D) get_major_rank_floor()
# -------------------------------------------------------------
# This function returns the next-lower major rank multiple of 10 if we want
# to exclude minor ranks. For instance:
#   if input=57 => output=50
#   if input=50 => output=40
#   if input=70 => output=60
#
# If the user wants minor ranks, we might skip or do partial rounding logic.
# For now, we do a straightforward approach:
#|LN|300|
#
function get_major_rank_floor() {
  local input_rank="$1"
  # We'll do a naive loop:
  # possible major ranks = [70,60,50,40,30,20,10,5]
  # or we can do math: floor((input_rank/10))*10 => but that fails for e.g. 57 => 50 is fine
  # Actually that might be enough, but let's handle if it's exactly a multiple of 10 => we subtract 10 again
  # e.g. 50 => 40, because we want "strictly less than the root rank".
  # If input=57 => floor(57/10)*10=50 => good
  # If input=50 => floor(50/10)*10=50 => but we want 40 => so let's do -10 if exactly multiple

  local base=$(( input_rank/10*10 ))
  if (( $(echo "$input_rank == $base" | bc) == 1 )); then
    # means input is multiple of 10
    base=$(( base-10 ))
  fi
  echo "$base"
}

export -f parse_clade_expression
export -f check_root_independence
export -f get_major_rank_floor
            </file>
            <file path="dbTools/export/v0/common/cladistic.sh" line_interval="25">
#!/bin/bash
# ------------------------------------------------------------------------------
# cladistic.sh (Revised for Option A: always quoted taxon-level columns)
# ------------------------------------------------------------------------------
# Creates a final observation subset for a user-specified clade/metaclade,
# referencing the "expanded_taxa" table. The input table for this script is
# typically provided in ANCESTORS_OBS_TABLE (set by regional_base.sh).
#
# Steps:
#   1) Validate environment & drop <EXPORT_GROUP>_observations if it exists
#   2) Construct a filtering WHERE clause for research/non-research quality
#   3) Create the <EXPORT_GROUP>_observations table by joining to expanded_taxa
#   4) Optionally wipe partial ranks (L20, L30, L40) if MIN_OCCURRENCES_PER_RANK is set
#   5) Export to CSV via partition-based random sampling
#
# Environment Variables (required):
#   ANCESTORS_OBS_TABLE
#   EXPORT_GROUP
#   DB_CONTAINER, DB_USER, DB_NAME, BASE_DIR, etc.
#
# Optional:
#   RG_FILTER_MODE, MIN_OCCURRENCES_PER_RANK, MAX_RN, PRIMARY_ONLY,
#   INCLUDE_ELEVATION_EXPORT
#
#|LN|25|
# Revision highlights:
#   - All references to columns like L5_taxonID, L10_taxonID, etc. are double-quoted
#     so that Postgres stores them in mixed-case and we can select them without
#     running into case-folding issues.
# ------------------------------------------------------------------------------

set -e

# 1) Source common functions & check ANCESTORS_OBS_TABLE
source "${BASE_DIR}/common/functions.sh"

if [ -z "${ANCESTORS_OBS_TABLE:-}" ]; then
  echo "ERROR: cladistic.sh requires ANCESTORS_OBS_TABLE to be set."
  exit 1
fi

print_progress "cladistic.sh: Using ancestor-based table = ${ANCESTORS_OBS_TABLE}"

TABLE_NAME="${EXPORT_GROUP}_observations"
execute_sql "DROP TABLE IF EXISTS \"${TABLE_NAME}\" CASCADE;"

# ------------------------------------------------------------------------------
# 2) Construct RG filter condition & possibly rewrite "L10_taxonID"
# ------------------------------------------------------------------------------
rg_where_condition="TRUE"
#|LN|50|
rg_l10_col="e.\"L10_taxonID\""  # might become NULL::integer if we do a wipe

case "${RG_FILTER_MODE:-ALL}" in
  "ONLY_RESEARCH")
    rg_where_condition="o.quality_grade='research'"
    ;;
  "ALL")
    rg_where_condition="TRUE"
    ;;
  "ALL_EXCLUDE_SPECIES_NON_RESEARCH")
    rg_where_condition="NOT (o.quality_grade!='research' AND e.\"L10_taxonID\" IS NOT NULL)"
    ;;
  "ONLY_NONRESEARCH")
    rg_where_condition="o.quality_grade!='research'"
    ;;
  "ONLY_NONRESEARCH_EXCLUDE_SPECIES")
    rg_where_condition="(o.quality_grade!='research' AND e.\"L10_taxonID\" IS NULL)"
    ;;
  "ONLY_NONRESEARCH_WIPE_SPECIES_LABEL")
    rg_where_condition="o.quality_grade!='research'"
    rg_l10_col="NULL::integer"
    ;;
  *)
    rg_where_condition="TRUE"
    ;;
#|LN|75|
esac

# ------------------------------------------------------------------------------
# 3) Create <EXPORT_GROUP>_observations by joining ANCESTORS_OBS_TABLE + expanded_taxa
# ------------------------------------------------------------------------------
OBS_COLUMNS="$(get_obs_columns)"  # e.g. observation_uuid, observer_id, latitude, longitude, etc.

# We explicitly alias each expanded_taxa column in quotes, so that Postgres
# stores them in mixed-case (e.g. "L5_taxonID") and we can select them reliably.
EXPANDED_TAXA_COLS="
    e.\"taxonID\"        AS \"expanded_taxonID\",
    e.\"rankLevel\"      AS \"expanded_rankLevel\",
    e.\"name\"           AS \"expanded_name\",
    e.\"L5_taxonID\"     AS \"L5_taxonID\",
    ${rg_l10_col}        AS \"L10_taxonID\",
    e.\"L11_taxonID\"    AS \"L11_taxonID\",
    e.\"L12_taxonID\"    AS \"L12_taxonID\",
    e.\"L13_taxonID\"    AS \"L13_taxonID\",
    e.\"L15_taxonID\"    AS \"L15_taxonID\",
    e.\"L20_taxonID\"    AS \"L20_taxonID\",
    e.\"L24_taxonID\"    AS \"L24_taxonID\",
    e.\"L25_taxonID\"    AS \"L25_taxonID\",
    e.\"L26_taxonID\"    AS \"L26_taxonID\",
    e.\"L27_taxonID\"    AS \"L27_taxonID\",
    e.\"L30_taxonID\"    AS \"L30_taxonID\",
#|LN|100|
    e.\"L32_taxonID\"    AS \"L32_taxonID\",
    e.\"L33_taxonID\"    AS \"L33_taxonID\",
    e.\"L33_5_taxonID\"  AS \"L33_5_taxonID\",
    e.\"L34_taxonID\"    AS \"L34_taxonID\",
    e.\"L34_5_taxonID\"  AS \"L34_5_taxonID\",
    e.\"L35_taxonID\"    AS \"L35_taxonID\",
    e.\"L37_taxonID\"    AS \"L37_taxonID\",
    e.\"L40_taxonID\"    AS \"L40_taxonID\",
    e.\"L43_taxonID\"    AS \"L43_taxonID\",
    e.\"L44_taxonID\"    AS \"L44_taxonID\",
    e.\"L45_taxonID\"    AS \"L45_taxonID\",
    e.\"L47_taxonID\"    AS \"L47_taxonID\",
    e.\"L50_taxonID\"    AS \"L50_taxonID\",
    e.\"L53_taxonID\"    AS \"L53_taxonID\",
    e.\"L57_taxonID\"    AS \"L57_taxonID\",
    e.\"L60_taxonID\"    AS \"L60_taxonID\",
    e.\"L67_taxonID\"    AS \"L67_taxonID\",
    e.\"L70_taxonID\"    AS \"L70_taxonID\"
"

execute_sql "
CREATE TABLE \"${TABLE_NAME}\" AS
SELECT
    ${OBS_COLUMNS},        -- these are unquoted columns like observation_uuid, etc.
    o.in_region,           -- already all-lowercase
#|LN|125|
    ${EXPANDED_TAXA_COLS}
FROM \"${ANCESTORS_OBS_TABLE}\" o
JOIN expanded_taxa e ON e.\"taxonID\" = o.taxon_id
WHERE e.\"taxonActive\" = TRUE
  AND (${rg_where_condition});
"

# ------------------------------------------------------------------------------
# 4) Optional partial-rank wipe for L20, L30, L40
# ------------------------------------------------------------------------------
if [ -z "${MIN_OCCURRENCES_PER_RANK:-}" ] || [ "${MIN_OCCURRENCES_PER_RANK}" = "-1" ]; then
  print_progress "Skipping partial-rank wipe (MIN_OCCURRENCES_PER_RANK not set or -1)."
else
  print_progress "Applying partial-rank wipe with threshold = ${MIN_OCCURRENCES_PER_RANK}"
  for rc in L20_taxonID L30_taxonID L40_taxonID; do
    print_progress "Wiping low-occurrence ${rc} if usage < ${MIN_OCCURRENCES_PER_RANK}"
    execute_sql "
    WITH usage_ct AS (
      SELECT \"${rc}\" as tid, COUNT(*) as c
      FROM \"${TABLE_NAME}\"
      WHERE \"${rc}\" IS NOT NULL
      GROUP BY 1
    )
    UPDATE \"${TABLE_NAME}\"
    SET \"${rc}\" = NULL
#|LN|150|
    FROM usage_ct
    WHERE usage_ct.tid = \"${TABLE_NAME}\".\"${rc}\"
      AND usage_ct.c < ${MIN_OCCURRENCES_PER_RANK};
    "
  done
fi

# ------------------------------------------------------------------------------
# 5) Export Final CSV with partition-based sampling for research-grade species
# ------------------------------------------------------------------------------
print_progress "cladistic.sh: Exporting final CSV with partition-based sampling"

pos_condition="TRUE"
if [ "${PRIMARY_ONLY:-false}" = "true" ]; then
  pos_condition="p.position=0"
fi

# Build the final column list for the CSV, all double-quoted so that the
# headers appear exactly as L5_taxonID, etc., in the CSV.
#
# We'll re-use get_obs_columns() output but wrap them in quotes for final usage.
# Then we manually append in_region, expanded columns, etc., all in quotes.

quote_columns() {
  # Helper that takes a comma-delimited string of column names and
#|LN|175|
  # returns them as a quoted, comma-delimited list (e.g. "colA","colB",...).
  local input="$1"
  local quoted=""
  IFS=',' read -ra cols <<< "$input"
  for col in "${cols[@]}"; do
    col="$(echo "$col" | xargs)"  # trim spaces
    [ -n "$quoted" ] && quoted="$quoted, \"$col\"" || quoted="\"$col\""
  done
  echo "$quoted"
}

RAW_OBS_COLS="$(get_obs_columns)"  # e.g. observation_uuid, observer_id, ...
QUOTED_OBS_COLS="$(quote_columns "$RAW_OBS_COLS")"

# Now define the expanded columns in quotes. Note that these match the aliases
# we used above (AS "L5_taxonID", etc.). Also in_region is a lowercase column,
# but we quote it for consistency in the final CSV.
CSV_OBS_COLS="$QUOTED_OBS_COLS,
\"in_region\",
\"expanded_taxonID\",
\"expanded_rankLevel\",
\"expanded_name\",
\"L5_taxonID\",
\"L10_taxonID\",
\"L11_taxonID\",
#|LN|200|
\"L12_taxonID\",
\"L13_taxonID\",
\"L15_taxonID\",
\"L20_taxonID\",
\"L24_taxonID\",
\"L25_taxonID\",
\"L26_taxonID\",
\"L27_taxonID\",
\"L30_taxonID\",
\"L32_taxonID\",
\"L33_taxonID\",
\"L33_5_taxonID\",
\"L34_taxonID\",
\"L34_5_taxonID\",
\"L35_taxonID\",
\"L37_taxonID\",
\"L40_taxonID\",
\"L43_taxonID\",
\"L44_taxonID\",
\"L45_taxonID\",
\"L47_taxonID\",
\"L50_taxonID\",
\"L53_taxonID\",
\"L57_taxonID\",
\"L60_taxonID\",
#|LN|225|
\"L67_taxonID\",
\"L70_taxonID\""

# Photo columns, also quoted
CSV_PHOTO_COLS="\"photo_uuid\", \"photo_id\", \"extension\", \"license\", \"width\", \"height\", \"position\""

# Debug logging: use quote_literal() to avoid array-literal syntax errors
debug_sql_obs="
SELECT 'DEBUG: Final CSV obs columns => ' ||
       quote_literal('${CSV_OBS_COLS}');
"
debug_sql_photo="
SELECT 'DEBUG: Final CSV photo columns => ' ||
       quote_literal('${CSV_PHOTO_COLS}');
"

execute_sql "$debug_sql_obs"
execute_sql "$debug_sql_photo"

# If no MAX_RN, default to 3000
if [ -z "${MAX_RN:-}" ]; then
  echo "Warning: MAX_RN not set, defaulting to 3000"
  MAX_RN=3000
fi

#|LN|250|
EXPORT_FILE="${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv"

# Final COPY query uses these columns in quotes
execute_sql "
COPY (
  WITH
  capped_research_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"  -- This is a quoted column in the new table
        ORDER BY
          CASE WHEN o.in_region THEN 0 ELSE 1 END,
          random()
      ) AS rn
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE ${pos_condition}
#|LN|275|
      AND o.quality_grade='research'
      AND o.\"L10_taxonID\" IS NOT NULL
  ),
  everything_else AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      NULL::bigint AS rn
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE ${pos_condition}
      AND NOT (o.quality_grade='research' AND o.\"L10_taxonID\" IS NOT NULL)
  )
  SELECT
    ${CSV_OBS_COLS},
    ${CSV_PHOTO_COLS},
    rn
  FROM capped_research_species
  WHERE rn <= ${MAX_RN}
#|LN|300|

  UNION ALL

  SELECT
    ${CSV_OBS_COLS},
    ${CSV_PHOTO_COLS},
    rn
  FROM everything_else
) TO '${EXPORT_FILE}'
  WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"

print_progress "cladistic.sh: CSV export complete"
print_progress "Exported final CSV to ${EXPORT_FILE}"
            </file>
            <file path="dbTools/export/v0/common/functions.sh">
#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

get_obs_columns() {
    # Start with standard columns
    local cols="observation_uuid, observer_id, latitude, longitude"
    
    # Add elevation_meters if export is enabled.
    if [ "${INCLUDE_ELEVATION_EXPORT:-true}" = "true" ] && [ "${RELEASE_VALUE:-r1}" != "r0" ]; then
        cols="${cols}, elevation_meters"
    fi
    
    # Then add the remaining columns
    cols="${cols}, positional_accuracy, taxon_id, quality_grade, observed_on"
    
    # If anomaly_score exists for any release value other than r0, add it.
    if [[ "${RELEASE_VALUE}" != "r0" ]]; then
        cols="${cols}, anomaly_score"
    fi
    
    echo "$cols"
}


# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    if [ ! -d "${dir}" ]; then
        mkdir -p "${dir}"
        chmod -R 777 "${dir}"
    fi
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f get_obs_columns
export -f ensure_directory
export -f send_notification
            </file>
            <file path="dbTools/export/v0/common/main.sh" line_interval="25">
#!/bin/bash
#
# main.sh
#
# Orchestrates the export pipeline by:
#   1) Validating environment variables
#   2) Always calling regional_base.sh (which handles creating/reusing
#      the region/clade-specific ancestor tables as needed).
#   3) Calling cladistic.sh to produce the final <EXPORT_GROUP>_observations table
#   4) Writing a unified export summary (environment variables + final stats)
#   5) Optionally copying the wrapper script for reproducibility
#
# NOTE:
#  - We no longer do skip/existence checks here. Instead, regional_base.sh
#    performs partial skip logic for its tables (_all_sp, _all_sp_and_ancestors_*, etc.).
#  - We have removed references to ANCESTOR_ROOT_RANKLEVEL, since our new multi-root
#    approach does not require it.
#
# This script expects the following environment variables to be set by the wrapper:
#   DB_USER           -> PostgreSQL user (e.g. "postgres")
#   VERSION_VALUE     -> Database version identifier (e.g. "v0")
#   RELEASE_VALUE     -> Release identifier (e.g. "r1")
#   ORIGIN_VALUE      -> (Optional) For logging context
#   DB_NAME           -> Name of the database (e.g. "ibrida-v0-r1")
#|LN|25|
#   REGION_TAG        -> Region bounding box key (e.g. "NAfull")
#   MIN_OBS           -> Minimum observations required for a species to be included
#   MAX_RN            -> Max random number of research-grade rows per species in final CSV
#   DB_CONTAINER      -> Docker container name for exec (e.g. "ibridaDB")
#   HOST_EXPORT_BASE_PATH -> Host system directory for exports
#   CONTAINER_EXPORT_BASE_PATH -> Container path that maps to HOST_EXPORT_BASE_PATH
#   EXPORT_SUBDIR     -> Subdirectory for the export (e.g. "v0/r1/primary_only_50min_4000max")
#   EXPORT_GROUP      -> Name of the final group (used in final table naming)
#
# Additionally, you may define:
#   WRAPPER_PATH      -> Path to the wrapper script for reproducibility (copied into the output dir if present).
#   INCLUDE_OUT_OF_REGION_OBS -> Whether to keep out-of-region observations for a region-based species.
#   RG_FILTER_MODE    -> One of: ONLY_RESEARCH, ALL, ALL_EXCLUDE_SPECIES_NON_RESEARCH, etc.
#   PRIMARY_ONLY      -> If true, only the primary (position=0) photo is included.
#   SKIP_REGIONAL_BASE-> If true, we skip regeneration of base tables if they exist.
#   INCLUDE_ELEVATION_EXPORT -> If "true", we include the 'elevation_meters' column (provided the DB has it, e.g. not "r0").
#
# All these environment variables are typically set in the release-specific wrapper (e.g. r1/wrapper_amphibia_all_exc_nonrg_sp.sh).
#

set -e

# ------------------------------------------------------------------------------
# 0) Source common functions
# ------------------------------------------------------------------------------
#|LN|50|
# We'll assume the caller sets BASE_DIR to the root of export/v0
# so that we can find common/functions.sh easily.
source "${BASE_DIR}/common/functions.sh"

# ------------------------------------------------------------------------------
# 1) Validate Required Environment Variables
# ------------------------------------------------------------------------------
required_vars=(
    "DB_USER" "VERSION_VALUE" "RELEASE_VALUE" "ORIGIN_VALUE"
    "DB_NAME" "REGION_TAG" "MIN_OBS" "MAX_RN"
    "DB_CONTAINER" "HOST_EXPORT_BASE_PATH" "CONTAINER_EXPORT_BASE_PATH"
    "EXPORT_SUBDIR" "EXPORT_GROUP"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var:-}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Some environment variables are optional but relevant:
# - SKIP_REGIONAL_BASE, INCLUDE_OUT_OF_REGION_OBS, RG_FILTER_MODE, MIN_OCCURRENCES_PER_RANK,
#   INCLUDE_MINOR_RANKS_IN_ANCESTORS, PRIMARY_ONLY, etc.
# We'll let them default if not set.
#|LN|75|

# ------------------------------------------------------------------------------
# 2) Create Export Directory Structure
# ------------------------------------------------------------------------------
print_progress "Creating export directory structure"
EXPORT_DIR="${CONTAINER_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
HOST_EXPORT_DIR="${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
ensure_directory "${HOST_EXPORT_DIR}"

# ------------------------------------------------------------------------------
# 3) Create PostgreSQL Extension & Role if needed (once per container, safe to run again)
# ------------------------------------------------------------------------------
execute_sql "
DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS dblink;
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'exportuser') THEN
        CREATE ROLE exportuser;
    END IF;
END \$\$;"

# ------------------------------------------------------------------------------
# Optional Logging for Elevation Setting
# ------------------------------------------------------------------------------
if [ "${INCLUDE_ELEVATION_EXPORT:-true}" = "true" ]; then
#|LN|100|
    print_progress "INCLUDE_ELEVATION_EXPORT=true => Elevation data (elevation_meters) will be included if present"
else
    print_progress "INCLUDE_ELEVATION_EXPORT=false => Elevation data will NOT be included"
fi

# ------------------------------------------------------------------------------
# Timing: We'll measure how long each major phase takes
# ------------------------------------------------------------------------------
overall_start=$(date +%s)

# ------------------------------------------------------------------------------
# 4) Always Invoke regional_base.sh
# ------------------------------------------------------------------------------
# The script 'regional_base.sh' is responsible for building or reusing
# region/clade-specific base tables. If SKIP_REGIONAL_BASE=true and the table
# exists, it is reused. Otherwise, it is created fresh.
regional_start=$(date +%s)
print_progress "Invoking ancestor-aware regional_base.sh"
source "${BASE_DIR}/common/regional_base.sh"
print_progress "regional_base.sh completed"
regional_end=$(date +%s)
regional_secs=$(( regional_end - regional_start ))

# ------------------------------------------------------------------------------
# 5) Apply Cladistic Filtering => Produces <EXPORT_GROUP>_observations
#|LN|125|
# ------------------------------------------------------------------------------
cladistic_start=$(date +%s)
print_progress "Applying cladistic filters via cladistic.sh"
source "${BASE_DIR}/common/cladistic.sh"
print_progress "Cladistic filtering complete"
cladistic_end=$(date +%s)
cladistic_secs=$(( cladistic_end - cladistic_start ))

# ------------------------------------------------------------------------------
# 6) Single Unified Export Summary
# ------------------------------------------------------------------------------
# We'll store environment variables, row counts, timing, etc. in a single file.
stats_start=$(date +%s)
print_progress "Creating unified export summary"

STATS=$(execute_sql "
WITH export_stats AS (
    SELECT
        COUNT(DISTINCT observation_uuid) AS num_observations,
        COUNT(DISTINCT taxon_id) AS num_taxa,
        COUNT(DISTINCT observer_id) AS num_observers
    FROM \"${EXPORT_GROUP}_observations\"
)
SELECT format(
    'Observations: %s\nUnique Taxa: %s\nUnique Observers: %s',
#|LN|150|
    num_observations, num_taxa, num_observers
)
FROM export_stats;")

SUMMARY_FILE="${HOST_EXPORT_DIR}/${EXPORT_GROUP}_export_summary.txt"
{
  echo "Export Summary"
  echo "Version: ${VERSION_VALUE}"
  echo "Release: ${RELEASE_VALUE}"
  echo "Origin: ${ORIGIN_VALUE}"
  echo "Region: ${REGION_TAG}"
  echo "Minimum Observations (species): ${MIN_OBS}"
  echo "Maximum Random Number (MAX_RN): ${MAX_RN}"
  echo "Export Group: ${EXPORT_GROUP}"
  echo "Date: $(date)"
  echo "SKIP_REGIONAL_BASE: ${SKIP_REGIONAL_BASE}"
  echo "SKIP_ALL_SP_TABLE: ${SKIP_ALL_SP_TABLE:-$SKIP_REGIONAL_BASE}"
  echo "SKIP_ANCESTORS_TABLE: ${SKIP_ANCESTORS_TABLE:-$SKIP_REGIONAL_BASE}"
  echo "INCLUDE_OUT_OF_REGION_OBS: ${INCLUDE_OUT_OF_REGION_OBS}"
  echo "INCLUDE_MINOR_RANKS_IN_ANCESTORS: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
  echo "RG_FILTER_MODE: ${RG_FILTER_MODE}"
  echo "MIN_OCCURRENCES_PER_RANK: ${MIN_OCCURRENCES_PER_RANK}"
  echo "INCLUDE_ELEVATION_EXPORT: ${INCLUDE_ELEVATION_EXPORT}"
  echo ""
  echo "Final Table Stats:"
#|LN|175|
  echo "${STATS}"
  echo ""
  echo "Timing:"
  echo " - Regional Base: ${regional_secs} seconds"
} > "${SUMMARY_FILE}"

stats_end=$(date +%s)
stats_secs=$(( stats_end - stats_start ))
print_progress "Stats/summary step took ${stats_secs} seconds"

# ------------------------------------------------------------------------------
# 7) Optionally Copy the Wrapper Script for Reproducibility
# ------------------------------------------------------------------------------
if [ -n "${WRAPPER_PATH:-}" ] && [ -f "${WRAPPER_PATH}" ]; then
    cp "${WRAPPER_PATH}" "${HOST_EXPORT_DIR}/"
fi

# ------------------------------------------------------------------------------
# 8) Wrap Up
# ------------------------------------------------------------------------------
overall_end=$(date +%s)
overall_secs=$(( overall_end - overall_start ))
print_progress "Export process complete (total time: ${overall_secs} seconds)"

{
#|LN|200|
  echo " - Cladistic: ${cladistic_secs} seconds"
  echo " - Summary/Stats Step: ${stats_secs} seconds"
  echo " - Overall: ${overall_secs} seconds"
} >> "${SUMMARY_FILE}"

send_notification "Export for ${EXPORT_GROUP} complete. Summary at ${SUMMARY_FILE}"
            </file>
            <file path="dbTools/export/v0/common/region_defns.sh">
#!/bin/bash
# ------------------------------------------------------------------------------
# region_defns.sh
# ------------------------------------------------------------------------------
# This file defines the bounding box coordinates for each supported region.
#
# Usage:
#   source region_defns.sh
#   Then set REGION_TAG in your environment, and use get_region_coordinates()
#   to populate XMIN, XMAX, YMIN, YMAX environment variables.
# ------------------------------------------------------------------------------

declare -A REGION_COORDINATES

# North America
REGION_COORDINATES["NAfull"]="(-169.453125 12.211180 -23.554688 84.897147)"

# Europe
REGION_COORDINATES["EURwest"]="(-12.128906 40.245992 12.480469 60.586967)"
REGION_COORDINATES["EURnorth"]="(-25.927734 54.673831 45.966797 71.357067)"
REGION_COORDINATES["EUReast"]="(10.722656 41.771312 39.550781 59.977005)"
REGION_COORDINATES["EURfull"]="(-30.761719 33.284620 43.593750 72.262310)"

# Mediterranean
REGION_COORDINATES["MED"]="(-16.259766 29.916852 36.474609 46.316584)"

# Australia
REGION_COORDINATES["AUSfull"]="(111.269531 -47.989922 181.230469 -9.622414)"

# Asia
REGION_COORDINATES["ASIAse"]="(82.441406 -11.523088 153.457031 28.613459)"
REGION_COORDINATES["ASIAeast"]="(462.304688 23.241346 550.195313 78.630006)"
REGION_COORDINATES["ASIAcentral"]="(408.515625 36.031332 467.753906 76.142958)"
REGION_COORDINATES["ASIAsouth"]="(420.468750 1.581830 455.097656 39.232253)"
REGION_COORDINATES["ASIAsw"]="(386.718750 12.897489 423.281250 48.922499)"
REGION_COORDINATES["ASIA_nw"]="(393.046875 46.800059 473.203125 81.621352)"

# South America
REGION_COORDINATES["SAfull"]="(271.230469 -57.040730 330.644531 15.114553)"

# Africa
REGION_COORDINATES["AFRfull"]="(339.082031 -37.718590 421.699219 39.232253)"

# ------------------------------------------------------------------------------
# get_region_coordinates()
# ------------------------------------------------------------------------------
# Sets XMIN, YMIN, XMAX, YMAX variables from the region definition for REGION_TAG.
# If REGION_TAG is not recognized, prints an error and returns 1.
#
# Usage:
#   export REGION_TAG="XYZ"
#   source region_defns.sh
#   get_region_coordinates  # => sets XMIN, YMIN, XMAX, YMAX
# ------------------------------------------------------------------------------
function get_region_coordinates() {
    local coords="${REGION_COORDINATES[$REGION_TAG]}"
    if [ -z "$coords" ]; then
        echo "ERROR: Unknown REGION_TAG: $REGION_TAG" >&2
        return 1
    fi
    
    # Parse the coordinate quadruple from parentheses
    read XMIN YMIN XMAX YMAX <<< "${coords//[()]/}"

    # Export them for use by the caller
    export XMIN YMIN XMAX YMAX
}

export -f get_region_coordinates

            </file>
            <file path="dbTools/export/v0/common/regional_base.sh" line_interval="25">
#!/bin/bash
# ------------------------------------------------------------------------------
# regional_base.sh
# ------------------------------------------------------------------------------
# Generates region-specific species tables and associated ancestor sets,
# factoring in the user's clade/metaclade and the major/minor rank mode.
#
# Steps:
#   1) Parse environment variables and region coordinates.
#   2) Build or reuse the <REGION_TAG>_min<MIN_OBS>_all_sp table (region + MIN_OBS only).
#   3) Parse clade condition (single or multi-root). If multi-root, check overlap.
#   4) Build or reuse <REGION_TAG>_min<MIN_OBS>_all_sp_and_ancestors_<cladeID>_<mode>
#   5) Build or reuse <REGION_TAG>_min<MIN_OBS>_sp_and_ancestors_obs_<cladeID>_<mode>
#   6) Output final info/summary
#
# Requires:
#   - environment variables: DB_NAME, DB_CONTAINER, DB_USER, ...
#   - script variables: REGION_TAG, MIN_OBS, SKIP_REGIONAL_BASE,
#     INCLUDE_OUT_OF_REGION_OBS, INCLUDE_MINOR_RANKS_IN_ANCESTORS,
#     etc.
#
# ------------------------------------------------------------------------------

source "${BASE_DIR}/common/functions.sh"
#|LN|25|
source "${BASE_DIR}/common/clade_defns.sh"
source "${BASE_DIR}/common/clade_helpers.sh"
source "${BASE_DIR}/common/region_defns.sh"

# ---------------------------------------------------------------------------
# 0) Validate Environment + Setup
# ---------------------------------------------------------------------------
: "${REGION_TAG:?Error: REGION_TAG is not set}"
: "${MIN_OBS:?Error: MIN_OBS is not set}"
: "${SKIP_REGIONAL_BASE:?Error: SKIP_REGIONAL_BASE is not set}"
: "${INCLUDE_OUT_OF_REGION_OBS:?Error: INCLUDE_OUT_OF_REGION_OBS is not set}"
: "${INCLUDE_MINOR_RANKS_IN_ANCESTORS:?Error: INCLUDE_MINOR_RANKS_IN_ANCESTORS is not set}"

# ---------------------------------------------------------------------------
# New Granular Control Flags
# ---------------------------------------------------------------------------
# If granular flags aren't explicitly set, default to the value of SKIP_REGIONAL_BASE
# This maintains backward compatibility
export SKIP_ALL_SP_TABLE="${SKIP_ALL_SP_TABLE:-$SKIP_REGIONAL_BASE}"
export SKIP_ANCESTORS_TABLE="${SKIP_ANCESTORS_TABLE:-$SKIP_REGIONAL_BASE}"

print_progress "=== regional_base.sh: Starting Ancestor-Aware Regional Base Generation ==="
print_progress "Using granular control flags: SKIP_ALL_SP_TABLE=${SKIP_ALL_SP_TABLE}, SKIP_ANCESTORS_TABLE=${SKIP_ANCESTORS_TABLE}"

# Retrieve bounding box for the region
#|LN|50|
get_region_coordinates || {
  echo "Failed to retrieve bounding box for REGION_TAG=${REGION_TAG}" >&2
  exit 1
}

print_progress "Using bounding box => XMIN=${XMIN}, YMIN=${YMIN}, XMAX=${XMAX}, YMAX=${YMAX}"

# ---------------------------------------------------------------------------
# 1) Build or Reuse <REGION_TAG>_min<MIN_OBS>_all_sp
# ---------------------------------------------------------------------------
ALL_SP_TABLE="${REGION_TAG}_min${MIN_OBS}_all_sp"

check_and_build_all_sp() {
  # Check existence
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ALL_SP_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    # If table exists, check row count
    local row_count
#|LN|75|
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ALL_SP_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ALL_SP_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_ALL_SP_TABLE}" = "true" ]; then
        print_progress "SKIP_ALL_SP_TABLE=true => reusing existing _all_sp table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating (or recreating) table \"${ALL_SP_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ALL_SP_TABLE}\" CASCADE;"

  # Build the table with bounding box + rank_level=10 + MIN_OBS filter
  execute_sql "
  CREATE TABLE \"${ALL_SP_TABLE}\" AS
  SELECT s.taxon_id
  FROM observations s
#|LN|100|
  JOIN taxa t ON t.taxon_id = s.taxon_id
  WHERE t.rank_level = 10
    AND s.quality_grade = 'research'
    AND s.geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
  GROUP BY s.taxon_id
  HAVING COUNT(s.observation_uuid) >= ${MIN_OBS};
  "
}

check_and_build_all_sp

# ---------------------------------------------------------------------------
# 2) Parse Clade Condition & Check Overlap if Multi-root
# ---------------------------------------------------------------------------
CLADE_CONDITION="$(get_clade_condition)"
print_progress "Clade Condition: ${CLADE_CONDITION}"

root_list=( $(parse_clade_expression "${CLADE_CONDITION}") )
root_count="${#root_list[@]}"
print_progress "Found ${root_count} root(s) from the clade condition"

# Decide on a short ID for the clade/metaclade
# (if you want to embed actual environment variables: e.g. $CLADE or $METACLADE
#  or parse the user-supplied string from the condition. We'll do a naive approach.)
if [ -n "${METACLADE}" ]; then
#|LN|125|
  CLADE_ID="${METACLADE}"
elif [ -n "${CLADE}" ]; then
  CLADE_ID="${CLADE}"
elif [ -n "${MACROCLADE}" ]; then
  CLADE_ID="${MACROCLADE}"
else
  # fallback if user didn't set anything
  CLADE_ID="universal"
fi

# Clean up the clade_id so it doesn't contain spaces or special chars
CLADE_ID="${CLADE_ID// /_}"

# If multi-root => check overlap
if [ "${root_count}" -gt 1 ]; then
  print_progress "Multiple roots => checking independence"
  check_root_independence "${DB_NAME}" "${root_list[@]}"
  if [ $? -ne 0 ]; then
    echo "ERROR: Overlap detected among metaclade roots. Aborting."
    exit 1
  fi
  print_progress "All roots are mutually independent"
fi

# Decide majorOrMinor string
#|LN|150|
if [ "${INCLUDE_MINOR_RANKS_IN_ANCESTORS}" = "true" ]; then
  RANK_MODE="inclMinor"
else
  RANK_MODE="majorOnly"
fi

# Build final table names
ANCESTORS_TABLE="${REGION_TAG}_min${MIN_OBS}_all_sp_and_ancestors_${CLADE_ID}_${RANK_MODE}"
ANCESTORS_OBS_TABLE="${REGION_TAG}_min${MIN_OBS}_sp_and_ancestors_obs_${CLADE_ID}_${RANK_MODE}"

# ---------------------------------------------------------------------------
# 3) Build or Reuse <REGION_TAG>_min<MIN_OBS>_all_sp_and_ancestors_<cladeID>_<mode>
# ---------------------------------------------------------------------------
check_and_build_ancestors() {
  # 1) Check if the table already exists and skip if user wants to skip ancestors
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ANCESTORS_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    local row_count
#|LN|175|
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ANCESTORS_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ANCESTORS_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_ANCESTORS_TABLE}" = "true" ]; then
        print_progress "SKIP_ANCESTORS_TABLE=true => reusing existing ancestors table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating table \"${ANCESTORS_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ANCESTORS_TABLE}\" CASCADE;"
  execute_sql "
  CREATE TABLE \"${ANCESTORS_TABLE}\" (
    taxon_id integer PRIMARY KEY
  );
  "

#|LN|200|
  local insert_ancestors_for_root
  insert_ancestors_for_root() {
    local root_pair="$1"  # e.g. "50=47158"
    local rank_part="${root_pair%%=*}"
    local root_taxid="${root_pair##*=}"

    local col_name="L${rank_part}_taxonID"

    # Decide boundary (majorOnly vs. inclMinor)
    local boundary_rank="$rank_part"
    if [ "${INCLUDE_MINOR_RANKS_IN_ANCESTORS}" = "false" ]; then
      boundary_rank="$(get_major_rank_floor "${rank_part}")"
    fi

    execute_sql "
    ----------------------------------------------------------------
    -- 1) Gather species from <ALL_SP_TABLE> that belong to this root
    ----------------------------------------------------------------
    DROP TABLE IF EXISTS temp_${root_taxid}_sp_list CASCADE;
    CREATE TEMP TABLE temp_${root_taxid}_sp_list AS
    SELECT s.taxon_id
    FROM \"${ALL_SP_TABLE}\" s
    JOIN expanded_taxa e ON e.\"taxonID\" = s.taxon_id
    WHERE e.\"${col_name}\" = ${root_taxid};

#|LN|225|
    ----------------------------------------------------------------
    -- 2) Unroll each species's ancestor IDs (L5..L70) and filter by rank
    ----------------------------------------------------------------
    DROP TABLE IF EXISTS temp_${root_taxid}_all_ancestors CASCADE;

    WITH unravel AS (
      -- 'unravel' yields each row's potential ancestor columns
      SELECT
        e.\"taxonID\"        AS sp_id,
        e.\"L5_taxonID\"     AS L5_id,
        e.\"L10_taxonID\"    AS L10_id,
        e.\"L11_taxonID\"    AS L11_id,
        e.\"L12_taxonID\"    AS L12_id,
        e.\"L13_taxonID\"    AS L13_id,
        e.\"L15_taxonID\"    AS L15_id,
        e.\"L20_taxonID\"    AS L20_id,
        e.\"L24_taxonID\"    AS L24_id,
        e.\"L25_taxonID\"    AS L25_id,
        e.\"L26_taxonID\"    AS L26_id,
        e.\"L27_taxonID\"    AS L27_id,
        e.\"L30_taxonID\"    AS L30_id,
        e.\"L32_taxonID\"    AS L32_id,
        e.\"L33_taxonID\"    AS L33_id,
        e.\"L33_5_taxonID\"  AS L33_5_id,
        e.\"L34_taxonID\"    AS L34_id,
#|LN|250|
        e.\"L34_5_taxonID\"  AS L34_5_id,
        e.\"L35_taxonID\"    AS L35_id,
        e.\"L37_taxonID\"    AS L37_id,
        e.\"L40_taxonID\"    AS L40_id,
        e.\"L43_taxonID\"    AS L43_id,
        e.\"L44_taxonID\"    AS L44_id,
        e.\"L45_taxonID\"    AS L45_id,
        e.\"L47_taxonID\"    AS L47_id,
        e.\"L50_taxonID\"    AS L50_id,
        e.\"L53_taxonID\"    AS L53_id,
        e.\"L57_taxonID\"    AS L57_id,
        e.\"L60_taxonID\"    AS L60_id,
        e.\"L67_taxonID\"    AS L67_id,
        e.\"L70_taxonID\"    AS L70_id
      FROM expanded_taxa e
      JOIN temp_${root_taxid}_sp_list sp
         ON e.\"taxonID\" = sp.taxon_id
    ),
    all_ancestors AS (
      -- We'll produce rows for the species' own ID (sp_id)
      -- plus each potential ancestor ID, then filter by rankLevel < boundary_rank.
      SELECT sp_id AS taxon_id
      FROM unravel

      UNION ALL
#|LN|275|

      SELECT x.\"taxonID\" AS taxon_id
      FROM unravel u
      CROSS JOIN LATERAL (VALUES
        (u.L5_id),(u.L10_id),(u.L11_id),(u.L12_id),(u.L13_id),(u.L15_id),
        (u.L20_id),(u.L24_id),(u.L25_id),(u.L26_id),(u.L27_id),(u.L30_id),
        (u.L32_id),(u.L33_id),(u.L33_5_id),(u.L34_id),(u.L34_5_id),(u.L35_id),
        (u.L37_id),(u.L40_id),(u.L43_id),(u.L44_id),(u.L45_id),(u.L47_id),
        (u.L50_id),(u.L53_id),(u.L57_id),(u.L60_id),(u.L67_id),(u.L70_id)
      ) anc(ancestor_id)
      JOIN expanded_taxa x ON x.\"taxonID\" = anc.ancestor_id
      WHERE x.\"rankLevel\" < ${boundary_rank}
    )
    SELECT DISTINCT taxon_id
    INTO TEMP temp_${root_taxid}_all_ancestors
    FROM all_ancestors
    WHERE taxon_id IS NOT NULL;

    ----------------------------------------------------------------
    -- 3) Insert into the final ancestors table
    ----------------------------------------------------------------
    INSERT INTO \"${ANCESTORS_TABLE}\"(taxon_id)
    SELECT DISTINCT taxon_id
    FROM temp_${root_taxid}_all_ancestors;
    "
#|LN|300|
  }

  # Decide single vs. multi-root
  if [ "${root_count}" -eq 0 ]; then
    print_progress "No recognized root => no ancestors inserted. (Might be 'TRUE' clade?)"
  elif [ "${root_count}" -eq 1 ]; then
    print_progress "Single root => straightforward insertion"
    insert_ancestors_for_root "${root_list[0]}"
  else
    print_progress "Multi-root => union each root's ancestor set"
    for root_entry in "${root_list[@]}"; do
      insert_ancestors_for_root "${root_entry}"
    done
  fi
}

check_and_build_ancestors

# ---------------------------------------------------------------------------
# 4) Build or Reuse <REGION_TAG>_min<MIN_OBS>_sp_and_ancestors_obs_<cladeID>_<mode>
# ---------------------------------------------------------------------------
check_and_build_ancestors_obs() {
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
#|LN|325|
    WHERE schemaname='public'
      AND tablename='${ANCESTORS_OBS_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    local row_count
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ANCESTORS_OBS_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ANCESTORS_OBS_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_ANCESTORS_TABLE}" = "true" ]; then
        print_progress "SKIP_ANCESTORS_TABLE=true => reusing existing ancestors_obs table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating table \"${ANCESTORS_OBS_TABLE}\""
#|LN|350|
  execute_sql "DROP TABLE IF EXISTS \"${ANCESTORS_OBS_TABLE}\" CASCADE;"

  local OBS_COLUMNS
  OBS_COLUMNS="$(get_obs_columns)"

  # ---------------------------------------------------------------------------
  # ADDED FEATURE: Always store an `in_region` boolean for each observation
  # ---------------------------------------------------------------------------
  # If INCLUDE_OUT_OF_REGION_OBS=true, we do NOT filter by bounding box in the
  # WHERE clause, but we compute a boolean:
  #    COALESCE(ST_Within(geom, ST_MakeEnvelope(...)), false) AS in_region
  #
  # If INCLUDE_OUT_OF_REGION_OBS=false, we do filter by bounding box
  #    AND geom && ST_MakeEnvelope(...)
  # and simply store in_region=TRUE for all rows.

  local BBOX="ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)"

  if [ "${INCLUDE_OUT_OF_REGION_OBS}" = "true" ]; then
    execute_sql "
    CREATE TABLE \"${ANCESTORS_OBS_TABLE}\" AS
    SELECT
      ${OBS_COLUMNS},
      COALESCE(ST_Within(geom, ${BBOX}), false) AS in_region
    FROM observations
#|LN|375|
    WHERE taxon_id IN (
      SELECT taxon_id
      FROM \"${ANCESTORS_TABLE}\"
    );
    "
  else
    execute_sql "
    CREATE TABLE \"${ANCESTORS_OBS_TABLE}\" AS
    SELECT
      ${OBS_COLUMNS},
      true AS in_region
    FROM observations
    WHERE taxon_id IN (
      SELECT taxon_id
      FROM \"${ANCESTORS_TABLE}\"
    )
    AND geom && ${BBOX};
    "
  fi
}

check_and_build_ancestors_obs

export ANCESTORS_OBS_TABLE="${ANCESTORS_OBS_TABLE}" # for cladistic.sh

#|LN|400|
print_progress "=== regional_base.sh: Completed building base tables for ${REGION_TAG}, minObs=${MIN_OBS}, clade=${CLADE_ID}, mode=${RANK_MODE} ==="
            </file>
          </dir>
          <dir path="dbTools/export/v0/r1">
            <file path="dbTools/export/v0/r1/wrapper.sh">
#!/bin/bash
#
# wrapper.sh
#
# A typical user-facing script that sets environment variables and then calls main.sh.
# We define WRAPPER_PATH="$0" so that main.sh can copy this file for reproducibility.

SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"

echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Provide path to wrapper for reproducibility
export WRAPPER_PATH="$0"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=200
export PRIMARY_ONLY=true

# We could set CLADE or METACLADE here; let's pick something as an example:
export CLADE="amphibia"
export EXPORT_GROUP="${CLADE}"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false

# NEW ENV VARS
export INCLUDE_OUT_OF_REGION_OBS=true
export RG_FILTER_MODE="ALL"

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"

"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"
            </file>
            <file path="dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_all_exc_nonrg_sp_inc_out_of_region"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true
# Include elevation_meters in the final dataset?
export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false # NOTE this differs from the other wrappers, export group name isn't accurate

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_amphibia_all_miniTest.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=200
export MAX_RN=800
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_all_miniTest"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=false    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true
# Include elevation_meters in the final dataset?
export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=200
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_angiospermae_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=60
export MAX_RN=1750
export PRIMARY_ONLY=true

export CLADE="angiospermae"
export EXPORT_GROUP="angiospermae_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=60
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_aves_all_exc_nonrg_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="aves"
export EXPORT_GROUP="aves_all_exc_nonrg_sp_inc_oor_full_ancestor_search_no_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# NOTE elevation not ready yet, exclude for now
export INCLUDE_ELEVATION_EXPORT=false
# TODO when ready, set to true, adjust EXPORT_GROUP to include elevation

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_aves_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="aves"
export EXPORT_GROUP="aves_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_aves_reuse_all_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="aves"
export EXPORT_GROUP="aves_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip All Species Table Creation: ${SKIP_ALL_SP_TABLE}"
log_message "Skip Ancestors Table Creation: ${SKIP_ANCESTORS_TABLE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!" 
            </file>
            <file path="dbTools/export/v0/r1/wrapper_mammalia_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="mammalia"
export EXPORT_GROUP="mammalia_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_pta.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

# We’ll use a metaclade here as an example
export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=false

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ONLY_RESEARCH"

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_pta_all_exc_nonrg_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2750
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp_inc_oor_full_ancestor_search_no_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# NOTE elevation not ready yet, exclude for now
export INCLUDE_ELEVATION_EXPORT=false
# TODO when ready, set to true, adjust EXPORT_GROUP to include elevation
# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_pta_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2750
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true
# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_pta_all_exc_nonrg_sp_inc_oor_fas_elev_mini.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=100
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp_inc_oor_fas_elev_mini"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true
# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_pta_non_rg.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

# We’ll use a metaclade here as an example
export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_non_rg"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=false

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ONLY_NONRESEARCH_WIPE_SPECIES_LABEL"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_reptilia.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=4000
export PRIMARY_ONLY=true
export CLADE="reptilia"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"
            </file>
            <file path="dbTools/export/v0/r1/wrapper_reptilia_all_exc_nonrg_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="reptilia"
export EXPORT_GROUP="reptilia_all_exc_nonrg_sp_inc_oor_full_ancestor_search_no_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# NOTE elevation not ready yet, exclude for now
export INCLUDE_ELEVATION_EXPORT=false
# TODO when ready, set to true, adjust EXPORT_GROUP to include elevation

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="dbTools/export/v0/r1/wrapper_reptilia_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="reptilia"
export EXPORT_GROUP="reptilia_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
          </dir>
        </dir>
      </dir>
      <dir path="dbTools/ingest">
        <dir path="dbTools/ingest/v0">
          <dir path="dbTools/ingest/v0/common">
            <file path="dbTools/ingest/v0/common/functions.sh">
#!bin/bash

## dbTools/ingest/v0/common/functions.sh

# Common functions used across ingest scripts (mostly mirrored from export functions.sh)

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    mkdir -p "${dir}"
    chmod -R 777 "${dir}"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f ensure_directory
export -f send_notification
            </file>
            <file path="dbTools/ingest/v0/common/geom.sh">
#!/bin/bash

# Function to run the update in parallel
run_update() {
  local OFFSET=$1
  local LIMIT=$2
  local DB_NAME=$3
  local TABLE_NAME=$4
  local DB_CONTAINER=$5

  docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -c "
  UPDATE ${TABLE_NAME}
  SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)::public.geometry
  WHERE observation_uuid IN (
    SELECT observation_uuid
    FROM ${TABLE_NAME}
    ORDER BY observation_uuid
    OFFSET ${OFFSET}
    LIMIT ${LIMIT}
  );"
}

# Check if correct number of arguments are provided
if [ "$#" -ne 4 ]; then
  echo "Usage: $0 <database_name> <table_name> <num_workers> <base_dir>"
  exit 1
fi

# Define arguments
DB_NAME=$1
TABLE_NAME=$2
NUM_PROCESSES=$3
BASE_DIR=$4

# Use container name from environment or default
DB_CONTAINER=${DB_CONTAINER:-"ibridaDB"}

# Calculate total rows and batch size
TOTAL_ROWS=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};")
BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES))

# Run updates in parallel
for ((i=0; i<NUM_PROCESSES; i++)); do
  OFFSET=$((i * BATCH_SIZE))
  run_update ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${TABLE_NAME} ${DB_CONTAINER} &
done

# Wait for all processes to finish
wait
echo "All updates completed."
            </file>
            <file path="dbTools/ingest/v0/common/main.sh" line_interval="25">
#!/bin/bash
#
# main.sh
#
# Core ingestion logic for a given database release. Creates the database,
# imports CSV data, configures geometry, version columns, etc. Now also
# optionally calls the elevation pipeline if ENABLE_ELEVATION=true.
#
# This script expects the following variables to be set by the wrapper:
#   - DB_USER
#   - DB_TEMPLATE
#   - NUM_PROCESSES
#   - BASE_DIR
#   - SOURCE
#   - ORIGIN_VALUE
#   - VERSION_VALUE
#   - RELEASE_VALUE
#   - DB_NAME
#   - DB_CONTAINER
#   - METADATA_PATH
#   - STRUCTURE_SQL
#   - ENABLE_ELEVATION (new; optional, defaults to "false" if not set)
#
# Example usage:
#|LN|25|
#   ENABLE_ELEVATION=true /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
#

set -euo pipefail

# ------------------------------------------------------------------------------
# 1. Validate required variables
# ------------------------------------------------------------------------------
required_vars=(
    "DB_USER" "DB_TEMPLATE" "NUM_PROCESSES" "BASE_DIR"
    "SOURCE" "ORIGIN_VALUE" "VERSION_VALUE" "DB_NAME"
    "DB_CONTAINER" "METADATA_PATH" "STRUCTURE_SQL"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var:-}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Default ENABLE_ELEVATION to "false" if not defined
ENABLE_ELEVATION="${ENABLE_ELEVATION:-false}"

# ------------------------------------------------------------------------------
#|LN|50|
# 2. Source shared functions
# ------------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

print_progress "Starting core ingestion for ${DB_NAME}"
send_notification "[INFO] Starting ingestion for ${DB_NAME}"

# ------------------------------------------------------------------------------
# 3. Create Database
# ------------------------------------------------------------------------------
print_progress "Creating database ${DB_NAME} from template ${DB_TEMPLATE}"
execute_sql_postgres() {
    local sql="$1"
    docker exec "${DB_CONTAINER}" psql -U "${DB_USER}" -c "$sql"
}

execute_sql_postgres "DROP DATABASE IF EXISTS \"${DB_NAME}\";"
execute_sql_postgres "CREATE DATABASE \"${DB_NAME}\" WITH TEMPLATE ${DB_TEMPLATE} OWNER ${DB_USER};"

# ------------------------------------------------------------------------------
# 4. Create tables from structure SQL
# ------------------------------------------------------------------------------
print_progress "Creating tables from ${STRUCTURE_SQL}"
if [ ! -f "${STRUCTURE_SQL}" ]; then
  echo "Error: STRUCTURE_SQL file not found: ${STRUCTURE_SQL}"
#|LN|75|
  exit 1
fi

cat "${STRUCTURE_SQL}" | docker exec -i "${DB_CONTAINER}" psql -U "${DB_USER}" -d "${DB_NAME}"

# ------------------------------------------------------------------------------
# 5. Import data
# ------------------------------------------------------------------------------
print_progress "Importing CSV data from ${METADATA_PATH}"
execute_sql "
BEGIN;

COPY observations
FROM '${METADATA_PATH}/observations.csv'
DELIMITER E'\t'
QUOTE E'\b'
CSV HEADER;

COPY photos
FROM '${METADATA_PATH}/photos.csv'
DELIMITER E'\t'
QUOTE E'\b'
CSV HEADER;

COPY taxa
#|LN|100|
FROM '${METADATA_PATH}/taxa.csv'
DELIMITER E'\t'
QUOTE E'\b'
CSV HEADER;

COPY observers
FROM '${METADATA_PATH}/observers.csv'
DELIMITER E'\t'
QUOTE E'\b'
CSV HEADER;

COMMIT;
"

# ------------------------------------------------------------------------------
# 6. Create indexes
# ------------------------------------------------------------------------------
print_progress "Creating base indexes"
execute_sql "
BEGIN;

CREATE INDEX index_photos_photo_uuid         ON photos USING btree (photo_uuid);
CREATE INDEX index_photos_observation_uuid   ON photos USING btree (observation_uuid);
CREATE INDEX index_photos_position           ON photos USING btree (position);
CREATE INDEX index_photos_photo_id           ON photos USING btree (photo_id);
#|LN|125|
CREATE INDEX index_taxa_taxon_id             ON taxa   USING btree (taxon_id);
CREATE INDEX index_observers_observers_id    ON observers USING btree (observer_id);
CREATE INDEX index_observations_observer_id  ON observations USING btree (observer_id);
CREATE INDEX index_observations_quality      ON observations USING btree (quality_grade);
CREATE INDEX index_observations_taxon_id     ON observations USING btree (taxon_id);
CREATE INDEX index_taxa_active               ON taxa USING btree (active);

COMMIT;
"

# Conditional index for anomaly_score
execute_sql "
DO \$\$
BEGIN
    IF EXISTS (
        SELECT 1
        FROM information_schema.columns
        WHERE table_name = 'observations'
        AND column_name = 'anomaly_score'
    ) THEN
        CREATE INDEX idx_observations_anomaly ON observations (anomaly_score);
    END IF;
END \$\$;"

# ------------------------------------------------------------------------------
#|LN|150|
# 7. Add geom column & compute geometry in parallel
# ------------------------------------------------------------------------------
print_progress "Adding geom column to observations"
execute_sql "ALTER TABLE observations ADD COLUMN geom public.geometry;"

print_progress "Populating geom column in parallel"
"${BASE_DIR}/common/geom.sh" "${DB_NAME}" "observations" "${NUM_PROCESSES}" "${BASE_DIR}"

# Create geom index
print_progress "Creating GIST index on geom"
execute_sql "CREATE INDEX observations_geom ON observations USING GIST (geom);"

# ------------------------------------------------------------------------------
# 8. Vacuum
# ------------------------------------------------------------------------------
print_progress "Vacuum/Analyze after geometry load"
execute_sql "VACUUM ANALYZE;"

# ------------------------------------------------------------------------------
# 9. Add origin, version, and release columns
# ------------------------------------------------------------------------------
print_progress "Adding origin/version/release columns in parallel"
execute_sql "
BEGIN;

#|LN|175|
ALTER TABLE taxa         ADD COLUMN origin   VARCHAR(255);
ALTER TABLE observers    ADD COLUMN origin   VARCHAR(255);
ALTER TABLE observations ADD COLUMN origin   VARCHAR(255);
ALTER TABLE photos       ADD COLUMN origin   VARCHAR(255);

ALTER TABLE photos       ADD COLUMN version  VARCHAR(255);
ALTER TABLE observations ADD COLUMN version  VARCHAR(255);
ALTER TABLE observers    ADD COLUMN version  VARCHAR(255);
ALTER TABLE taxa         ADD COLUMN version  VARCHAR(255);

ALTER TABLE photos       ADD COLUMN release  VARCHAR(255);
ALTER TABLE observations ADD COLUMN release  VARCHAR(255);
ALTER TABLE observers    ADD COLUMN release  VARCHAR(255);
ALTER TABLE taxa         ADD COLUMN release  VARCHAR(255);

COMMIT;
"

print_progress "Populating origin/version/release columns"
"${BASE_DIR}/common/vers_origin.sh" "${DB_NAME}" "${NUM_PROCESSES}" "${ORIGIN_VALUE}" "${VERSION_VALUE}" "${RELEASE_VALUE}"

# ------------------------------------------------------------------------------
# 10. Create GIN indexes for origin/version/release
# ------------------------------------------------------------------------------
print_progress "Creating GIN indexes for origin/version/release"
#|LN|200|
execute_sql "
BEGIN;

CREATE INDEX index_taxa_origins        ON taxa        USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_taxa_name           ON taxa        USING GIN (to_tsvector('simple', name));
CREATE INDEX index_observers_origins   ON observers   USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_observations_origins ON observations USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_origins      ON photos      USING GIN (to_tsvector('simple', origin));

CREATE INDEX index_photos_version      ON photos      USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observations_version ON observations USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observers_version   ON observers   USING GIN (to_tsvector('simple', version));
CREATE INDEX index_taxa_version        ON taxa        USING GIN (to_tsvector('simple', version));

CREATE INDEX index_photos_release      ON photos      USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observations_release ON observations USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observers_release   ON observers   USING GIN (to_tsvector('simple', release));
CREATE INDEX index_taxa_release        ON taxa        USING GIN (to_tsvector('simple', release));

COMMIT;
"

# ------------------------------------------------------------------------------
# 11. Optional Elevation Flow
# ------------------------------------------------------------------------------
#|LN|225|
if [ "${ENABLE_ELEVATION}" == "true" ]; then
  print_progress "ENABLE_ELEVATION=true, proceeding with elevation pipeline"
  send_notification "[INFO] Elevation pipeline triggered for ${DB_NAME}"

  # Either call the 'wrapper.sh' or call 'main.sh' directly.
  # We'll illustrate direct call to main.sh here: (note; makes sense to direct call here, wrapper is for standalone use)
  ELEVATION_SCRIPT="${BASE_DIR}/utils/elevation/main.sh"

  # Example: pass your dem directory, concurrency, etc. 
  # If your release wrapper already sets DEM_DIR, EPSG, etc. environment variables, you can do:
  DEM_DIR="${DEM_DIR:-"/datasets/dem/merit"}"
  EPSG="${EPSG:-"4326"}"
  TILE_SIZE="${TILE_SIZE:-"100x100"}"

  if [ -x "${ELEVATION_SCRIPT}" ]; then
    "${ELEVATION_SCRIPT}" \
      "${DB_NAME}" \
      "${DB_USER}" \
      "${DB_CONTAINER}" \
      "${DEM_DIR}" \
      "${NUM_PROCESSES}" \
      "${EPSG}" \
      "${TILE_SIZE}"
  else
    echo "Warning: Elevation script not found or not executable at ${ELEVATION_SCRIPT}"
#|LN|250|
  fi

  print_progress "Elevation pipeline complete for ${DB_NAME}"
else
  print_progress "ENABLE_ELEVATION=false, skipping elevation pipeline"
  send_notification "[INFO] Skipping elevation pipeline for ${DB_NAME}"
fi

# ------------------------------------------------------------------------------
# 12. Final notice
# ------------------------------------------------------------------------------
print_progress "Database setup complete for ${DB_NAME}"
send_notification "[OK] Ingestion (and optional elevation) complete for ${DB_NAME}"
            </file>
            <file path="dbTools/ingest/v0/common/vers_origin.sh" line_interval="25">
#!/bin/bash

# COMMENT: populates origin, version, and release columns in parallel
# COMMENT: these are the only columns on the base tables that are not populated by the ingest process

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/vers_origin_$(date +%Y%m%d_%H%M%S).log"
echo "Starting version/origin/release updates at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "${LOG_FILE}"
}

# Function for error logging and exit
error_exit() {
    log_message "ERROR: $1"
    exit 1
}

# Function to run the update in parallel
run_update() {
    local TABLE_NAME=$1
#|LN|25|
    local COLUMN_NAME=$2
    local VALUE=$3
    local OFFSET=$4
    local LIMIT=$5
    local DB_NAME=$6
    local DB_CONTAINER=$7
    local PROCESS_NUM=$8

    log_message "Process $PROCESS_NUM: Updating $TABLE_NAME.$COLUMN_NAME (offset: $OFFSET, limit: $LIMIT)"
    
    UPDATE_RESULT=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "
    UPDATE ${TABLE_NAME}
    SET ${COLUMN_NAME} = '${VALUE}'
    WHERE ctid IN (
        SELECT ctid
        FROM ${TABLE_NAME}
        ORDER BY ctid
        OFFSET ${OFFSET}
        LIMIT ${LIMIT}
    );")
    
    if [ $? -ne 0 ]; then
        error_exit "Failed to update ${TABLE_NAME}.${COLUMN_NAME} in process $PROCESS_NUM"
    fi
    
#|LN|50|
    log_message "Process $PROCESS_NUM: Completed update of $TABLE_NAME.$COLUMN_NAME"
}

# Validate arguments
if [ "$#" -ne 5 ]; then
    error_exit "Usage: $0 <database_name> <num_workers> <origin_value> <version_value> <release_value>"
fi

# Define arguments
DB_NAME=$1
NUM_PROCESSES=$2
ORIGIN_VALUE=$3
VERSION_VALUE=$4
RELEASE_VALUE=$5

# Validate NUM_PROCESSES is a positive integer
if ! [[ "$NUM_PROCESSES" =~ ^[1-9][0-9]*$ ]]; then
    error_exit "Number of workers must be a positive integer"
fi

# Use container name from environment or default
DB_CONTAINER=${DB_CONTAINER:-"ibridaDB"}

# Verify database exists
if ! docker exec ${DB_CONTAINER} psql -U postgres -lqt | cut -d \| -f 1 | grep -qw "${DB_NAME}"; then
#|LN|75|
    error_exit "Database ${DB_NAME} does not exist"
fi

# Tables and their columns to update
declare -A TABLES_COLUMNS
TABLES_COLUMNS=(
    ["taxa"]="origin,version,release"
    ["observers"]="origin,version,release"
    ["observations"]="origin,version,release"
    ["photos"]="origin,version,release"
)

# Function to update columns in parallel
update_columns_in_parallel() {
    local TABLE_NAME=$1
    local COLUMN_NAME=$2
    local VALUE=$3
    local TOTAL_ROWS

    # Verify table exists
    if ! docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -c "\d ${TABLE_NAME}" &>/dev/null; then
        error_exit "Table ${TABLE_NAME} does not exist in database ${DB_NAME}"
    }

    # Get total rows with error handling
#|LN|100|
    TOTAL_ROWS=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};" | tr -d ' ')
    if [ $? -ne 0 ] || ! [[ "$TOTAL_ROWS" =~ ^[0-9]+$ ]]; then
        error_exit "Failed to get row count for ${TABLE_NAME}"
    }

    log_message "Starting parallel update of ${TABLE_NAME}.${COLUMN_NAME} (${TOTAL_ROWS} total rows)"
    
    local BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES + 1))
    local PIDS=()

    for ((i=0; i<NUM_PROCESSES; i++)); do
        local OFFSET=$((i * BATCH_SIZE))
        run_update ${TABLE_NAME} ${COLUMN_NAME} ${VALUE} ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${DB_CONTAINER} $i &
        PIDS+=($!)
    done

    # Wait for all processes and check their exit status
    for pid in "${PIDS[@]}"; do
        if ! wait $pid; then
            error_exit "One of the parallel update processes failed"
        fi
    done
    
    log_message "Completed update of ${TABLE_NAME}.${COLUMN_NAME}"
}
#|LN|125|

# Main update process
log_message "Starting updates with parameters:"
log_message "Database: ${DB_NAME}"
log_message "Number of processes: ${NUM_PROCESSES}"
log_message "Origin value: ${ORIGIN_VALUE}"
log_message "Version value: ${VERSION_VALUE}"
log_message "Release value: ${RELEASE_VALUE}"

for TABLE_NAME in "${!TABLES_COLUMNS[@]}"; do
    log_message "Processing table: ${TABLE_NAME}"
    IFS=',' read -ra COLUMNS <<< "${TABLES_COLUMNS[$TABLE_NAME]}"
    for COLUMN in "${COLUMNS[@]}"; do
        case "$COLUMN" in
            "origin")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$ORIGIN_VALUE"
                ;;
            "version")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$VERSION_VALUE"
                ;;
            "release")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$RELEASE_VALUE"
                ;;
        esac
    done
#|LN|150|
done

log_message "All updates completed successfully"
            </file>
          </dir>
          <dir path="dbTools/ingest/v0/r1">
            <file path="dbTools/ingest/v0/r1/structure.sql">
-- Structure for v0r1 (December 2024 release)
-- Note: anomaly_score column added in r1, not present in r0

CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date,
    anomaly_score numeric(15,6)  -- New column in r1
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

-- Note: The following columns are added by our ingestion process:
-- All tables:
--   origin VARCHAR(255)
--   version VARCHAR(255)
--   release VARCHAR(255)
-- Observations table:
--   geom public.geometry
            </file>
            <file path="dbTools/ingest/v0/r1/wrapper.sh">
#!/bin/bash

### REVIEW: Previous run didn't populate version/origin columns. We applied a fix to vers_origin.sh (argument mismatch) but watch logs carefully next run.

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing ingest process with configuration:"

# Database and user variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"
log_message "Database User: ${DB_USER}"
log_message "Template DB: ${DB_TEMPLATE}"
log_message "Parallel Processes: ${NUM_PROCESSES}"

# Source variable
export SOURCE="Dec2024"
export METADATA_PATH="/metadata/${SOURCE}"
log_message "Source: ${SOURCE}"
log_message "Metadata Path: ${METADATA_PATH}"

# Version and origin values
export ORIGIN_VALUE="iNat-${SOURCE}"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r1/structure.sql"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"
log_message "Origin: ${ORIGIN_VALUE}"
log_message "Structure SQL: ${STRUCTURE_SQL}"

# Execute main script
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
            </file>
          </dir>
        </dir>
      </dir>
      <dir path="dbTools/taxa">
        <file path="dbTools/taxa/ColDP_raw_samples.txt">
<Distribution.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:areaID	col:area	col:gazetteer	col:status	col:referenceID	col:remarks
BLL2J		CN		iso	native		
BLL2J		TW		iso	native		
3LWT2		CR		iso	native		
444RB			Uruguay (Rivera)	text	native		
69PYD			North America	text	native		
8VVZD		MXC		tdwg	native		
8VVZD		MXG		tdwg	native		
8VVZD		MXE		tdwg	native		
8VVZD		MXT		tdwg	native		
</Distribution.tsv>
</head -n 10>

<Media.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:url	col:type	col:format	col:title	col:created	col:creator	col:license	col:link	col:remarks
</Media.tsv>
</head -n 10>

<metadata.yaml>
<head -n 10>
---
key: 299029
doi: 10.48580/dg9ld
title: Catalogue of Life
alias: COL24
description: |-
  The Catalogue of Life is an assembly of expert-based global species checklists with the aim to build a comprehensive catalogue of all known species of organisms on Earth. Continuous progress is made towards completion, but for now, it probably includes just over 80% of the world's known species. The Catalogue of Life estimates 2.3M extant species on the planet recognised by taxonomists at present time. This means that for many groups it continues to be deficient, and users may notice that many species are still missing from the Catalogue.

  ### What's new in 2024 Annual Checklist?

</metadata.yaml>
</head -n 10>

<NameRelation.tsv>
<head -n 10>
col:nameID	col:relatedNameID	col:sourceID	col:type	col:referenceID	col:page	col:remarks
CX5Y	6ZBQ2		basionym			
4DQ2L	6ZDLT		basionym			
4DQ2M	6ZDLY		basionym			
85QXH	BKZY		basionym			
4J2RF	6XKWB		basionym			
67CMV	CDYT5		basionym			
67CN7	4KGYM		basionym			
5VCMV	3L8K9		basionym			
67CMQ	6DV6D		basionym			
</NameRelation.tsv>
</head -n 10>

<NameUsage.tsv>
<head -n 10>
col:ID	col:alternativeID	col:nameAlternativeID	col:sourceID	col:parentID	col:basionymID	col:status	col:scientificName	col:authorship	col:rank	col:notho	col:originalSpelling	col:uninomial	col:genericName	col:infragenericEpithet	col:specificEpithet	col:infraspecificEpithet	col:cultivarEpithet	col:combinationAuthorship	col:combinationAuthorshipID	col:combinationExAuthorship	col:combinationExAuthorshipID	col:combinationAuthorshipYear	col:basionymAuthorship	col:basionymAuthorshipID	col:basionymExAuthorship	col:basionymExAuthorshipID	col:basionymAuthorshipYear	col:namePhrase	col:nameReferenceID	col:publishedInYear	col:publishedInPage	col:publishedInPageLink	col:gender	col:genderAgreement	col:etymology	col:code	col:nameStatus	col:accordingToID	col:accordingToPage	col:accordingToPageLink	col:referenceID	col:scrutinizer	col:scrutinizerID	col:scrutinizerDate	col:extinct	col:temporalRangeStart	col:temporalRangeEnd	col:environment	col:species	col:section	col:subgenus	col:genus	col:subtribe	col:tribe	col:subfamily	col:family	col:superfamily	col:suborder	col:order	col:subclass	col:class	col:subphylum	col:phylum	col:kingdom	col:ordinal	col:branchLength	col:link	col:nameRemarks	col:remarks
673FW			2232	3CP83	3CP83	synonym	Anisophyllum hyssopifolium	(L.) Haw.	species				Anisophyllum		hyssopifolium			Haw.					L.						0a8270ab-617f-402d-b3bf-c3f0fe6b0a0a							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:1042669-2		
9L2J9			2232	3F5B8	3F5B8	synonym	Asperula aparine var. aparine		variety				Asperula		aparine	aparine													308d3079-d3d3-4523-af56-80e4507eade8							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:77168438-1		
8RX53			1130	9WRMR		synonym	Saccoia escoffierae var. triangulata	Sacco, 1895	variety				Saccoia		escoffierae	triangulata		Sacco				1895							75d294da-5cc2-4241-b178-c5686ae93d97		42, pl. 1, fig. 116					zoological					28419c41-a4be-4567-9131-512c920576bd																										https://www.molluscabase.org/aphia.php?p=taxdetails&id=1545847		
555CQ			1141	C9FM		synonym	Telanthera flavescens	(Kunth) Moq.	species				Telanthera		flavescens			Moq.					Kunth						c59b73dc-e818-4ec0-8131-4f317c9139c6							botanical																																	
7TDYP			1130	3PP2B	9BD4Q	synonym	Triphora sarissa	Dall, 1889	species				Triphora		sarissa			Dall				1889														zoological																															https://www.molluscabase.org/aphia.php?p=taxdetails&id=1481479		
C33N7			1175	858KS		accepted	Amalocythere fulgida	Guan, 1978	species				Amalocythere		fulgida			Guan				1978							8db3f7fe-e3df-49ec-978c-717af25cea87							zoological	established				8db3f7fe-e3df-49ec-978c-717af25cea87	Brandão, Simone Nunes		2016-06-30	true			marine																			https://www.marinespecies.org/ostracoda/aphia.php?p=taxdetails&id=773107		
85BKX			2144	84JVC		accepted	Coronalpheus	Wicksten, 1999	genus			Coronalpheus						Wicksten				1999														zoological	acceptable				bc596268-7fff-4a3c-868b-3c392a7be32b			2021-04-26	false																						https://www.itis.gov/servlet/SingleRpt/SingleRpt?search_topic=TSN&search_value=1147818		
B3XCS			1141	5ZXM8		ambiguous synonym	Convolvulus rupestris	Buch	species				Convolvulus		rupestris			Buch											5f264aa0-ccf7-4da8-ad97-0ef3f4913f7e		193					botanical																																	
3HNHY			2232	8VXZW		accepted	Guzmania variegata	L.B.Sm.	species				Guzmania		variegata			L.B.Sm.											6683ea9d-d407-472a-8daa-943b551b3eaf							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:115257-2		Ecuador to Peru
</NameUsage.tsv>
</head -n 10>

<reference.json>
<head -n 10>
[
{"id":"1382","author":[{"family":"Weiss, Norbert, ed."}],"issued":{"date-parts":[[2001]]},"title":"Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM","container-title":"International Journal of Systematic and Evolutionary Microbiology, vol. 51, no. 2"},
{"id":"1383","author":[{"family":"Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al."}],"issued":{"date-parts":[[1992]]},"title":"Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b","container-title":"Archives in Microbiology, vol. 157"},
{"id":"1384","author":[{"family":"Partensky, F., W. R. Hess, and D. Vaulot"}],"issued":{"date-parts":[[1999]]},"title":"Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance","container-title":"Microbiology and Molecular Biology Reviews, vol. 63, no. 1"},
{"id":"171","author":[{"family":"Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.)"}],"issued":{"date-parts":[[1986]]},"container-title":"Bergey's Manual of Systematic Bacteriology. vol. 2"},
{"id":"1331","author":[{"family":"Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds."}],"issued":{"date-parts":[[1980]]},"title":"Approved Lists of Bacterial Names","container-title":"International Journal of Systematic Bacteriology, vol. 30, no.1"},
{"id":"4731","author":[{"family":"Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds."}],"issued":{"date-parts":[[1989]]},"container-title":"Approved Lists of Bacterial Names"},
{"author":[{"family":"S"}],"issued":{"date-parts":[[2000,1,1]]},"title":"SN2000/Levine, 1982"},
{"author":[{"family":"S"}],"issued":{"date-parts":[[2000,1,1]]},"title":"SN2000/Perkins, 2000"},
{"title":"As per Gigaductus"},
</reference.json>
</head -n 10>

<Reference.tsv>
<head -n 10>
col:ID	col:alternativeID	col:sourceID	col:citation	col:type	col:author	col:editor	col:title	col:titleShort	col:containerAuthor	col:containerTitle	col:containerTitleShort	col:issued	col:accessed	col:collectionTitle	col:collectionEditor	col:volume	col:issue	col:edition	col:page	col:publisher	col:publisherPlace	col:version	col:isbn	col:issn	col:doi	col:link	col:remarks
12ddc4b3-d551-4c38-8d20-4a2246b9e8d6		2144	Weiss, Norbert, ed. (2001). Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM. International Journal of Systematic and Evolutionary Microbiology, Vol. 51, No. 2.		Weiss, Norbert, ed.		Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM			International Journal of Systematic and Evolutionary Microbiology, vol. 51, no. 2		2001															
5eb14d90-e68f-4fda-be6c-453c6965707b		2144	Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al. (1992). Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b. Archives in Microbiology, Vol. 157.		Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al.		Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b			Archives in Microbiology, vol. 157		1992															
6b833931-90b9-4dca-b98c-d3e300075c7e		2144	Partensky, F., W. R. Hess, and D. Vaulot. (1999). Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance. Microbiology and Molecular Biology Reviews, Vol. 63, No. 1.		Partensky, F., W. R. Hess, and D. Vaulot		Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance			Microbiology and Molecular Biology Reviews, vol. 63, no. 1		1999															
c28f7f52-c223-4410-90ec-9142da3e34ab		2144	Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.). (1986). Bergey’s Manual of Systematic Bacteriology. Vol. 2.		Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.)					Bergey's Manual of Systematic Bacteriology. vol. 2		1986															
fa90c9ed-a347-4ae8-9feb-d2dec6bd8f97		2144	Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds. (1980). Approved Lists of Bacterial Names. International Journal of Systematic Bacteriology, Vol. 30, no.1.		Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds.		Approved Lists of Bacterial Names			International Journal of Systematic Bacteriology, vol. 30, no.1		1980															
be12e305-5fb7-4184-b33d-b6601e9c4140		2144	Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds. (1989). Approved Lists of Bacterial Names.		Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds.					Approved Lists of Bacterial Names		1989															
c75a73a2-0c85-4d63-a46a-c7f46d5f7913		2007	Seenivasan, R.; Sausen, N.; Medlin, L. K.; Melkonian, M. (2013). Picomonas judraskeda gen. et sp. nov.: the first identified member of the Picozoa phylum nov., a widespread group of picoeukaryotes, formerly known as 'picobiliphytes'. PLoS ONE. 8(3): e59565.																								
2ea6bf13-11f2-474f-b9ab-4e30333d3ade		2007	Article title: Phylogeny of novel naked filose and reticulose Cercozoa: Granofilosea cl. n. and Proteomyxidea revised.																								
535b1b3f-d59c-4da8-bc3b-abf664243b17		2007	Yabuki, A.; Chao, E. E.; Ishida, K.-I.; Cavalier-Smith, T. (2012). Microheliella maris (Microhelida ord. n.), an ultrastructurally highly distinctive new axopodial protist species and genus, and the unity of phylum Heliozoa. Protist. 163(3): 356-388.																								
</Reference.tsv>
</head -n 10>

<SpeciesEstimate.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:estimate	col:type	col:referenceID	col:remarks
BRKNQ		5385	species living	74	
622DH		317	species living	30	
4XK		85	species living	74	
FJ5		37	species living	30	
625XT		2	species living	55	
HJR		93	species living	30	
9LT		46	species living	30	
CZK		79	species living	55	
8NKJ8		71	species living	29	
</SpeciesEstimate.tsv>
</head -n 10>

<SpeciesInteraction.tsv>
<head -n 10>
col:taxonID	col:relatedTaxonID	col:sourceID	col:relatedTaxonScientificName	col:type	col:referenceID	col:remarks
</SpeciesInteraction.tsv>
</head -n 10>

<TaxonConceptRelation.tsv>
<head -n 10>
col:taxonID	col:relatedTaxonID	col:sourceID	col:type	col:referenceID	col:remarks
</TaxonConceptRelation.tsv>
</head -n 10>

<TaxonProperty.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:property	col:value	col:ordinal	col:referenceID	col:page	col:remarks
</TaxonProperty.tsv>
</head -n 10>

<TypeMaterial.tsv>
<head -n 10>
col:ID	col:nameID	col:sourceID	col:citation	col:status	col:referenceID	col:page	col:country	col:locality	col:latitude	col:longitude	col:altitude	col:sex	col:host	col:associatedSequences	col:date	col:collector	col:institutionCode	col:catalogNumber	col:link	col:remarks
</TypeMaterial.tsv>
</head -n 10>

<VernacularName.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:name	col:transliteration	col:language	col:preferred	col:country	col:area	col:sex	col:referenceID	col:remarks
B6LM6		bacteria	bacteria	eng						
B6LM6		bacterias	bacterias	spa						
R5LB		Sandcarpet	Sandcarpet	eng						
76SYT		Alcajes	Alcajes							
76SYT		Rajamatraca	Rajamatraca							
333V8		Patana	Patana	cbq						
6C59B		Clavellina	Clavellina							
3XSNW		Pitayita	Pitayita							
3XSJY		Cochilinque	Cochilinque							
</VernacularName.tsv>
</head -n 10>

<source/1005.yaml>
<head -n 50>
---
key: 1005
doi: 10.48580/dfrdl-37p
title: Catalogue of Craneflies of the World
alias: CCW
description: "The Catalogue of the Craneflies of the World (CCW) covers all genera,\
  \ subgenera, species, subspecies, and synonyms of the World Tipuloidea (Insecta\
  \ – Diptera – Nematocera – families Pediciidae, Limoniidae, Cylindrotomidae, Tipulidae).\
  \ It also includes up to date information on the distribution of the species and\
  \ subspecies, specified by countries and, for the larger countries, states, provinces\
  \ and islands. The website’s list of references has over 7.400 titles, about 4.300\
  \ of which available as easy downloadable pdf."
issued: 2021-05-07
version: May 2021
contact:
  given: P
  family: Oosterbroek
creator:
 -
  given: P.
  family: Oosterbroek
contributor:
 -
  city: Leiden
  country: NL
  address: "Leiden, Netherlands"
  organisation: Naturalis Biodiversity Center
keyword: []
containerKey: 299029
containerTitle: Catalogue of Life
containerCreator:
 -
  orcid: 0000-0001-6197-9951
  given: Olaf
  family: Bánki
  city: Amsterdam
  country: NL
  note: COL Managing Director
  address: "Amsterdam, Netherlands"
  organisation: Catalogue of Life
 -
  orcid: 0000-0003-2137-2690
  given: Yury
  family: Roskov
  city: Champaign
  state: IL
  country: US
  note: COL Executive Editor
  address: "Champaign, IL, United States of America"
  organisation: Illinois Natural History Survey				
</source/1005.yaml>
</head -n 50>

        </file>
        <file path="dbTools/taxa/__init__.py">

        </file>
        <file path="dbTools/taxa/analysis_utils.py">
from ibridaDB.schema import Observations, Taxa, TaxaTemp

def count_new_taxa(session, taxon_id):
    return session.query(TaxaTemp).filter(TaxaTemp.taxon_id == taxon_id, ~session.query(Taxa).filter(Taxa.taxon_id == taxon_id).exists()).count()

def count_deprecated_taxa(session, taxon_id):
    return session.query(Taxa).filter(Taxa.taxon_id == taxon_id, ~session.query(TaxaTemp).filter(TaxaTemp.taxon_id == taxon_id).exists()).count()

def count_active_status_changes(session, taxon_id):
    return session.query(Taxa, TaxaTemp).filter(
        Taxa.taxon_id == taxon_id,
        TaxaTemp.taxon_id == taxon_id,
        Taxa.active != TaxaTemp.active
    ).count()

def count_name_changes(session, taxon_id):
    return session.query(Taxa, TaxaTemp).filter(
        Taxa.taxon_id == taxon_id,
        TaxaTemp.taxon_id == taxon_id,
        Taxa.name != TaxaTemp.name
    ).count()

def count_other_attribute_changes(session, taxon_id):
    return session.query(Taxa, TaxaTemp).filter(
        Taxa.taxon_id == taxon_id,
        TaxaTemp.taxon_id == taxon_id,
        (Taxa.ancestry != TaxaTemp.ancestry) |
        (Taxa.rank_level != TaxaTemp.rank_level) |
        (Taxa.rank != TaxaTemp.rank)
    ).count()

def count_observations_for_taxa(session, taxon_id, category):
    if category == 'new':
        return session.query(Observations).join(TaxaTemp, Observations.taxon_id == TaxaTemp.taxon_id).filter(
            TaxaTemp.taxon_id == taxon_id,
            ~session.query(Taxa).filter(Taxa.taxon_id == taxon_id).exists()
        ).count()
    elif category == 'deprecated':
        return session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).filter(
            Taxa.taxon_id == taxon_id,
            ~session.query(TaxaTemp).filter(TaxaTemp.taxon_id == taxon_id).exists()
        ).count()
    elif category == 'active_status_changes':
        return session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
            Taxa.taxon_id == taxon_id,
            Taxa.active != TaxaTemp.active
        ).count()

def count_observations_for_common_taxa(session, taxon_id, category):
    if category == 'new':
        return session.query(Observations).join(TaxaTemp, Observations.taxon_id == TaxaTemp.taxon_id).filter(
            TaxaTemp.taxon_id == taxon_id,
            ~session.query(Taxa).filter(Taxa.taxon_id == taxon_id).exists(),
            session.query(Observations).filter(Observations.taxon_id == TaxaTemp.taxon_id).count() > 180
        ).count()
    elif category == 'deprecated':
        return session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).filter(
            Taxa.taxon_id == taxon_id,
            ~session.query(TaxaTemp).filter(TaxaTemp.taxon_id == taxon_id).exists(),
            session.query(Observations).filter(Observations.taxon_id == Taxa.taxon_id).count() > 180
        ).count()
    elif category == 'active_status_changes':
        return session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
            Taxa.taxon_id == taxon_id,
            Taxa.active != TaxaTemp.active,
            session.query(Observations).filter(Observations.taxon_id == Taxa.taxon_id).count() > 180
        ).count()
        </file>
        <file path="dbTools/taxa/analyze_diff.py" line_interval="25">
import os
import csv
import argparse
from sqlalchemy import create_engine, Column, Integer, String, Float, Date, Boolean, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from ibridaDB.schema import Observations, Photos, Taxa, TaxaTemp, Observers
from ibridaDB.taxa.analysis_utils import (
    count_new_taxa,
    count_deprecated_taxa,
    count_active_status_changes,
    count_name_changes,
    count_other_attribute_changes,
    count_observations_for_taxa,
    count_observations_for_common_taxa
)
import logging


# Setup basic configuration for logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def create_db_engine(db_user, db_password, db_host, db_port, db_name):
#|LN|25|
    connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    return create_engine(connection_string)


def create_temp_taxa_table(session):
    TaxaTemp.__table__.create(session.bind, checkfirst=True)


def drop_temp_taxa_table(session):
    TaxaTemp.__table__.drop(session.bind, checkfirst=True)


def load_temp_taxa_data(origin, session, csv_file_path):
    with open(csv_file_path, 'r') as f:
        next(f)  # Skip the header row
        reader = csv.reader(f, delimiter='\t', quotechar='\b')
        for row in reader:
            if len(row) < 6:
                logging.warning(f"Skipping incomplete row: {row}")
                continue
            try:
                new_taxa = TaxaTemp(
                    taxon_id=int(row[0]),
                    ancestry=row[1],
                    rank_level=float(row[2]) if row[2] else None,
#|LN|50|
                    rank=row[3],
                    name=row[4],
                    active=row[5].lower() == 'true'
                )
                session.add(new_taxa)
            except ValueError as e:
                logging.error(f"Error processing row {row}: {e}")
                session.rollback()
                continue
    session.commit()


def analyze_specific_taxa_changes(origin, session, taxa_groups):
    output_dir = f"diffs/{origin}"
    os.makedirs(output_dir, exist_ok=True)

    for rank_level, taxa_ids in taxa_groups.items():
        print(f"Analyzing changes for {rank_level} taxa...")
        with open(f"{output_dir}/{rank_level}_analysis.txt", "w") as f:
            f.write(f"Analysis for {rank_level} taxa:\n\n")

            for taxon_id in taxa_ids:
                print(f"Analyzing changes for taxon ID: {taxon_id}")
                f.write(f"Taxon ID: {taxon_id}\n")

#|LN|75|
                new_taxa_count = count_new_taxa(session, taxon_id)
                f.write(f"New taxa count: {new_taxa_count}\n")

                deprecated_taxa_count = count_deprecated_taxa(session, taxon_id)
                f.write(f"Deprecated taxa count: {deprecated_taxa_count}\n")

                active_status_changes_count = count_active_status_changes(session, taxon_id)
                f.write(f"Active status changes count: {active_status_changes_count}\n")

                name_changes_count = count_name_changes(session, taxon_id)
                f.write(f"Name changes count: {name_changes_count}\n")

                other_attribute_changes_count = count_other_attribute_changes(session, taxon_id)
                f.write(f"Other attribute changes count: {other_attribute_changes_count}\n")

                observations_new_taxa_count = count_observations_for_taxa(session, taxon_id, 'new')
                f.write(f"Observations with new taxa count: {observations_new_taxa_count}\n")

                observations_deprecated_taxa_count = count_observations_for_taxa(session, taxon_id, 'deprecated')
                f.write(f"Observations with deprecated taxa count: {observations_deprecated_taxa_count}\n")

                observations_active_status_changes_count = count_observations_for_taxa(session, taxon_id, 'active_status_changes')
                f.write(f"Observations with active status changes count: {observations_active_status_changes_count}\n")

                observations_common_new_taxa_count = count_observations_for_common_taxa(session, taxon_id, 'new')
#|LN|100|
                f.write(f"Observations with common new taxa count: {observations_common_new_taxa_count}\n")

                observations_common_deprecated_taxa_count = count_observations_for_common_taxa(session, taxon_id, 'deprecated')
                f.write(f"Observations with common deprecated taxa count: {observations_common_deprecated_taxa_count}\n")

                observations_common_active_status_changes_count = count_observations_for_common_taxa(session, taxon_id, 'active_status_changes')
                f.write(f"Observations with common active status changes count: {observations_common_active_status_changes_count}\n")

                f.write("\n")


def analyze_taxa_changes(origin, session, output_dir):
    print("Analyzing overall taxa changes...")
    
    print("Counting taxon IDs in the new taxa data...")
    new_taxon_count = session.query(TaxaTemp).count()
    with open(f"{output_dir}/new_taxon_count.csv", "w") as f:
        f.write(f"new_taxon_count\n{new_taxon_count}\n")

    print("Finding deprecated taxon IDs...")
    deprecated_taxon_ids = session.query(Taxa.taxon_id).filter(~Taxa.taxon_id.in_(session.query(TaxaTemp.taxon_id))).all()
    with open(f"{output_dir}/deprecated_taxon_ids.csv", "w") as f:
        f.write("taxon_id\n")
        for taxon_id in deprecated_taxon_ids:
            f.write(f"{taxon_id[0]}\n")
#|LN|125|

    print("Finding new taxon IDs...")
    new_taxon_ids = session.query(TaxaTemp.taxon_id, TaxaTemp.ancestry, TaxaTemp.rank_level, TaxaTemp.active).filter(~TaxaTemp.taxon_id.in_(session.query(Taxa.taxon_id))).all()
    with open(f"{output_dir}/new_taxon_ids.csv", "w") as f:
        f.write("taxon_id,ancestry,rank_level,active\n")
        for taxon_id, ancestry, rank_level, active in new_taxon_ids:
            f.write(f"{taxon_id},{ancestry},{rank_level},{active}\n")

    print("Finding taxon IDs with changed attributes...")
    changed_attributes = session.query(
        Taxa.taxon_id,
        Taxa.ancestry, TaxaTemp.ancestry,
        Taxa.rank_level, TaxaTemp.rank_level,
        Taxa.rank, TaxaTemp.rank,
        Taxa.name, TaxaTemp.name,
        Taxa.active, TaxaTemp.active
    ).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
        (Taxa.ancestry != TaxaTemp.ancestry) |
        (Taxa.rank_level != TaxaTemp.rank_level) |
        (Taxa.rank != TaxaTemp.rank) |
        (Taxa.name != TaxaTemp.name) |
        (Taxa.active != TaxaTemp.active)
    ).all()

    with open(f"{output_dir}/changed_attributes.csv", "w") as f:
#|LN|150|
        f.write("taxon_id,existing_ancestry,new_ancestry,existing_rank_level,new_rank_level,existing_rank,new_rank,existing_name,new_name,existing_active,new_active\n")
        for row in changed_attributes:
            f.write(",".join(str(value) for value in row) + "\n")

    print("Listing taxon IDs with changed 'active' values...")
    active_status_changes = session.query(
        Taxa.taxon_id,
        Taxa.rank_level,
        Taxa.active,
        TaxaTemp.active
    ).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
        Taxa.active != TaxaTemp.active
    ).all()

    with open(f"{output_dir}/active_status_changes.csv", "w") as f:
        f.write("taxon_id,rank_level,existing_active,new_active\n")
        for taxon_id, rank_level, existing_active, new_active in active_status_changes:
            f.write(f"{taxon_id},{rank_level},{existing_active},{new_active}\n")

    print("Listing taxon IDs with changed 'name' values...")
    name_changes = session.query(
        Taxa.taxon_id,
        Taxa.rank_level,
        Taxa.name,
        TaxaTemp.name
#|LN|175|
    ).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
        Taxa.name != TaxaTemp.name
    ).all()

    with open(f"{output_dir}/name_changes.csv", "w") as f:
        f.write("taxon_id,rank_level,existing_name,new_name\n")
        for taxon_id, rank_level, existing_name, new_name in name_changes:
            f.write(f"{taxon_id},{rank_level},{existing_name},{new_name}\n")

    print("Counting observations with inactive taxon IDs...")
    inactive_observations_count = session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).filter(Taxa.active == False).count()
    with open(f"{output_dir}/inactive_observations_count.csv", "w") as f:
        f.write(f"inactive_observations_count\n{inactive_observations_count}\n")


def main():
    parser = argparse.ArgumentParser(description="Analyze taxa changes between existing database and new CSV")
    parser.add_argument("--origin", required=True, help="Date code of the new taxa CSV (e.g., May2024)")
    parser.add_argument("--db-user", default="postgres", help="Database user")
    parser.add_argument("--db-password", default="password", help="Database password")
    parser.add_argument("--db-host", default="localhost", help="Database host")
    parser.add_argument("--db-port", default="5432", help="Database port")
    parser.add_argument("--db-name", default="postgres", help="Database name")
    parser.add_argument("--csv-file-path", help="Path to the new taxa CSV file")
    parser.add_argument("--use-existing-temp-table", action="store_true", help="Use existing TaxaTemp table instead of loading a new one")
#|LN|200|
    parser.add_argument("--clear-temp", action="store_true", help="Drop the existing TaxaTemp table if it exists")

    args = parser.parse_args()

    engine = create_db_engine(args.db_user, args.db_password, args.db_host, args.db_port, args.db_name)
    Session = sessionmaker(bind=engine)
    session = Session()

    if args.clear_temp:
        if args.use_existing_temp_table:
            logging.error("Cannot use both --clear-temp and --use-existing-temp-table flags together.")
            return
        drop_temp_taxa_table(session)
        create_temp_taxa_table(session)
        if args.csv_file_path:
            load_temp_taxa_data(args.origin, session, args.csv_file_path)
        else:
            csv_file_path = f'/ibrida/metadata/{args.origin}/taxa.csv'
            load_temp_taxa_data(args.origin, session, csv_file_path)
    else:
        create_temp_taxa_table(session)
        if not args.use_existing_temp_table:
            if args.csv_file_path:
                load_temp_taxa_data(args.origin, session, args.csv_file_path)
            else:
#|LN|225|
                csv_file_path = f'/ibrida/metadata/{args.origin}/taxa.csv'
                load_temp_taxa_data(args.origin, session, csv_file_path)

    output_dir = f"diffs/{args.origin}"
    os.makedirs(output_dir, exist_ok=True)

    analyze_taxa_changes(args.origin, session, output_dir)

    taxa_groups = {
        "L60": [47120],
        "L50": [47163, 47124, 40151, 3, 26036, 20978, 47119, 47158],
        "L40": [47744, 47157, 47792, 47651, 47208, 47822, 47201]
    }
    analyze_specific_taxa_changes(args.origin, session, taxa_groups)

    session.close()


if __name__ == "__main__":
    main()
        </file>
        <file path="dbTools/taxa/reference.md">
# Taxon ranks
## code_to_name
*maps taxon rank polli-style code to rank names*
code_to_name = {
    'L5': 'subspecies',
    'L10': 'species',
    'L11': 'complex',
    'L12': 'subsection', 
    'L13': 'section',
    'L15': 'subgenus',
    'L20': 'genus',
    'L24': 'subtribe',
    'L25': 'tribe',
    'L26': 'supertribe',
    'L27': 'subfamily',
    'L30': 'family',
    'L32': 'epifamily',
    'L33': 'superfamily',
    'L33_5': 'zoosubsection',
    'L34': 'zoosection',
    'L34_5': 'parvorder',
    'L35': 'infraorder',
    'L37': 'suborder',
    'L40': 'order',
    'L43': 'superorder',
    'L44': 'subterclass',
    'L45': 'infraclass',
    'L47': 'subclass',
    'L50': 'class',
    'L53': 'superclass',
    'L57': 'subphylum',
    'L60': 'phylum',
    'L67': 'subkingdom',
    'L70': 'kingdom'
}
### ambiguous ranks
We assume that the possibly ambiguous ranks are of the above ranks downstream. However, note that the following levels could be ambiguous:
*Possible ranks:*
- L5: form, infrahybrid, subspecies
- L10: hybrid, species
- L20: genus, genushybrid
        </file>
        <dir path="dbTools/taxa/diffs">
          <dir path="dbTools/taxa/diffs/May2024">
            <file path="dbTools/taxa/diffs/May2024/L40_analysis.txt">
Analysis for L40 taxa:

Taxon ID: 47744
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47157
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47792
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47651
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47208
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47822
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47201
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0


            </file>
            <file path="dbTools/taxa/diffs/May2024/L50_analysis.txt">
Analysis for L50 taxa:

Taxon ID: 47163
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47124
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 40151
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 3
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 26036
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 20978
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47119
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47158
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0


            </file>
            <file path="dbTools/taxa/diffs/May2024/L60_analysis.txt">
Analysis for L60 taxa:

Taxon ID: 47120
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0


            </file>
          </dir>
        </dir>
        <dir path="dbTools/taxa/expand">
          <file path="dbTools/taxa/expand/expand_taxa.sh" line_interval="25">
#!/usr/bin/env bash
# -----------------------------------------------------------------------------
# expand_taxa.sh
# -----------------------------------------------------------------------------
# Creates a new "expanded_taxa" table by expanding ancestry from the existing
# "taxa" table into structured columns ("L{level}_taxonID", "L{level}_name", etc.).
#
# This version uses *string concatenation* with quote_ident(...) and quote_nullable(...),
# bypassing placeholders entirely. This is the "sure" approach: no risk of
# placeholders vanishing, since we embed the actual values directly into the
# final SQL string.
#
# Steps:
#   1) Drop 'expanded_taxa' if exists; create base columns with quotes.
#   2) Add columns for each rank level ("L5_taxonID", "L5_name", etc.).
#   3) Create expand_taxa_procedure() as a function:
#      - We skip rank levels not in RANK_LEVELS (no 100).
#      - If debugging is enabled (DEBUG_EXPAND_TAXA=true), we RAISE NOTICE
#        about the row's data and the final SQL statement.
#      - We *string-concatenate* the column references, so no placeholders are used.
#   4) SELECT expand_taxa_procedure() to populate the table.
#   5) Create indexes on "L10_taxonID"... "L70_taxonID", plus "taxonID", "rankLevel", "name".
#   6) VACUUM (ANALYZE), notifications, done.
#
#|LN|25|
# Usage:
#   DEBUG_EXPAND_TAXA=true ./expand_taxa.sh
# -----------------------------------------------------------------------------

# ===[ 1) Setup & Logging ]====================================================
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Log messages (with timestamps) to both console and file
log_message() {
    local timestamp
    timestamp="$(date +'%Y-%m-%dT%H:%M:%S%z')"
    echo "[$timestamp] $1" | tee -a "${LOG_FILE}"
}

# Redirect stdout/stderr to console+log
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

# Source your common functions (for execute_sql, send_notification, etc.)
source "/home/caleb/repo/ibridaDB/dbTools/export/v0/common/functions.sh"

# Environment / defaults
DB_CONTAINER="${DB_CONTAINER:-ibridaDB}"
#|LN|50|
DB_NAME="${DB_NAME:-ibrida-v0-r1}"
DB_USER="${DB_USER:-postgres}"

# If DEBUG_EXPAND_TAXA=true, we pass a GUC variable into Postgres to enable debug
DEBUG_EXPAND="${DEBUG_EXPAND_TAXA:-false}"  # "true" or "false"

# rank levels to expand (no 100). We'll handle 5, 10, 11, ... 70
RANK_LEVELS=(5 10 11 12 13 15 20 24 25 26 27 30 32 33 33.5 34 34.5 35 37 40 43 44 45 47 50 53 57 60 67 70)

# We'll create indexes only on L10..L70
INDEX_LEVELS=(10 20 30 40 50 60 70)

log_message "Beginning expand_taxa.sh for DB: ${DB_NAME} (DEBUG_EXPAND_TAXA=${DEBUG_EXPAND})"

# ===[ 2) Create expanded_taxa schema ]========================================
log_message "Step 1: Dropping old expanded_taxa and creating base columns with quotes."

execute_sql "
DROP TABLE IF EXISTS \"expanded_taxa\" CASCADE;
CREATE TABLE \"expanded_taxa\" (
    \"taxonID\"       INTEGER PRIMARY KEY,
    \"rankLevel\"     DOUBLE PRECISION,
    \"rank\"          VARCHAR(255),
    \"name\"          VARCHAR(255),
    \"commonName\"    VARCHAR(255),
#|LN|75|
    \"taxonActive\"   BOOLEAN
    -- We'll add \"LXX_taxonID\", \"LXX_name\", \"LXX_commonName\" columns next
);
"

# ===[ 3) Add columns for each rank level ]====================================
log_message "Step 2: Adding L{level}_taxonID, L{level}_name, L{level}_commonName columns."

ADD_COLS=""
for L in "${RANK_LEVELS[@]}"; do
  SAFE_L=$(echo "${L}" | sed 's/\./_/g')
  ADD_COLS+=" ADD COLUMN \"L${SAFE_L}_taxonID\" INTEGER,
             ADD COLUMN \"L${SAFE_L}_name\" VARCHAR(255),
             ADD COLUMN \"L${SAFE_L}_commonName\" VARCHAR(255),"
done

# Remove trailing comma
ADD_COLS="${ADD_COLS%,}"

execute_sql "
ALTER TABLE \"expanded_taxa\"
${ADD_COLS};
"

# ===[ 4) Create expand_taxa_procedure() function ]============================
#|LN|100|
log_message "Step 3: Creating expand_taxa_procedure() with string-concatenation for dynamic columns."

# We'll incorporate a GUC "myapp.debug_expand" to signal debug mode in PL/pgSQL
if [ "${DEBUG_EXPAND}" = "true" ]; then
  execute_sql "SET myapp.debug_expand = 'on';"
else
  execute_sql "SET myapp.debug_expand = 'off';"
fi

execute_sql "
DROP FUNCTION IF EXISTS expand_taxa_procedure() CASCADE;

CREATE OR REPLACE FUNCTION expand_taxa_procedure()
RETURNS void
LANGUAGE plpgsql
AS \$\$
DECLARE
    t_rec RECORD;
    ancestor_ids TEXT[];
    this_ancestor TEXT;
    anc_data RECORD;
    effective_level TEXT;
    row_sql TEXT;
    debugging boolean := false;
BEGIN
#|LN|125|
    -- We'll read our GUC to see if debug is on
    BEGIN
        IF current_setting('myapp.debug_expand') = 'on' THEN
            debugging := true;
        END IF;
    EXCEPTION
        WHEN others THEN
            debugging := false;  -- if the GUC is not set, do nothing
    END;

    -- Only retrieve active taxa rows, ignoring inactive ones
    FOR t_rec IN
        SELECT taxon_id, ancestry, rank_level, rank, name, active
        FROM taxa
        WHERE active = true
    LOOP
        -- Insert base row
        INSERT INTO \"expanded_taxa\"(\"taxonID\", \"rankLevel\", \"rank\", \"name\", \"taxonActive\")
        VALUES (t_rec.taxon_id, t_rec.rank_level, t_rec.rank, t_rec.name, t_rec.active);

        IF t_rec.ancestry IS NOT NULL AND t_rec.ancestry <> '' THEN
            ancestor_ids := string_to_array(t_rec.ancestry, '/');
        ELSE
            ancestor_ids := ARRAY[]::TEXT[];
        END IF;
#|LN|150|

        -- Include self
        ancestor_ids := ancestor_ids || t_rec.taxon_id::TEXT;

        FOREACH this_ancestor IN ARRAY ancestor_ids
        LOOP
            BEGIN
                IF this_ancestor IS NULL THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping NULL ancestor for row taxon_id=%', t_rec.taxon_id;
                    END IF;
                    CONTINUE;
                END IF;

                SELECT rank_level, rank, name
                  INTO anc_data
                  FROM taxa
                 WHERE taxon_id = this_ancestor::INTEGER
                 LIMIT 1;

                IF NOT FOUND OR anc_data.name IS NULL THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping ancestor=% for row taxon_id=%: not found or name is NULL', this_ancestor, t_rec.taxon_id;
                    END IF;
                    CONTINUE;
#|LN|175|
                END IF;

                IF anc_data.rank_level NOT IN (
                    5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30,
                    32, 33, 33.5, 34, 34.5, 35, 37, 40, 43, 44, 45,
                    47, 50, 53, 57, 60, 67, 70
                ) THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping rank_level=% for row taxon_id=% (ancestor=%)', anc_data.rank_level, t_rec.taxon_id, this_ancestor;
                    END IF;
                    CONTINUE;
                END IF;

                effective_level := replace(CAST(anc_data.rank_level AS TEXT), '.', '_');

                -- Build dynamic SQL via string concat + quote_ident(...) + quote_nullable(...)
                row_sql :=
                    'UPDATE \"expanded_taxa\" SET '
                    || quote_ident('L' || effective_level || '_taxonID') || ' = '
                        || quote_nullable(this_ancestor)
                    || ', '
                    || quote_ident('L' || effective_level || '_name') || ' = '
                        || quote_nullable(anc_data.name)
                    || ' WHERE \"taxonID\" = ' || quote_nullable(t_rec.taxon_id::text);

#|LN|200|
                IF debugging THEN
                    RAISE NOTICE 'Row taxon_id=% => rank_level=% => built SQL: %',
                                 t_rec.taxon_id, anc_data.rank_level, row_sql;
                END IF;

                EXECUTE row_sql;

            EXCEPTION WHEN OTHERS THEN
                RAISE NOTICE 'Error updating row => base taxon_id=%, ancestor=%, anc_data=(%,%,%), row_sql=[%]',
                  t_rec.taxon_id, this_ancestor, anc_data.rank_level, anc_data.rank, anc_data.name, row_sql;
                RAISE;
            END;
        END LOOP;
    END LOOP;
END;
\$\$;
"

# ===[ 5) Populate expanded_taxa ]=============================================
log_message "Step 4: SELECT expand_taxa_procedure() to populate."

execute_sql "
SELECT expand_taxa_procedure();
"

#|LN|225|
log_message "Population of expanded_taxa complete. Running: \\d \"expanded_taxa\""
execute_sql "\d \"expanded_taxa\""

send_notification "expand_taxa.sh: Step 4 complete (expanded_taxa populated)."

# ===[ 6) Create indexes (only on L10..L70) ]===================================
log_message "Step 5: Creating indexes on L10_taxonID, L20_taxonID, ..., L70_taxonID plus base columns."

for L in "${INDEX_LEVELS[@]}"; do
  SAFE_L=$(echo "${L}" | sed 's/\./_/g')
  execute_sql "
  CREATE INDEX IF NOT EXISTS idx_expanded_taxa_L${SAFE_L}_taxonID
    ON \"expanded_taxa\"(\"L${SAFE_L}_taxonID\");
  "
done

execute_sql "
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_taxonID    ON \"expanded_taxa\"(\"taxonID\");
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_rankLevel  ON \"expanded_taxa\"(\"rankLevel\");
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_name       ON \"expanded_taxa\"(\"name\");
"

log_message "Index creation done. Running: \\d \"expanded_taxa\""
execute_sql "\d \"expanded_taxa\""

#|LN|250|
send_notification "expand_taxa.sh: Step 5 complete (indexes created)."

# ===[ 7) VACUUM ANALYZE ]====================================================
log_message "Step 6: VACUUM ANALYZE \"expanded_taxa\" (final step)."

execute_sql "
VACUUM (ANALYZE) \"expanded_taxa\";
"

send_notification "expand_taxa.sh: Step 6 complete (VACUUM ANALYZE done)."
log_message "expand_taxa.sh complete. Exiting."
          </file>
        </dir>
        <dir path="dbTools/taxa/models">
          <file path="dbTools/taxa/models/__init__.py">
from .expanded_taxa import TaxaExpanded
from .expanded_taxa_cmn import ExpandedTaxaCmn
from .coldp_models import (
    ColdpVernacularName,
    ColdpDistribution, 
    ColdpMedia,
    ColdpReference,
    ColdpTypeMaterial
)

__all__ = [
    'TaxaExpanded',
    'ExpandedTaxaCmn',
    'ColdpVernacularName',
    'ColdpDistribution',
    'ColdpMedia',
    'ColdpReference',
    'ColdpTypeMaterial'
]
          </file>
          <file path="dbTools/taxa/models/coldp_models.py">
from sqlalchemy import (
    Column, String, Text, Boolean, Date, Numeric, Integer
)
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class ColdpVernacularName(Base):
    __tablename__ = "coldp_vernacular_name"
    taxonID        = Column(String(10), primary_key=True)
    sourceID       = Column(String(10))
    name           = Column(Text, nullable=False)
    transliteration= Column(Text)
    language       = Column(String(3))      # ISO‑639‑3
    preferred      = Column(Boolean)
    country        = Column(String(2))      # ISO‑3166‑1‑alpha‑2
    area           = Column(Text)
    sex            = Column(String(20))
    referenceID    = Column(String(64))
    remarks        = Column(Text)

class ColdpDistribution(Base):
    __tablename__ = "coldp_distribution"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(10), index=True)
    sourceID       = Column(String(10))
    areaID         = Column(String(10))
    area           = Column(Text)
    gazetteer      = Column(String(10))
    status         = Column(String(25))     # e.g. native, introduced
    referenceID    = Column(String(64))
    remarks        = Column(Text)

class ColdpMedia(Base):
    __tablename__ = "coldp_media"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(10), index=True)
    sourceID       = Column(String(10))
    url            = Column(Text, nullable=False)
    type           = Column(String(50))     # stillImage, sound, video …
    format         = Column(String(50))     # MIME type or file suffix
    title          = Column(Text)
    created        = Column(Date)
    creator        = Column(Text)
    license        = Column(String(100))
    link           = Column(Text)           # landing page
    remarks        = Column(Text)

class ColdpReference(Base):
    __tablename__ = "coldp_reference"
    ID             = Column(String(64), primary_key=True)   # UUID or short key
    alternativeID  = Column(String(64))
    sourceID       = Column(String(10))
    citation       = Column(Text)
    type           = Column(String(30))
    author         = Column(Text)
    editor         = Column(Text)
    title          = Column(Text)
    titleShort     = Column(Text)
    containerAuthor= Column(Text)
    containerTitle = Column(Text)
    containerTitleShort = Column(Text)
    issued         = Column(String(50))
    accessed       = Column(String(50))
    collectionTitle= Column(Text)
    collectionEditor= Column(Text)
    volume         = Column(String(30))
    issue          = Column(String(30))
    edition        = Column(String(30))
    page           = Column(String(50))
    publisher      = Column(Text)
    publisherPlace = Column(Text)
    version        = Column(String(30))
    isbn           = Column(String(20))
    issn           = Column(String(20))
    doi            = Column(String(100))
    link           = Column(Text)
    remarks        = Column(Text)

class ColdpTypeMaterial(Base):
    """
    ColDP entity `TypeMaterial` (called TypeSpecimen in the user request).
    """
    __tablename__ = "coldp_type_material"
    ID              = Column(String(64), primary_key=True)
    nameID          = Column(String(10), index=True)
    sourceID        = Column(String(10))
    citation        = Column(Text)
    status          = Column(String(50))
    referenceID     = Column(String(64))
    page            = Column(String(50))
    country         = Column(String(2))
    locality        = Column(Text)
    latitude        = Column(Numeric(9,5))
    longitude       = Column(Numeric(9,5))
    altitude        = Column(String(50))
    sex             = Column(String(12))
    host            = Column(Text)
    associatedSequences = Column(Text)
    date            = Column(Date)
    collector       = Column(Text)
    institutionCode = Column(String(25))
    catalogNumber   = Column(String(50))
    link            = Column(Text)
    remarks         = Column(Text)
          </file>
          <file path="dbTools/taxa/models/expanded_taxa.py">
from sqlalchemy import Boolean, Column, Float, Index, Integer, String
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()


class ExpandedTaxa(Base):
    __tablename__ = "expanded_taxa"

    taxonID = Column(Integer, primary_key=True, nullable=False)
    rankLevel = Column(Float, index=True)
    rank = Column(String(255))
    name = Column(String(255), index=True)
    commonName = Column(String(255))
    taxonActive = Column(Boolean, index=True)

    # Ancestral columns
    L5_taxonID = Column(Integer)
    L5_name = Column(String(255))
    L5_commonName = Column(String(255))
    L10_taxonID = Column(Integer)
    L10_name = Column(String(255))
    L10_commonName = Column(String(255))
    L11_taxonID = Column(Integer)
    L11_name = Column(String(255))
    L11_commonName = Column(String(255))
    L12_taxonID = Column(Integer)
    L12_name = Column(String(255))
    L12_commonName = Column(String(255))
    L13_taxonID = Column(Integer)
    L13_name = Column(String(255))
    L13_commonName = Column(String(255))
    L15_taxonID = Column(Integer)
    L15_name = Column(String(255))
    L15_commonName = Column(String(255))
    L20_taxonID = Column(Integer)
    L20_name = Column(String(255))
    L20_commonName = Column(String(255))
    L24_taxonID = Column(Integer)
    L24_name = Column(String(255))
    L24_commonName = Column(String(255))
    L25_taxonID = Column(Integer)
    L25_name = Column(String(255))
    L25_commonName = Column(String(255))
    L26_taxonID = Column(Integer)
    L26_name = Column(String(255))
    L26_commonName = Column(String(255))
    L27_taxonID = Column(Integer)
    L27_name = Column(String(255))
    L27_commonName = Column(String(255))
    L30_taxonID = Column(Integer)
    L30_name = Column(String(255))
    L30_commonName = Column(String(255))
    L32_taxonID = Column(Integer)
    L32_name = Column(String(255))
    L32_commonName = Column(String(255))
    L33_taxonID = Column(Integer)
    L33_name = Column(String(255))
    L33_commonName = Column(String(255))
    L33_5_taxonID = Column(Integer)
    L33_5_name = Column(String(255))
    L33_5_commonName = Column(String(255))
    L34_taxonID = Column(Integer)
    L34_name = Column(String(255))
    L34_commonName = Column(String(255))
    L34_5_taxonID = Column(Integer)
    L34_5_name = Column(String(255))
    L34_5_commonName = Column(String(255))
    L35_taxonID = Column(Integer)
    L35_name = Column(String(255))
    L35_commonName = Column(String(255))
    L37_taxonID = Column(Integer)
    L37_name = Column(String(255))
    L37_commonName = Column(String(255))
    L40_taxonID = Column(Integer)
    L40_name = Column(String(255))
    L40_commonName = Column(String(255))
    L43_taxonID = Column(Integer)
    L43_name = Column(String(255))
    L43_commonName = Column(String(255))
    L44_taxonID = Column(Integer)
    L44_name = Column(String(255))
    L44_commonName = Column(String(255))
    L45_taxonID = Column(Integer)
    L45_name = Column(String(255))
    L45_commonName = Column(String(255))
    L47_taxonID = Column(Integer)
    L47_name = Column(String(255))
    L47_commonName = Column(String(255))
    L50_taxonID = Column(Integer)
    L50_name = Column(String(255))
    L50_commonName = Column(String(255))
    L53_taxonID = Column(Integer)
    L53_name = Column(String(255))
    L53_commonName = Column(String(255))
    L57_taxonID = Column(Integer)
    L57_name = Column(String(255))
    L57_commonName = Column(String(255))
    L60_taxonID = Column(Integer)
    L60_name = Column(String(255))
    L60_commonName = Column(String(255))
    L67_taxonID = Column(Integer)
    L67_name = Column(String(255))
    L67_commonName = Column(String(255))
    L70_taxonID = Column(Integer)
    L70_name = Column(String(255))
    L70_commonName = Column(String(255))


# Important indexes for lookups
Index("idx_expanded_taxa_L10_taxonID", "L10_taxonID")

          </file>
          <file path="dbTools/taxa/models/expanded_taxa_cmn.py">
from sqlalchemy import (
    Column, Integer, String, Text, Boolean, Float, Index
)
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class ExpandedTaxaCmn(Base):
    """
    Copy of expanded_taxa with additional common‑name columns.
    Only the *preferred English* common name is stored per taxon.
    """
    __tablename__ = "expanded_taxa_cmn"

    taxonID       = Column(Integer, primary_key=True, nullable=False)
    rankLevel     = Column(Float, index=True)
    rank          = Column(String(255))
    name          = Column(String(255), index=True)
    commonName    = Column(String(255))               # NEW
    taxonActive   = Column(Boolean, index=True)

    # Ancestral columns – dots converted to underscores to match SQL
    L5_taxonID    = Column(Integer)
    L5_name       = Column(Text)
    L5_commonName = Column(String(255))
    L10_taxonID    = Column(Integer)
    L10_name       = Column(Text)
    L10_commonName = Column(String(255))
    L11_taxonID    = Column(Integer)
    L11_name       = Column(Text)
    L11_commonName = Column(String(255))
    L12_taxonID    = Column(Integer)
    L12_name       = Column(Text)
    L12_commonName = Column(String(255))
    L13_taxonID    = Column(Integer)
    L13_name       = Column(Text)
    L13_commonName = Column(String(255))
    L15_taxonID    = Column(Integer)
    L15_name       = Column(Text)
    L15_commonName = Column(String(255))
    L20_taxonID    = Column(Integer)
    L20_name       = Column(Text)
    L20_commonName = Column(String(255))
    L24_taxonID    = Column(Integer)
    L24_name       = Column(Text)
    L24_commonName = Column(String(255))
    L25_taxonID    = Column(Integer)
    L25_name       = Column(Text)
    L25_commonName = Column(String(255))
    L26_taxonID    = Column(Integer)
    L26_name       = Column(Text)
    L26_commonName = Column(String(255))
    L27_taxonID    = Column(Integer)
    L27_name       = Column(Text)
    L27_commonName = Column(String(255))
    L30_taxonID    = Column(Integer)
    L30_name       = Column(Text)
    L30_commonName = Column(String(255))
    L32_taxonID    = Column(Integer)
    L32_name       = Column(Text)
    L32_commonName = Column(String(255))
    L33_taxonID    = Column(Integer)
    L33_name       = Column(Text)
    L33_commonName = Column(String(255))
    L33_5_taxonID    = Column(Integer)
    L33_5_name       = Column(Text)
    L33_5_commonName = Column(String(255))
    L34_taxonID    = Column(Integer)
    L34_name       = Column(Text)
    L34_commonName = Column(String(255))
    L34_5_taxonID    = Column(Integer)
    L34_5_name       = Column(Text)
    L34_5_commonName = Column(String(255))
    L35_taxonID    = Column(Integer)
    L35_name       = Column(Text)
    L35_commonName = Column(String(255))
    L37_taxonID    = Column(Integer)
    L37_name       = Column(Text)
    L37_commonName = Column(String(255))
    L40_taxonID    = Column(Integer)
    L40_name       = Column(Text)
    L40_commonName = Column(String(255))
    L43_taxonID    = Column(Integer)
    L43_name       = Column(Text)
    L43_commonName = Column(String(255))
    L44_taxonID    = Column(Integer)
    L44_name       = Column(Text)
    L44_commonName = Column(String(255))
    L45_taxonID    = Column(Integer)
    L45_name       = Column(Text)
    L45_commonName = Column(String(255))
    L47_taxonID    = Column(Integer)
    L47_name       = Column(Text)
    L47_commonName = Column(String(255))
    L50_taxonID    = Column(Integer)
    L50_name       = Column(Text)
    L50_commonName = Column(String(255))
    L53_taxonID    = Column(Integer)
    L53_name       = Column(Text)
    L53_commonName = Column(String(255))
    L57_taxonID    = Column(Integer)
    L57_name       = Column(Text)
    L57_commonName = Column(String(255))
    L60_taxonID    = Column(Integer)
    L60_name       = Column(Text)
    L60_commonName = Column(String(255))
    L67_taxonID    = Column(Integer)
    L67_name       = Column(Text)
    L67_commonName = Column(String(255))
    L70_taxonID    = Column(Integer)
    L70_name       = Column(Text)
    L70_commonName = Column(String(255))

# Helpful composite index for frequent ancestor look‑ups
Index("idx_expanded_taxa_cmn_L10_taxonID", "L10_taxonID")
          </file>
        </dir>
        <dir path="dbTools/taxa/tools">
          <file path="dbTools/taxa/tools/extract_ColDP_samples.sh">
#!/usr/bin/env bash
set -euo pipefail

INPUT_DIR="/datasets/taxa/catalogue_of_life/2024/ColDP"
OUTPUT_FILE="/home/caleb/repo/ibridaDB/dbTools/taxa/ColDP_raw_samples.txt"

# Start with an empty output file
> "$OUTPUT_FILE"

for filepath in "$INPUT_DIR"/*; do
  # Only process regular files that are not .png
  if [[ -f "$filepath" && "${filepath##*.}" != "png" ]]; then
    filename=$(basename "$filepath")
    {
      echo "<$filename>"
      echo "<head -n 10>"
      head -n 10 "$filepath"
      echo "</$filename>"
      echo "</head -n 10>"
      echo  # blank line between entries
    } >> "$OUTPUT_FILE"
  fi
done
          </file>
        </dir>
      </dir>
    </dir>
    <dir path="dev">
      <file path="dev/ancestor_aware.md">
# 1. Overview

We plan to introduce an **ancestor‐aware approach** into our **iNaturalist data export pipeline** to better support hierarchical classification tasks. Currently, our pipeline focuses only on species that meet a minimum threshold of research‐grade observations in the bounding box and does not systematically gather all their ancestral taxa (e.g., genus, family, order). This design limits the data’s usefulness in scenarios where the model must “know when to stop”—i.e., return a coarser taxonomic label for rare or partially identified specimens. By explicitly collecting each species’s ancestors, we can generate training data that captures the broad taxonomic context for each in‐threshold species, plus partial observations for species that do not meet the threshold but are still informative at higher ranks. Furthermore, new user requirements—such as allowing a user to specify a root rank (not always L70) and automatically wiping low‐occurrence taxa—underscore the need for a flexible, robust mechanism to unify coarse and fine ranks in a single workflow.

# 2. Requirements & Goals

1. **Ancestor Inclusion**  
   - For each species that meets the regional threshold (`MIN_OBS` of research‐grade observations), we must add all relevant ancestral taxa (genus, family, order, etc.) up to a specified root rank. This ensures we do not discard potentially valuable coarser labels.

2. **Root Rank Flexibility**  
   - The user may define a “clade root” to limit how far up we gather ancestors (e.g., only up to L40=order if `CLADE` is at L50=class).  
   - If the user provides a multi‐root `METACLADE`, we repeat the logic for each root.  

3. **Preserving Partial Observations**  
   - Observations of rare species (below the threshold) must still be included if they share an ancestor with a species that meets the threshold. Example: a species that has only 20 research‐grade observations might be worthless for species‐level classification, but still valuable for genus/family modeling.  

4. **Low‐Occurrence Taxon Wiping**  
   - Even after we gather the full lineage, if a particular taxon (say a genus with few total occurrences) fails to meet a user‐configured threshold, we wipe its label from the relevant observations. This prevents the model from trying to learn extremely rare or ill‐defined ranks, while still retaining the rest of the observation’s taxonomic ranks if they exceed the threshold.

5. **Integration with Existing Pipeline**  
   - The solution must integrate cleanly with the existing code structure: `regional_base.sh` for building the base set of species, and `cladistic.sh` for applying final logic.  
   - We also must consider how `SKIP_REGIONAL_BASE=true` and table naming might need to accommodate new parameters or extended logic, ensuring we do not skip creation of a base table that should differ.

# 3. Proposed Approaches

Below are conceptual solutions to implement the ancestor‐aware feature, balancing correctness, performance, and maintainability.

1. **Phase 1 (Species Selection):**  
   - Unchanged at first: gather species that meet `MIN_OBS` in the bounding box. This set becomes \( S \).  
   - For each species in \( S \), we identify its ancestors up to a user‐defined or clade‐derived root rank. This step queries `expanded_taxa` to retrieve `L20_taxonID`, `L30_taxonID`, etc.

2. **Phase 2 (Ancestor Inclusion):**  
   - Compute the union of all ancestor IDs for the species in \( S \). Denote that union as \( T \).  
   - Combine \( S \cup T \) to form `_all_taxa`.  
   - `_all_taxa_obs` is then defined as **all** observations whose `taxon_id` is in `_all_taxa`. If `INCLUDE_OUT_OF_REGION_OBS=true`, we include those observations globally; if not, we reapply the bounding box.  

3. **Phase 3 (Filtering & Wiping):**  
   - Once `_all_taxa_obs` is formed, we apply any logic that wipes certain ranks if they fail an absolute threshold. For instance, if a genus `G` has fewer than `MIN_OBS_GENUS` occurrences, we set `L20_taxonID=NULL` on all relevant rows, effectively turning them into coarser labels.  
   - We might also unify or refine the rank threshold with the existing `RG_FILTER_MODE` concept.

4. **Table Naming & Skip Logic:**  
   - If `INCLUDE_OUT_OF_REGION_OBS` or other new variables impact the final set of `_all_taxa`, we can incorporate them into the naming pattern of the base table, e.g., `<REGION_TAG>_min${MIN_OBS}_IOORtrue_ancestors_all_taxa`. This reduces confusion about whether a table truly matches the current pipeline parameters.  
   - Alternatively, we could skip storing `_all_taxa` entirely and build the final `_all_taxa_obs` in one pass. But storing `_all_taxa` can be helpful for debugging or subsequent reuse.

5. **Performance & Indices:**  
   - Gathering ancestors for each species might be expensive if we do it row by row. We can rely on `expanded_taxa` columns and a single or few set-based queries. For instance, do a join on `expanded_taxa` once, unnest relevant columns (up to the root rank), and deduplicate.

# 4. Decision & Summary

We will adopt a **single pass** approach to ancestor inclusion at the end of `regional_base.sh`:

- **After** we identify the in-threshold species, we gather their ancestors via a SQL query that unrolls `L10_taxonID`, `L20_taxonID`, etc. up to the chosen root rank.  
- The union of those IDs with the species set becomes `_all_taxa`.  
- Then `_all_taxa_obs` is formed by including all observations referencing any ID in `_all_taxa`, subject to bounding-box toggles.  
- Next, in `cladistic.sh`, we optionally wipe certain ranks (genus/family/etc.) if they fail a usage threshold. We also apply `RG_FILTER_MODE` for research vs. non‐research filtering.

**Key Gains:**
- We correctly keep partial-labeled or rare species observations.  
- Observations referencing a genus or family are still included, even if that rank wasn’t physically observed in bounding box, because it is the ancestor of a species in the region.  
- `INCLUDE_OUT_OF_REGION_OBS` remains a toggle controlling whether we gather out-of-region records for the selected `_all_taxa`.  

# 5. Optional: Future Extensions

1. **Multi‐Rank Thresholds**  
   - We might eventually define distinct `MIN_OBS` for genus vs. family vs. species. This would refine or unify partial-labeled data.  
2. **Selective Mix of “Ancestor Only”**  
   - In some cases, the user might not want to preserve, say, all L70=kingdom observations. We could define a cut at `L50` or `L40`.  
3. **Precomputed Caches**  
   - For large datasets, we might precompute each species’s ancestry in a separate table to avoid repeated unnest queries.  

# 6. Implementation Plan

Below is the plan for introducing ancestor‐aware functionality and partial‐rank handling. We integrate the new user requirements: (1) gathering ancestral taxa for each species that meets the bounding‐box threshold, up to a user‐specified root rank, and (2) automatically “wiping” taxonIDs that fail an extended usage threshold. We also clarify table naming conventions so that skip logic in `main.sh` can be consistently applied.

## 6.1 Step‐by‐Step Outline

1. **Extend `regional_base.sh` to produce two base tables**:  
   - **`<REGION_TAG>_min${MIN_OBS}_all_sp`**: This is the current table of *just* species (rank=10) that pass the research‐grade threshold in region.  
   - **`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors`**: A table that includes each species from the first table plus all of its ancestral taxonIDs (up to the root rank).  
     - **CLARIFY**: If `CLADE`=“angiospermae” is at L57, we only gather ancestors up to L50. If `METACLADE` is multi‐root, we do the same for each root.  
     - To build this, we join `<REGION_TAG>_min${MIN_OBS}_all_sp` to `expanded_taxa e` on `e."taxonID" = species_id`, then unnest or gather columns L10, L20, etc., up to the user’s root rank, and insert them into the final table.  

2. **Form the final `_all_taxa_obs` using the union of those IDs**:  
   - Once we have `<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors` (the union of species plus any ancestors), we can produce `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs` the same way we do now, except the condition is:  
     - `observations.taxon_id` in `(SELECT taxon_id FROM <REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors)`  
     - If `INCLUDE_OUT_OF_REGION_OBS=false`, we re‐apply bounding box. If `true`, we do not.  
   - **CLARIFY**: We must confirm how skip logic and table naming incorporate `INCLUDE_OUT_OF_REGION_OBS`. Possibly name the table `"_all_sp_and_ancestors_obs_ioorFalse"` if `INCLUDE_OUT_OF_REGION_OBS=false`, etc.  

3. **Refine `cladistic.sh`** to handle partial labeling logic:  
   1. **RG_FILTER_MODE** remains as is for controlling research‐grade vs. non‐research, species vs. genus, etc.  
   2. **Introduce the new “taxon usage threshold”**: e.g. `MIN_OCCURRENCES_PER_RANK`. If a rank’s usage is below that threshold, we nullify that rank for each relevant observation.  
      - We can do this after the table creation by an `UPDATE` pass: “UPDATE `<EXPORT_GROUP>_observations` SET L20_taxonID = NULL if L20_taxonID is below threshold, etc.” Or we embed a join in the creation query that checks usage counts.  
      - The usage count for each taxon can be computed by grouping `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs` or `<EXPORT_GROUP>_observations`. If a particular `L20_taxonID` or `L30_taxonID` has fewer than `MIN_OCCURRENCES_PER_RANK` occurrences, it is wiped.  
   3. The final CSV export subqueries remain the same (two subqueries unioned: capped research species vs. everything else).

4. **Adjust Table Naming & Skip Logic**:  
   - To avoid collisions:  
     1. **`<REGION_TAG>_min${MIN_OBS}_all_sp`** for the species list.  
     2. **`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors`** for the union of species + their ancestors.  
     3. **`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors_obs`** (with `_ioorTrue` or `_ioorFalse` suffix if we want) for the final base observations table.  
   - Then in `main.sh`, if `SKIP_REGIONAL_BASE=true`, we check for the presence of exactly the right table name. If we see partial or missing suffix, we know it’s not the same config.

5. **Implement “Root Rank” Logic**:  
   - For a single `CLADE` or `MACROCLADE` (like “amphibia” at L50=20978), gather ancestors only up to L40, or up to user’s specified rank. Possibly store this in an env var `ANCESTOR_ROOT_RANKLEVEL`. If `METACLADE` is multi‐root, we do it for each root and union them.  
   - This approach ensures we do not ascend beyond the clade definition. We can store a small dictionary of known ranklevels for the user’s clade roots, or parse from `clade_defns.sh`.

6. **Performance & Indices**:  
   - Each step might do large set operations if we have many species. We can consider an approach:  
     1. Build a temp table for the species set.  
     2. Join to `expanded_taxa` once, unnest columns L10–L70. Filter out columns above `ANCESTOR_ROOT_RANKLEVEL`.  
     3. Insert into the final `_all_sp_and_ancestors`.  
   - If performance is an issue, we might add a partial index or store a precomputed table of “taxon -> all ancestors up to rank=??.” But we can start with the simpler approach first.

## 6.2 Validation & Testing

- **Comparisons** between old pipeline vs. new pipeline with `RG_FILTER_MODE=ONLY_RESEARCH` should yield similar results for species, except we now see additional ancestor taxonIDs in `_all_sp_and_ancestors`.  
- Check the partial-labeled expansions by verifying that extremely rare species rows appear with `L10_taxonID`=NULL in final outputs if `MIN_OCCURRENCES_PER_RANK` is not met.  

## 6.3 Proposed “Incremental” Implementation

1. **Add** code in `regional_base.sh`:
   - Create `..._all_sp`.  
   - Gather ancestors into `..._all_sp_and_ancestors`.  
   - Then produce the final `..._all_sp_and_ancestors_obs`.  
2. **Update** `main.sh` skip logic to search for the new table name. Possibly unify naming with `ANCESTOR_ROOT_RANKLEVEL` or `INCLUDE_OUT_OF_REGION_OBS`.  
3. **Refine** `cladistic.sh`:
   - Insert a step or function that checks usage counts for each rank (20, 30, etc.) and overwrites them with NULL if below threshold. This can be done via an `UPDATE` or a left join with a usage table.  
   - Retain the union approach for final CSV export.  

---

# 7. Additional Planning / Checklists

**7.1 Confirm Known Variables & Defaults**  
1. `ANCESTOR_ROOT_RANKLEVEL`: Derive from user’s clade definitions. If user picks “amphibia” at L50, set this to 40 if they only want up to order.  
2. `MIN_OCCURRENCES_PER_RANK`: If not specified, default to the same `MIN_OBS`.  
3. If `CLARIFY:` is needed for multi‐root `METACLADES`, confirm how we store or pass multiple root ranklevels.

**7.2 Table Name Finalization**  
- Need consistent naming. e.g. `NAfull_min50_sp`, `NAfull_min50_sp_and_ancestors`, `NAfull_min50_sp_and_ancestors_obs_ioorTrue`. This ensures we never skip incorrectly.

**7.3 Potential Edge Cases**  
1. A species might have no recognized ancestors up to the root rank if the DB is incomplete. We handle that gracefully by an empty union.  
2. Rare rank usage. If a genus is used 5 times, but the user sets `MIN_OCCURRENCES_PER_RANK=10`, we wipe L20 for those rows. They might still keep L30 or L40 if those are above threshold.  
3. Multi‐root `METACLADES`. We gather ancestors for each root, union them, and proceed.

**7.4 Next Steps**  
- After finalizing the plan, we’ll proceed to **Phase 3**: coding. We’ll create a new `regional_base.sh` block for ancestor inclusion, rename or unify table creation, and augment `cladistic.sh` for partial rank wiping.

---

# 8. Implementation Progress & Updates

- **[x]** Completed detailed design in Phase 2 (sections 6–7).
- **[x]** Implemented new logic in `regional_base.sh` to:
  - Create `<REGION_TAG>_min${MIN_OBS}_all_sp` (just in-threshold species).
  - Gather all ancestors up to the user’s specified root rank (or each root rank if multi-root) to form `<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors`.
  - Produce `<REGION_TAG>_min${MIN_OBS}_sp_and_ancestors_obs` (or similarly named) if `INCLUDE_OUT_OF_REGION_OBS=true/false`.
- **[x]** Updated `main.sh` skip logic to incorporate the new table names for accurate detection.
- **[x]** Updated `cladistic.sh` to do partial rank wiping (if `MIN_OCCURRENCES_PER_RANK` is set).  
  - Implemented an `UPDATE` step after `<EXPORT_GROUP>_observations` creation to nullify rank columns that fail usage thresholds.
- **[ ]** **CLARIFY**: The multi-root `METACLADE` approach merges each root rank’s ancestors via union. If user sets 2–3 separate roots, we do a union of all ancestor IDs. This is implemented, but user must confirm test results.

**New Env Vars**:
- **`ANCESTOR_ROOT_RANKLEVEL`** (optional) – user can override automatic root detection if they want to limit the lineage more strictly than the clade’s rank. 
- **`MIN_OCCURRENCES_PER_RANK`** – controls partial label wiping; defaults to same as `MIN_OBS` if unspecified.

# 9. Final Results & Discussion

With these changes:
- **Research**: Rare species are no longer outright removed; even if their species label is invalid for training, they’re retained at coarser ranks.
- **Partial Observations**: We can “wipe” sub-threshold labels, ensuring that extremely rare taxa do not pollute the dataset.
- **Ancestry**: Our new approach systematically gathers each in-threshold species’s ancestors, so the pipeline is now **ancestor‐aware** instead of ignoring unobserved higher-rank taxa.

**Performance**: For very large datasets, we may consider precomputing taxon→ancestors or adding indexes. However, for moderate data, the approach should be sufficiently fast.

**Next Steps**:
- Thorough QA and test runs, especially with multi-root `METACLADE`s. 
- Possibly unify the partial-labeled approach with `RG_FILTER_MODE` if user demands more advanced logic (like skipping research-grade for certain ranks).


---

Can you clarify what you mean in this comment  in clade_helpers.sh?
```
# NOTE: We do not forcibly integrate with existing "get_clade_condition()"
# in clade_defns.sh. Instead, you can call parse_clade_expression() if you
# want to do deeper multi-root logic.
```

Furthermore, on this point:
```
	2.	check_root_independence():
	•	Conceptual approach to gather each root’s ancestry from expanded_taxa, confirm disjoint sets.
	•	Currently placeholders. Implementation details would involve real SQL queries.
```
a conceptual approach is not acceptable. We need to generate the complete implementation of this method:
```
# -------------------------------------------------------------
# C) check_root_independence()
# -------------------------------------------------------------
# This function ensures that each root in a multi-root scenario
# is truly independent. That is, no root is an ancestor or descendant
# of another. We do so by building the set of ancestor IDs for each root,
# then verifying disjointness pairwise.
#
# usage: check_root_independence "myDatabaseName" rootArray
#   rootArray: an array of "rank=taxonID" strings, e.g. "50=47158"
# We assume we can run a quick query on 'expanded_taxa' to gather
# the ~30 possible ancestor columns for each root's row, then compare sets.
#
# If overlap is found, we can either abort or print a warning. We'll choose to abort here.
#
function check_root_independence() {
  local dbName="$1"
  shift
  local roots=("$@")  # e.g. ("50=47158" "60=9999")

  if [ "${#roots[@]}" -le 1 ]; then
    # Nothing to check
    return 0
  fi

  # We'll build arrays of sets. For each root r_i, gather its expanded ancestry.
  declare -A rootSets  # a map from index to "list of taxonIDs"

  for i in "${!roots[@]}"; do
    local pair="${roots[$i]}"
    local rank="${pair%%=*}"
    local tid="${pair##*=}"

    # We'll do a single row fetch in expanded_taxa where taxonID=tid,
    # gather all columns "L5_taxonID", "L10_taxonID", ... "L70_taxonID".
    # Then store them in a set in memory.
    # We'll do a naive approach: psql call, parse results, etc.

    # Real code might do:
    # row=$(docker exec ...)
    # Then parse. For now, we conceptualize a pseudo-result.

    # For demonstration, let's pretend we run:
    # row_of_ancestors might look like "47158|47157|...|<some nulls>"
    # We'll parse them into an array, ignoring nulls.

    # We'll store them in e.g. rootSets["$i"] as "47158 47157 1" etc.
    # PSEUDOCODE:
    # (No real code, just conceptual)

    # rootSets["$i"]="${list_of_ancestors}"

    # For the sake of demonstration, we'll skip real queries.

    # <snip>
    :
  done

  # Now compare pairwise sets for overlap
  for ((i=0; i<${#roots[@]}; i++)); do
    for ((j=i+1; j<${#roots[@]}; j++)); do
      # Compare rootSets["$i"] and rootSets["$j"] for intersection
      # If non-empty => abort
      # PSEUDOCODE:
      # overlapCheck ...
      # if [ "$foundOverlap" = "true" ]; then
      #   echo "ERROR: Overlap detected between root i and j"
      #   return 1
      # fi
      :
    done
  done

  return 0
}

```

You may wish to analyze the exact structure of the expanded_taxa table to assist in the completion:
```
ibrida-v0-r1=# \d expanded_taxa
                        Table "public.expanded_taxa"
      Column      |          Type          | Collation | Nullable | Default
------------------+------------------------+-----------+----------+---------
 taxonID          | integer                |           | not null |
 rankLevel        | double precision       |           |          |
 rank             | character varying(255) |           |          |
 name             | character varying(255) |           |          |
 taxonActive      | boolean                |           |          |
 L5_taxonID       | integer                |           |          |
 L5_name          | character varying(255) |           |          |
 L5_commonName    | character varying(255) |           |          |
 L10_taxonID      | integer                |           |          |
 L10_name         | character varying(255) |           |          |
 L10_commonName   | character varying(255) |           |          |
 L11_taxonID      | integer                |           |          |
 L11_name         | character varying(255) |           |          |
 L11_commonName   | character varying(255) |           |          |
 L12_taxonID      | integer                |           |          |
 L12_name         | character varying(255) |           |          |
 L12_commonName   | character varying(255) |           |          |
 L13_taxonID      | integer                |           |          |
 L13_name         | character varying(255) |           |          |
 L13_commonName   | character varying(255) |           |          |
 L15_taxonID      | integer                |           |          |
 L15_name         | character varying(255) |           |          |
 L15_commonName   | character varying(255) |           |          |
 L20_taxonID      | integer                |           |          |
 L20_name         | character varying(255) |           |          |
 L20_commonName   | character varying(255) |           |          |
 L24_taxonID      | integer                |           |          |
 L24_name         | character varying(255) |           |          |
 L24_commonName   | character varying(255) |           |          |
 L25_taxonID      | integer                |           |          |
 L25_name         | character varying(255) |           |          |
 L25_commonName   | character varying(255) |           |          |
 L26_taxonID      | integer                |           |          |
 L26_name         | character varying(255) |           |          |
 L26_commonName   | character varying(255) |           |          |
 L27_taxonID      | integer                |           |          |
 L27_name         | character varying(255) |           |          |
 L27_commonName   | character varying(255) |           |          |
 L30_taxonID      | integer                |           |          |
 L30_name         | character varying(255) |           |          |
 L30_commonName   | character varying(255) |           |          |
 L32_taxonID      | integer                |           |          |
 L32_name         | character varying(255) |           |          |
 L32_commonName   | character varying(255) |           |          |
 L33_taxonID      | integer                |           |          |
 L33_name         | character varying(255) |           |          |
 L33_commonName   | character varying(255) |           |          |
 L33_5_taxonID    | integer                |           |          |
 L33_5_name       | character varying(255) |           |          |
 L33_5_commonName | character varying(255) |           |          |
 L37_taxonID      | integer                |           |          |
 L37_name         | character varying(255) |           |          |
 L37_commonName   | character varying(255) |           |          |
 L40_taxonID      | integer                |           |          |
 L40_name         | character varying(255) |           |          |
 L40_commonName   | character varying(255) |           |          |
 L43_taxonID      | integer                |           |          |
 L43_name         | character varying(255) |           |          |
 L43_commonName   | character varying(255) |           |          |
 L44_taxonID      | integer                |           |          |
 L44_name         | character varying(255) |           |          |
 L44_commonName   | character varying(255) |           |          |
 L45_taxonID      | integer                |           |          |
 L45_name         | character varying(255) |           |          |
 L45_commonName   | character varying(255) |           |          |
 L47_taxonID      | integer                |           |          |
 L47_name         | character varying(255) |           |          |
 L47_commonName   | character varying(255) |           |          |
 L50_taxonID      | integer                |           |          |
 L50_name         | character varying(255) |           |          |
 L50_commonName   | character varying(255) |           |          |
 L53_taxonID      | integer                |           |          |
 L53_name         | character varying(255) |           |          |
 L53_commonName   | character varying(255) |           |          |
 L57_taxonID      | integer                |           |          |
 L57_name         | character varying(255) |           |          |
 L57_commonName   | character varying(255) |           |          |
 L60_taxonID      | integer                |           |          |
 L60_name         | character varying(255) |           |          |
 L60_commonName   | character varying(255) |           |          |
 L67_taxonID      | integer                |           |          |
 L67_name         | character varying(255) |           |          |
 L67_commonName   | character varying(255) |           |          |
 L70_taxonID      | integer                |           |          |
 L70_name         | character varying(255) |           |          |
 L70_commonName   | character varying(255) |           |          |
Indexes:
    "expanded_taxa_pkey" PRIMARY KEY, btree ("taxonID")
    "idx_expanded_taxa_l10_taxonid" btree ("L10_taxonID")
    "idx_expanded_taxa_l20_taxonid" btree ("L20_taxonID")
    "idx_expanded_taxa_l30_taxonid" btree ("L30_taxonID")
    "idx_expanded_taxa_l40_taxonid" btree ("L40_taxonID")
    "idx_expanded_taxa_l50_taxonid" btree ("L50_taxonID")
    "idx_expanded_taxa_l60_taxonid" btree ("L60_taxonID")
    "idx_expanded_taxa_l70_taxonid" btree ("L70_taxonID")
```
Otherwise you should have enough information about the interfaces and requirements for the check_root_independence() method to implement it fully. 

Return the complete implementation of the method. Assume that the expanded_taxa table is always available. You could use the execute_sql() method from function.sh for the execution of any queries (assuming that this function can return in the form we need), or add a new function if needed to execute the actual SQL commands:
```
#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

....
```
I'm assuming that the DB_CONTAINER, DB_USER, and DB_NAME env vars will be defined in the scope that this function is called. The expanded_taxa table will always be available on the same database as all the other tables we are working with (so just use the DB_ env vars used elsewhere in the flow).

Were there any other 'conceptual' incomplete implementations in your most recent response? If so, you need to return full implementations; taking extra time to think if necessary. 

If not, please return the complete implementation of Section C of clade_helpers.sh:
```
# -------------------------------------------------------------
# C) check_root_independence()
# -------------------------------------------------------------
# This function ensures that each root in a multi-root scenario
# is truly independent. That is, no root is an ancestor or descendant
# of another. We do so by building the set of ancestor IDs for each root,
# then verifying disjointness pairwise.
#
# usage: check_root_independence "myDatabaseName" rootArray
#   rootArray: an array of "rank=taxonID" strings, e.g. "50=47158"
# We assume we can run a quick query on 'expanded_taxa' to gather
# the ~30 possible ancestor columns for each root's row, then compare sets.
#
# If overlap is found, we can either abort or print a warning. We'll choose to abort here.
#
function check_root_independence() {
  local dbName="$1"
  shift
  local roots=("$@")  # e.g. ("50=47158" "60=9999")

  if [ "${#roots[@]}" -le 1 ]; then
    # Nothing to check
    return 0
  fi

  # We'll build arrays of sets. For each root r_i, gather its expanded ancestry.
  declare -A rootSets  # a map from index to "list of taxonIDs"

  for i in "${!roots[@]}"; do
    local pair="${roots[$i]}"
    local rank="${pair%%=*}"
    local tid="${pair##*=}"

    # We'll do a single row fetch in expanded_taxa where taxonID=tid,
    # gather all columns "L5_taxonID", "L10_taxonID", ... "L70_taxonID".
    # Then store them in a set in memory.
    # We'll do a naive approach: psql call, parse results, etc.

    # Real code might do:
    # row=$(docker exec ...)
    # Then parse. For now, we conceptualize a pseudo-result.

    # For demonstration, let's pretend we run:
    # row_of_ancestors might look like "47158|47157|...|<some nulls>"
    # We'll parse them into an array, ignoring nulls.

    # We'll store them in e.g. rootSets["$i"] as "47158 47157 1" etc.
    # PSEUDOCODE:
    # (No real code, just conceptual)

    # rootSets["$i"]="${list_of_ancestors}"

    # For the sake of demonstration, we'll skip real queries.

    # <snip>
    :
  done

  # Now compare pairwise sets for overlap
  for ((i=0; i<${#roots[@]}; i++)); do
    for ((j=i+1; j<${#roots[@]}; j++)); do
      # Compare rootSets["$i"] and rootSets["$j"] for intersection
      # If non-empty => abort
      # PSEUDOCODE:
      # overlapCheck ...
      # if [ "$foundOverlap" = "true" ]; then
      #   echo "ERROR: Overlap detected between root i and j"
      #   return 1
      # fi
      :
    done
  done

  return 0
}
```

Now, looking towards the next steps:
```
	2.	Proposed Next Steps
	•	Step A: Integrate clade_helpers.sh usage in regional_base.sh. For multi-root detection, we call parse_clade_expression(get_clade_condition), partition species, etc.
	•	Step B: Expand check_root_independence() with real SQL queries to gather each root’s ancestor set from expanded_taxa.
	•	Step C: Add boundary logic (get_major_rank_floor()) or minor-rank logic as user toggles in regional_base.sh.
	•	Step D: In cladistic.sh, ensure partial-rank wiping remains consistent with any newly introduced rank columns or edge cases.

```

In your response, please return the complete implementation of Section C on clade_helpers.sh AND prepare a refined, highly specific implementation plan for the remaining steps. Please identify any outstanding clarifications that you will need to return full, robust reimplementations for all remaining steps.
      </file>
    </dir>
    <dir path="docker">
      <dir path="docker/stausee">
        <file path="docker/stausee/entrypoint.sh">
#!/bin/bash
set -e

# Just log and exit - let Docker's default entrypoint handle PostgreSQL
echo "Entrypoint script executed at $(date)"
        </file>
        <file path="docker/stausee/notes.md">
Moving the ibrida psql database to stausee-pool's 'database' dataset.
    'database' dataset is tuned for performance with psql:
```md
3. `stausee-pool/database` (Database Storage):
   - Purpose: PostgreSQL database files
   - Optimizations:
     - recordsize=8K: Matches database page size
     - logbias=latency: Optimized for write performance
     - sync=standard: Ensures data integrity
     - quota=2T: Controlled growth
   - Best for:
     - PostgreSQL data directory
     - High-IOPS workloads
     - Transaction-heavy applications
```
Note that the 'database' dataset is mounted at `mango/database` (`/database` lns here).
        </file>
      </dir>
    </dir>
    <dir path="docs">
      <file path="docs/README.md">
# ibridaDB

**ibridaDB** is a modular, reproducible database system designed to ingest, process, and export biodiversity observations from the [iNaturalist open data dumps](https://www.inaturalist.org/). It leverages PostgreSQL with PostGIS to efficiently store and query geospatial data and includes specialized pipelines for:

- **Data Ingestion:** Importing CSV dumps, calculating geospatial geometries, and updating metadata.
- **Elevation Integration:** Optionally enriching observations with elevation data derived from MERIT DEM tiles.
- **Data Export:** Filtering observations by region and taxonomic clade, performing advanced ancestor searches, and exporting curated CSV files for downstream model training.

This repository contains all the code, Docker configurations, and documentation required to build, run, and extend ibridaDB.

---

## Table of Contents

- [ibridaDB](#ibridadb)
  - [Table of Contents](#table-of-contents)
  - [Overview](#overview)
  - [Architecture](#architecture)
  - [Directory Structure](#directory-structure)
  - [Ingestion Pipeline](#ingestion-pipeline)
  - [Elevation Data Integration](#elevation-data-integration)
  - [Export Pipeline](#export-pipeline)
  - [Docker Build and Deployment](#docker-build-and-deployment)
  - [Configuration \& Environment Variables](#configuration--environment-variables)
  - [Adding a New Release](#adding-a-new-release)
  - [Release notes](#release-notes)
  - [License](#license)
  - [Final Notes](#final-notes)

---

## Overview

**ibridaDB** automates the process of:
- Reproducing a spatially enabled database from iNaturalist open data dumps.
- Optionally enriching the database with elevation data from MERIT DEM.
- Exporting curated subsets of observations for downstream training of specimen identification models.

The system is versioned both in terms of **database structure** (Version, e.g., "v0") and **data release** (Release, e.g., "r1"). These concepts allow you to reproduce different releases of the database while keeping the underlying schema consistent.

---

## Architecture

The overall workflow of ibridaDB is divided into three main stages:

1. **Ingestion:**  
   - **CSV Import:** Load observations, photos, taxa, and observers from CSV files.
   - **Geometry Calculation:** Compute geospatial geometries (using latitude/longitude) with PostGIS.
   - **Metadata Update:** Set version, release, and origin metadata on each table.
   - **Optional Elevation Processing:** If enabled, create a PostGIS raster table for DEM tiles, load MERIT DEM data, and update each observation with an elevation value.

2. **Export:**  
   - **Regional Base Tables:** Build tables that restrict species by geographic bounding boxes and minimum observation counts.
   - **Cladistic Filtering:** Further subset the observations based on taxonomic clade (or metaclade) conditions and other quality filters.
   - **Final CSV Export:** Generate CSV files with additional information (including photo metadata) and optionally include elevation data.

3. **Dockerized Deployment:**  
   - The system runs in a Docker container using a custom image that extends the official PostGIS image to include the `raster2pgsql` CLI tool (necessary for elevation data processing).

A high-level diagram of the export flow is shown below:

```mermaid
flowchart TB
    A["Wrapper Script<br/>(e.g., r1/wrapper.sh)"] --> B["Main Export Script<br/>(common/main.sh)"]
    B --> C{"Skip Regional Base?"}
    C -- "true" --> D["Reuse Existing Tables"]
    C -- "false" --> E["regional_base.sh<br/>(Create/Update Base Tables)"]
    D --> F["cladistic.sh<br/>(Apply Cladistic Filters & Export CSV)"]
    E --> F["cladistic.sh<br/>(Apply Cladistic Filters & Export CSV)"]
    F --> G["Export Summary Generated"]
```

---

## Directory Structure

The repository is organized as follows:

```
ibridaDB/
├── dbTools/
│   ├── ingest/
│   │   └── v0/
│   │       ├── common/
│   │       │   ├── geom.sh              # Geometry calculations
│   │       │   ├── vers_origin.sh       # Version and origin metadata updates
│   │       │   └── main.sh              # Core ingestion logic
│   │       ├── r0/                      # Parameters for the initial release (r0)
│   │       │   └── wrapper.sh
│   │       ├── r1/                      # Parameters for the r1 release
│   │       │   └── wrapper.sh
│   │       └── utils/
│   │           ├── add_release.sh       # Legacy release update script
│   │           └── elevation/           # Elevation pipeline tools
│   │               ├── create_elevation_table.sql
│   │               ├── create_elevation_table.sh
│   │               ├── load_dem.sh
│   │               ├── main.sh          # Orchestrates elevation ingestion
│   │               ├── update_elevation.sh
│   │               └── wrapper.sh
│   └── export/
│       └── v0/
│           ├── common/
│           │   ├── functions.sh         # Shared export functions (including get_obs_columns())
│           │   ├── clade_defns.sh         # Clade condition definitions
│           │   ├── clade_helpers.sh       # Helpers for multi-root and clade processing
│           │   ├── regional_base.sh       # Regional base table generation
│           │   ├── cladistic.sh           # Cladistic filtering and final CSV export
│           │   └── main.sh                # Main export orchestration
│           └── r1/                        # Release-specific export wrappers
│               ├── wrapper_amphibia_all_exc_nonrg_sp.sh
│               ├── wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
│               ├── wrapper_pta_all_exc_nonrg_sp.sh
│               └── wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh
├── docker/
│   ├── Dockerfile                       # Custom Docker image build (with raster2pgsql)
│   └── (other Docker-related files, e.g., docker-compose.yml)
└── README.md                            # This high-level documentation
```

---

## Ingestion Pipeline

The ingestion pipeline is contained in `dbTools/ingest/v0/` and performs the following:

1. **Database Initialization:**  
   - Uses wrapper scripts (e.g., `r1/wrapper.sh`) to set parameters (DB name, source info, etc.).
   - The main script (`common/main.sh`) creates the database, imports CSV files, sets up tables and indexes, and computes geometries via `geom.sh`.

2. **Metadata Updates:**  
   - Updates the `origin`, `version`, and `release` columns in each table (via `vers_origin.sh`).

3. **Elevation Integration (Optional):**  
   - When `ENABLE_ELEVATION=true` is set in the wrapper, the elevation pipeline in `utils/elevation/` is invoked.
   - This pipeline creates an `elevation_raster` table, loads MERIT DEM tiles (using `raster2pgsql`), and updates `observations.elevation_meters`.

**Quick Start Example for Ingestion:**

```bash
chmod +x dbTools/ingest/v0/common/main.sh dbTools/ingest/v0/common/geom.sh dbTools/ingest/v0/common/vers_origin.sh dbTools/ingest/v0/r1/wrapper.sh
# To ingest a new release with elevation:
ENABLE_ELEVATION=true dbTools/ingest/v0/r1/wrapper.sh
```

---

## Elevation Data Integration

The elevation pipeline (located in `dbTools/ingest/v0/utils/elevation/`) provides the following functionality:

- **Create Elevation Table:**  
  - Runs `create_elevation_table.sh` to ensure the `elevation_raster` table exists.

- **Load DEM Data:**  
  - Uses `load_dem.sh` to extract and load MERIT DEM tiles into the database using `raster2pgsql`.  
  - **Note:** This requires the custom Docker image built with `raster2pgsql` (see Docker Build section below).

- **Update Elevation:**  
  - Runs `update_elevation.sh` to populate `observations.elevation_meters` using spatial joins with the raster data.
  
The pipeline is activated by setting `ENABLE_ELEVATION=true` in your ingest wrapper.

---

## Export Pipeline

The export pipeline (located in `dbTools/export/v0/`) allows you to generate specialized CSV exports from your ibridaDB database. Key features include:

1. **Regional Base Table Generation:**  
   - `regional_base.sh` creates base tables filtering species by a geographic bounding box (defined by `REGION_TAG`) and by a minimum number of research-grade observations (`MIN_OBS`).
   - It supports an option (`INCLUDE_OUT_OF_REGION_OBS`) to include all observations for selected species, along with computing an `in_region` boolean.

2. **Cladistic Filtering:**  
   - `cladistic.sh` further filters the regional observations by taxonomic clade (using `CLADE`, `METACLADE`, or `MACROCLADE` defined in `clade_defns.sh`).
   - The script also supports advanced options such as partial rank wiping (using `MIN_OCCURRENCES_PER_RANK` and `INCLUDE_MINOR_RANKS_IN_ANCESTORS`) and research-grade filtering (using `RG_FILTER_MODE`).

3. **CSV Export:**  
   - The final CSV export includes explicit columns from the observations and photo tables.
   - A new column `elevation_meters` is now included in the export if enabled by the environment variable `INCLUDE_ELEVATION_EXPORT` (set to true by default for new releases).
   - The CSV is produced via a partition-based random sampling method, ensuring that in-region research-grade observations are preferentially selected (controlled by `PRIMARY_ONLY` and `MAX_RN`).

**Quick Start Example for Export:**

```bash
chmod +x dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
# Run export (with elevation enabled) using the wrapper:
dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
```

---

## Docker Build and Deployment

**Custom Docker Image:**

The official PostGIS Docker image does not include the `raster2pgsql` CLI tool. To support elevation processing, we build a custom image that:

- Uses a multi-stage Docker build to install the PostGIS package (which includes `raster2pgsql`) in a builder stage.
- Copies the `raster2pgsql` binary into the final image.
- Is used in our Docker Compose configuration.

**Build and Push Instructions:**

```bash
cd docker
docker build -t frontierkodiak/ibridadb:latest . --no-cache
docker login
docker push frontierkodiak/ibridadb:latest
```

**Docker Compose:**

Our `docker-compose.yml` maps necessary volumes for:
- Exporting data (`/datasets/ibrida-data/exports`)
- Ingesting metadata (`/datasets/ibrida-data/intake`)
- Providing DEM data (`/datasets/dem`)

Refer to the `docker/stausee/docker-compose.yml` file for details.

---

## Configuration & Environment Variables

Both the ingest and export pipelines are configured via a rich set of environment variables. Key ones include:

- **Database Configuration:**  
  - `DB_USER`, `VERSION_VALUE`, `RELEASE_VALUE`, `ORIGIN_VALUE`, `DB_NAME`, `DB_CONTAINER`

- **Ingestion-Specific:**  
  - `SOURCE`, `METADATA_PATH`, `ENABLE_ELEVATION`, `DEM_DIR`, `EPSG`, `TILE_SIZE`

- **Export-Specific:**  
  - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`
  - Taxonomic filters: `CLADE`, `METACLADE`, `MACROCLADE`
  - Advanced export toggles:  
    - `INCLUDE_OUT_OF_REGION_OBS`  
    - `RG_FILTER_MODE`  
    - `MIN_OCCURRENCES_PER_RANK`  
    - `INCLUDE_MINOR_RANKS_IN_ANCESTORS`  
    - **`INCLUDE_ELEVATION_EXPORT`** – controls whether `elevation_meters` is included in the final export.

- **Paths:**  
  - `HOST_EXPORT_BASE_PATH`, `CONTAINER_EXPORT_BASE_PATH`, `EXPORT_SUBDIR`, `BASE_DIR`

For full details, please consult the export and ingest wrapper scripts in their respective directories.

---

## Adding a New Release

To add a new data release:
1. Create a new release directory (e.g., `dbTools/ingest/v0/r2/` and `dbTools/export/v0/r2/`).
2. Copy an existing wrapper script (from r1) into the new directory.
3. Update parameters such as:
   - `SOURCE` (e.g., change from "Dec2024" to "Feb2025")
   - `RELEASE_VALUE` (e.g., from "r1" to "r2")
   - Any new configuration (e.g., enable elevation by setting `ENABLE_ELEVATION=true` in ingest and `INCLUDE_ELEVATION_EXPORT=true` in export).
4. Run the ingestion/export processes using the new wrapper scripts.

## Release notes

Current versions:
- v0r0: June 2024 iNat data release
- v0r1: December 2024 iNat data release (adds anomaly_score column to observations table)
  - (in-place update) added elevation_meters column to observations tables
- v0r2: February 2025 iNat data release (built with elevation data)

---

## License

[Insert License Information Here]

---

## Final Notes

- **Documentation Updates:**  
  This README provides a high-level overview. For detailed configuration of the ingest and export pipelines, please refer to:
  - `dbTools/ingest/v0/INGEST.md` (in progress)  
  - `dbTools/export/v0/export.md`

- **Contributions:**  
  Contributions and suggestions are welcome. Please submit pull requests or open issues if you encounter problems or have ideas for enhancements.

---

Happy Ingesting and Exporting!
      </file>
      <file path="docs/coldp_integration.md">
# ColDP Integration Documentation

This document provides detailed information about the integration of Catalog of Life Data Package (ColDP) data into ibridaDB. It covers the data ingestion process, mapping to iNaturalist taxa, and the use of ColDP data to enhance biodiversity information.

## Overview

The ColDP integration pipeline consists of three main steps:

1. **Loading ColDP Tables**: Ingesting the raw TSV files from a ColDP export into staging tables
2. **Mapping to iNaturalist Taxa**: Creating a mapping between iNaturalist taxa and ColDP taxa through exact and fuzzy matching
3. **Enriching Expanded Taxa**: Updating the expanded_taxa table with common names and other data from ColDP

## ColDP Data Structure

ColDP (Catalog of Life Data Package) is a standardized format for sharing taxonomic data. It consists of several TSV files, each representing a different aspect of taxonomic information. The key files ingested by ibridaDB include:

- **NameUsage.tsv**: Core taxonomic information (scientific names, status, hierarchy)
- **VernacularName.tsv**: Common names in various languages
- **Distribution.tsv**: Geographic distribution information
- **Media.tsv**: Links to images, sounds, and other media
- **Reference.tsv**: Bibliographic references
- **TypeMaterial.tsv**: Type specimen information

## Database Tables

### 1. ColDP Staging Tables

These tables directly mirror the structure of the ColDP TSV files, serving as the initial landing point for the data.

#### ColdpNameUsage

**Purpose**: Stores scientific names and taxonomic information from the ColDP NameUsage.tsv file.

**Schema**:
```sql
CREATE TABLE coldp_name_usage_staging (
    ID                  VARCHAR(64) PRIMARY KEY,
    scientificName      TEXT INDEX,
    authorship          TEXT,
    rank                VARCHAR(50) INDEX,
    status              VARCHAR(50) INDEX,
    parentID            VARCHAR(64),
    
    -- Name components
    uninomial           TEXT,
    genericName         TEXT,
    infragenericEpithet TEXT,
    specificEpithet     TEXT,
    infraspecificEpithet TEXT,
    basionymID          VARCHAR(64),
    
    -- Higher taxonomy for homonym resolution
    family              TEXT,
    order               TEXT,
    class_              TEXT,
    phylum              TEXT,
    kingdom             TEXT
);
```

**Key Fields**:
- `ID`: Primary identifier from Catalog of Life
- `scientificName`: Full scientific name including authorship
- `rank`: Taxonomic rank (e.g., "species", "genus")
- `status`: Status of the name (e.g., "accepted", "synonym")
- Taxonomic hierarchy fields (family, order, class_, etc.) to help resolve homonyms

#### ColdpVernacularName

**Purpose**: Stores common names for taxa in various languages.

**Schema**:
```sql
CREATE TABLE coldp_vernacular_name (
    id               INTEGER PRIMARY KEY AUTOINCREMENT,
    taxonID          VARCHAR(10) INDEX NOT NULL,
    sourceID         VARCHAR(10),
    name             TEXT NOT NULL,
    transliteration  TEXT,
    language         VARCHAR(3),      -- ISO‑639‑3
    preferred        BOOLEAN,
    country          VARCHAR(10),     -- ISO‑3166‑1‑alpha‑2
    area             TEXT,
    sex              VARCHAR(20),
    referenceID      VARCHAR(64),
    remarks          TEXT
);
```

**Key Fields**:
- `id`: Auto-incrementing primary key
- `taxonID`: Foreign key to ColdpNameUsage.ID
- `name`: The vernacular/common name
- `language`: ISO 639-3 language code (e.g., "eng" for English)
- `preferred`: Boolean flag indicating if this is the preferred common name

#### ColdpDistribution

**Purpose**: Contains geographic distribution information for taxa.

**Schema**:
```sql
CREATE TABLE coldp_distribution (
    id             INTEGER PRIMARY KEY AUTOINCREMENT,
    taxonID        VARCHAR(10) INDEX,
    sourceID       VARCHAR(10),
    areaID         VARCHAR(10),
    area           TEXT,
    gazetteer      VARCHAR(10),
    status         VARCHAR(25),     -- e.g. native, introduced
    referenceID    VARCHAR(64),
    remarks        TEXT
);
```

#### ColdpMedia

**Purpose**: Links to images, sounds, videos, and other media for taxa.

**Schema**:
```sql
CREATE TABLE coldp_media (
    id             INTEGER PRIMARY KEY AUTOINCREMENT,
    taxonID        VARCHAR(10) INDEX,
    sourceID       VARCHAR(10),
    url            TEXT NOT NULL,
    type           VARCHAR(50),     -- stillImage, sound, video …
    format         VARCHAR(50),     -- MIME type or file suffix
    title          TEXT,
    created        DATE,
    creator        TEXT,
    license        VARCHAR(100),
    link           TEXT,           -- landing page
    remarks        TEXT
);
```

#### ColdpReference

**Purpose**: Stores bibliographic references for taxonomic information.

**Schema**:
```sql
CREATE TABLE coldp_reference (
    ID                  VARCHAR(64) PRIMARY KEY,   -- UUID or short key
    alternativeID       VARCHAR(64),
    sourceID            VARCHAR(10),
    citation            TEXT,
    type                VARCHAR(30),
    author              TEXT,
    editor              TEXT,
    title               TEXT,
    titleShort          TEXT,
    containerAuthor     TEXT,
    containerTitle      TEXT,
    containerTitleShort TEXT,
    issued              VARCHAR(50),
    accessed            VARCHAR(50),
    collectionTitle     TEXT,
    collectionEditor    TEXT,
    volume              VARCHAR(30),
    issue               VARCHAR(30),
    edition             VARCHAR(30),
    page                VARCHAR(50),
    publisher           TEXT,
    publisherPlace      TEXT,
    version             VARCHAR(30),
    isbn                VARCHAR(20),
    issn                VARCHAR(20),
    doi                 VARCHAR(100),
    link                TEXT,
    remarks             TEXT
);
```

#### ColdpTypeMaterial

**Purpose**: Information about type specimens for taxonomic names.

**Schema**:
```sql
CREATE TABLE coldp_type_material (
    ID                  VARCHAR(64) PRIMARY KEY,
    nameID              VARCHAR(10) INDEX,
    sourceID            VARCHAR(10),
    citation            TEXT,
    status              VARCHAR(50),
    referenceID         VARCHAR(64),
    page                VARCHAR(50),
    country             VARCHAR(2),
    locality            TEXT,
    latitude            NUMERIC(9,5),
    longitude           NUMERIC(9,5),
    altitude            VARCHAR(50),
    sex                 VARCHAR(12),
    host                TEXT,
    associatedSequences TEXT,
    date                DATE,
    collector           TEXT,
    institutionCode     VARCHAR(25),
    catalogNumber       VARCHAR(50),
    link                TEXT,
    remarks             TEXT
);
```

### 2. Mapping Table

#### InatToColdpMap

**Purpose**: Cross-reference between iNaturalist taxa and Catalog of Life taxa, enabling integration of ColDP data with iNaturalist observations.

**Schema**:
```sql
CREATE TABLE inat_to_coldp_taxon_map (
    inat_taxon_id       INTEGER REFERENCES expanded_taxa(taxonID),
    col_taxon_id        VARCHAR(64) REFERENCES coldp_name_usage_staging(ID),
    match_type          VARCHAR(50) NOT NULL,  -- exact_name_rank, exact_name_only, fuzzy_name
    match_score         FLOAT,
    inat_scientific_name TEXT,
    col_scientific_name TEXT,
    PRIMARY KEY (inat_taxon_id, col_taxon_id)
);
```

**Key Fields**:
- `inat_taxon_id`: References the taxonID in expanded_taxa
- `col_taxon_id`: References the ID in coldp_name_usage_staging
- `match_type`: Describes how the match was made:
  - `exact_name_rank_accepted`: Exact match on name and rank with accepted status
  - `exact_name_rank_other_status`: Exact match on name and rank with non-accepted status
  - `exact_name_only_accepted`: Exact match on name only with accepted status
  - `exact_name_only_other_status`: Exact match on name only with non-accepted status
  - `fuzzy_name_single_match`: Single fuzzy match above threshold
  - `fuzzy_name_highest_score`: Highest scoring fuzzy match when multiple matches exist
  - `fuzzy_name_with_ancestors`: Fuzzy match using ancestor data to resolve homonyms
  - `fuzzy_name_no_ancestors`: Fuzzy match without ancestor data
- `match_score`: Confidence score (1.0 for exact matches, < 1.0 for fuzzy matches)

## Integration Process

### 1. Loading ColDP Data

The ColDP data loading process is handled by the `load_tables.py` script, which:

1. Creates the necessary tables in the database
2. Reads the TSV files from the ColDP export
3. Processes and cleans the data
4. Loads the data into the corresponding tables

### 2. Mapping Taxa

The mapping process is handled by the `map_taxa.py` script, which employs a multi-stage matching approach:

1. **Exact Name + Rank Matching**: First attempts to find exact matches on both scientific name and taxonomic rank, prioritizing taxa with "accepted" status
2. **Exact Name-Only Matching**: For taxa without exact name+rank matches, attempts to match on scientific name alone
3. **Fuzzy Matching**: For remaining unmatched taxa, uses fuzzy string matching with the rapidfuzz library
4. **Homonym Resolution**: When multiple fuzzy matches exist, uses taxonomic hierarchy information to resolve homonyms

The mapping results are stored in the `inat_to_coldp_taxon_map` table, which acts as a bridge between iNaturalist and Catalog of Life taxonomies.

### 3. Populating Common Names

Once the mapping is complete, the `populate_common_names.py` script:

1. Uses the mapping to identify corresponding ColDP taxa for iNaturalist taxa
2. Retrieves preferred common names from the `coldp_vernacular_name` table
3. Updates the `commonName` field in the `expanded_taxa` table
4. Additionally updates the `LXX_commonName` fields for ancestral ranks

## Usage Examples

### Retrieving Common Names for a Taxon

```sql
-- Get the common name for a specific taxon
SELECT t.taxonID, t.name, t.commonName
FROM expanded_taxa t
WHERE t.taxonID = 12345;

-- Get all taxa with common names from a specific family
SELECT t.taxonID, t.name, t.commonName
FROM expanded_taxa t
WHERE t.L30_taxonID = 67890 -- Family taxon ID
AND t.commonName IS NOT NULL;
```

### Finding Taxa with Distribution Information

```sql
-- Get distribution information for a specific taxon
SELECT t.name, d.area, d.status
FROM expanded_taxa t
JOIN inat_to_coldp_taxon_map m ON t.taxonID = m.inat_taxon_id
JOIN coldp_distribution d ON m.col_taxon_id = d.taxonID
WHERE t.taxonID = 12345;
```

### Accessing Media Links

```sql
-- Get media links for a specific taxon
SELECT t.name, m.type, m.url, m.license
FROM expanded_taxa t
JOIN inat_to_coldp_taxon_map map ON t.taxonID = map.inat_taxon_id
JOIN coldp_media m ON map.col_taxon_id = m.taxonID
WHERE t.taxonID = 12345;
```

## Performance Considerations

- The fuzzy matching process is computationally intensive and may take significant time for large taxonomic datasets
- Batch processing (1,000 records at a time) is used to manage memory usage during fuzzy matching
- Indexes are created on frequently queried columns to improve performance
- The mapping table facilitates efficient joins between iNaturalist and ColDP data

## Future Enhancements

Potential future enhancements to the ColDP integration include:

1. Improved homonym resolution using more advanced techniques
2. Integration of additional ColDP data types (e.g., specimens, interactions)
3. Periodic synchronization with updated ColDP releases
4. Extension to other taxonomic authorities beyond Catalog of Life

## References

- [Catalog of Life Data Package (ColDP) Specification](https://github.com/CatalogueOfLife/coldp)
- [Rapidfuzz Documentation](https://github.com/maxbachmann/RapidFuzz)
      </file>
      <file path="docs/export.md">
# ibridaDB Export Reference (v1)

This document describes how to configure and run an **ibridaDB** export job using our current export pipeline. The export process is driven by a set of **environment variables** that control which observations are included, how they are filtered, and where the outputs are written. These variables are typically set in a release‐specific wrapper script (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`).

> **Note:** This document has been updated to include new functionality such as ancestor‐based filtering, partial rank wiping, the `in_region` flag with out‐of‐region observation handling, and an optional inclusion of elevation data (via the `elevation_meters` column). See also the [Ingest Documentation](../ingest/INGEST.md) for details on how elevation data is integrated into the database.

---

## Table of Contents

- [ibridaDB Export Reference (v1)](#ibridadb-export-reference-v1)
  - [Table of Contents](#table-of-contents)
  - [Introduction \& Pipeline Overview](#introduction--pipeline-overview)
  - [Quick Start](#quick-start)
  - [Environment Variables](#environment-variables)
    - [Database Configuration](#database-configuration)
    - [Export Parameters](#export-parameters)
    - [Additional Flags and Advanced Settings](#additional-flags-and-advanced-settings)
    - [Paths](#paths)
  - [Export Flow \& Scripts](#export-flow--scripts)
  - [Output Files](#output-files)
  - [Future Work and Notes](#future-work-and-notes)

---

## Introduction & Pipeline Overview

The **ibridaDB Export Pipeline** extracts curated subsets of observations from a spatially enabled PostgreSQL/PostGIS database. It is designed to support advanced filtering based on:

- **Geographic Region:** Defined by a bounding box corresponding to a region tag (e.g., `"NAfull"`).
- **Species Observations:** A minimum number of research-grade observations per species (`MIN_OBS`) is required to be included.
- **Taxonomic Clade:** Filtering by a clade, metaclade, or macroclade is performed based on definitions in `clade_defns.sh`.
- **Quality & Ancestor Filtering:** The pipeline supports research-grade filtering (`RG_FILTER_MODE`), optional partial-rank wiping (using `MIN_OCCURRENCES_PER_RANK` and `INCLUDE_MINOR_RANKS_IN_ANCESTORS`), and computes an `in_region` boolean for each observation.
- **Elevation Data (Optional):** When enabled (via `INCLUDE_ELEVATION_EXPORT`), the final exported CSV will include an `elevation_meters` column immediately after `longitude`.

The export process follows these broad stages:

1. **Wrapper Script:** Sets all required environment variables.
2. **Main Export Script (`common/main.sh`):**
   - Validates variables and creates the export directory.
   - Ensures necessary PostgreSQL extensions and roles exist.
   - Calls **regional_base.sh** to generate region-based tables.
   - Invokes **cladistic.sh** to filter by clade and produce the final export table.
   - Generates a summary file with environment details and final statistics.
3. **Regional Base Generation (`common/regional_base.sh`):**
   - Sets the region's bounding box.
   - Creates tables that capture species meeting the `MIN_OBS` threshold and computes an `in_region` flag.
   - Builds ancestor tables based on clade definitions.
4. **Cladistic Filtering & CSV Export (`common/cladistic.sh`):**
   - Joins the regional base observations to taxonomic data from `expanded_taxa`.
   - Applies research-grade filters, partial-rank wiping, and random sampling (using `MAX_RN`) per species.
   - Exports a final CSV that explicitly lists observation and photo columns (including `elevation_meters` if enabled).

A high-level diagram of the export flow is shown below:

```mermaid
flowchart TB
    A["Wrapper Script<br/>(e.g., r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh)"] --> B["Main Export Script<br/>(common/main.sh)"]
    B --> C{"Control Flags?"}
    C -- "SKIP_ALL_SP_TABLE=true" --> D["Reuse Existing Species Table"]
    C -- "SKIP_ALL_SP_TABLE=false" --> E["Create/Update Species Table"]
    C -- "SKIP_ANCESTORS_TABLE=true" --> F["Reuse Existing Ancestor Tables"]
    C -- "SKIP_ANCESTORS_TABLE=false" --> G["Create New Ancestor Tables"]
    D --> H["cladistic.sh<br/>(Filter Observations & Export CSV)"]
    E --> H
    F --> H
    G --> H
    H --> I["Export Summary & Log Generation"]
```

---

## Quick Start

1. **Clone or navigate** to the `dbTools/export/v0` directory.
2. **Configure a wrapper script** (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`) to set your export parameters. For example:
   - Set region-specific parameters: `REGION_TAG`, `MIN_OBS`, `MAX_RN`, and `PRIMARY_ONLY`.
   - Specify the clade filter: `CLADE` or `METACLADE`.
   - Enable out-of-region observation handling: `INCLUDE_OUT_OF_REGION_OBS`.
   - Optionally, enable elevation export: `INCLUDE_ELEVATION_EXPORT=true` (if your database includes elevation data).
3. **Run the wrapper script** to initiate the export process:
   ```bash
   ./r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
   ```
4. **Review the output**:
   - A CSV file named `<EXPORT_GROUP>_photos.csv` will be saved to the export directory.
   - A summary file (`<EXPORT_GROUP>_export_summary.txt`) will be generated containing export parameters, statistics, and timing information.

---

## Environment Variables

The export pipeline is configured via several environment variables, which are typically set in your wrapper script.

### Database Configuration

- **`DB_USER`**  
  *Description:* PostgreSQL user (e.g., `"postgres"`).

- **`VERSION_VALUE`**  
  *Description:* Database version identifier (e.g., `"v0"`).

- **`RELEASE_VALUE`**  
  *Description:* Data release identifier (e.g., `"r1"`).  
  *Note:* For older releases (e.g., `"r0"`), certain features (such as `anomaly_score` and `elevation_meters`) may be absent.

- **`ORIGIN_VALUE`**  
  *Description:* Data provenance (e.g., `"iNat-Dec2024"`).

- **`DB_NAME`**  
  *Description:* Name of the database (e.g., `"ibrida-v0-r1"`).

- **`DB_CONTAINER`**  
  *Description:* Name of the Docker container running PostgreSQL (e.g., `"ibridaDB"`).

### Export Parameters

- **`REGION_TAG`**  
  *Description:* A key that defines the region's bounding box (e.g., `"NAfull"`).  
  *Usage:* Used in `regional_base.sh` to set geographic coordinates.

- **`MIN_OBS`**  
  *Description:* Minimum number of research-grade observations required per species for inclusion.  
  *Default:* `50`.

- **`MAX_RN`**  
  *Description:* Maximum number of research-grade observations to sample per species in the final CSV.  
  *Default:* `2500` (or your desired value).

- **`PRIMARY_ONLY`**  
  *Description:* If `true`, only the primary photo (position=0) is included; if `false`, all photos are exported.

- **`CLADE` / `METACLADE` / `MACROCLADE`**  
  *Description:* Defines the taxonomic filter. For example, `CLADE="amphibia"` or `METACLADE="pta"` (primary terrestrial arthropods).

- **`EXPORT_GROUP`**  
  *Description:* A label for the final export; used to name the final observations table and CSV file (e.g., `"amphibia_all_exc_nonrg_sp_oor_elev"`).

### Additional Flags and Advanced Settings

- **`PROCESS_OTHER`**  
  *Description:* A generic flag for additional processing (default: `false`).

- **`SKIP_REGIONAL_BASE`**  
  *Description:* Master flag that controls whether to reuse existing regional base tables rather than recreating them.  
  *Note:* This acts as a default for the more granular flags below if they are not explicitly set.

- **`SKIP_ALL_SP_TABLE`**  
  *Description:* Controls whether to reuse or recreate the base species table. When running multiple exports for different clades against the same region with the same `MIN_OBS`, set this to `true` to reuse the common species table.  
  *Default:* Inherits value from `SKIP_REGIONAL_BASE` if not explicitly set.

- **`SKIP_ANCESTORS_TABLE`**  
  *Description:* Controls whether to reuse or recreate the clade-specific ancestor tables. These tables are unique to each clade, so typically this should be `false` when running exports for different clades.  
  *Default:* Inherits value from `SKIP_REGIONAL_BASE` if not explicitly set.

- **`INCLUDE_OUT_OF_REGION_OBS`**  
  *Description:* If `true`, once a species is selected by `MIN_OBS`, all observations for that species (globally) are included; otherwise, only those within the bounding box are used.  
  *Note:* An `in_region` boolean is computed for each observation.

- **`RG_FILTER_MODE`**  
  *Description:* Controls how research-grade versus non-research observations are filtered.  
  *Possible values:* `ONLY_RESEARCH`, `ALL`, `ALL_EXCLUDE_SPECIES_NON_RESEARCH`, `ONLY_NONRESEARCH`, etc.

- **`MIN_OCCURRENCES_PER_RANK`**  
  *Description:* Minimum occurrences required per rank (e.g., L20, L30, L40) before that rank is retained.  
  *Usage:* Used to optionally wipe out low-occurrence partial rank labels.

- **`INCLUDE_MINOR_RANKS_IN_ANCESTORS`**  
  *Description:* If `true`, includes minor ranks in the ancestor search; otherwise, only major ranks are considered.

- **`INCLUDE_ELEVATION_EXPORT`**  
  *Description:* If `true` (the default for new releases), the final export will include the `elevation_meters` column (placed immediately after `longitude`).  
  *Note:* If the underlying database is older (e.g., release `"r0"`), set this to `false`.

### Paths

- **`HOST_EXPORT_BASE_PATH`**  
  *Description:* Host filesystem path where export files will be written (e.g., `"/datasets/ibrida-data/exports"`).

- **`CONTAINER_EXPORT_BASE_PATH`**  
  *Description:* Container path corresponding to `HOST_EXPORT_BASE_PATH` (e.g., `"/exports"`).

- **`EXPORT_SUBDIR`**  
  *Description:* A subdirectory constructed from variables (e.g., `"v0/r1/primary_only_50min_2500max"`).

- **`BASE_DIR`**  
  *Description:* Root directory of the export tools (e.g., `/home/caleb/repo/ibridaDB/dbTools/export/v0`).

---

## Export Flow & Scripts

The export process consists of several key steps:

1. **Wrapper Script:**  
   - A release-specific wrapper (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`) sets all necessary environment variables (including the new `INCLUDE_ELEVATION_EXPORT` toggle) and then calls the main export script.

2. **Main Export Script (`common/main.sh`):**  
   - Validates required variables and creates the export directory.
   - Installs necessary PostgreSQL extensions (such as `dblink`) and creates roles if needed.
   - Invokes **regional_base.sh** to generate region-specific base tables.
   - Calls **cladistic.sh** to join base tables with the taxonomic hierarchy (from `expanded_taxa`), apply quality and clade filters, and build the final export table.
   - Generates a summary file that documents the export parameters, final observation counts, and timing information.
   - Optionally copies the wrapper script into the output directory for reproducibility.

3. **Regional Base Generation (`common/regional_base.sh`):**  
   - Determines the geographic bounding box using `REGION_TAG` (from `region_defns.sh`).
   - Creates or reuses the species table (`<REGION_TAG>_min${MIN_OBS}_all_sp`) based on `SKIP_ALL_SP_TABLE`.
   - Builds or reuses an ancestor table (`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors_<cladeID>_<mode>`) based on `SKIP_ANCESTORS_TABLE`.
   - Generates or reuses a second table (`<REGION_TAG>_min${MIN_OBS}_sp_and_ancestors_obs_<cladeID>_<mode>`) based on `SKIP_ANCESTORS_TABLE`.
   - The `INCLUDE_OUT_OF_REGION_OBS` flag governs whether the observation table is filtered by the bounding box or not.

4. **Cladistic Filtering & CSV Export (`common/cladistic.sh`):**  
   - Joins the observation table with `expanded_taxa` using clade conditions defined in `clade_defns.sh` (and processed by `clade_helpers.sh`).
   - Applies research-grade filtering based on `RG_FILTER_MODE` and uses a partition-based random sampling (controlled by `MAX_RN` and `PRIMARY_ONLY`).
   - Explicitly enumerates columns in the final export, including `elevation_meters` (if `INCLUDE_ELEVATION_EXPORT=true`), ensuring that the column appears immediately after `longitude`.
   - Exports the final dataset as a CSV file with a header and tab-delimited fields.
   - Debug SQL is executed to confirm the final column list used.

---

## Output Files

After a successful export, you will find:

- A CSV file named `<EXPORT_GROUP>_photos.csv` in the export subdirectory (e.g., `/exports/v0/r1/primary_only_50min_2500max`).
- A summary file named `<EXPORT_GROUP>_export_summary.txt` that documents:
  - The values of key environment variables.
  - Final observation, taxa, and observer counts.
  - Timing information for each stage of the export process.
- Optionally, a copy of the export wrapper script (if `WRAPPER_PATH` is set).

---

## Future Work and Notes

- **Enhanced Documentation:**  
  Future revisions will further detail the logic of ancestor searches and the random sampling strategy used for research-grade observations.
  
- **Additional Filtering Options:**  
  We plan to refine the `RG_FILTER_MODE` and allow further customizations (e.g., combining multiple quality filters).

- **Schema Updates:**  
  As new releases are introduced (e.g., additional columns beyond `anomaly_score` or `elevation_meters`), the export documentation will be updated to reflect the schema changes.

- **User Feedback:**  
  Contributions, bug reports, and suggestions for improvements are welcome.

---

Happy Exporting!
      </file>
      <file path="docs/ingest.md">
# ibridaDB Ingestion Documentation

This document provides a detailed overview of the ibridaDB ingestion pipeline. It covers the steps to load iNaturalist data into a spatially enabled PostgreSQL/PostGIS database, how the elevation integration is handled, and how to run the ingestion process using the provided wrapper scripts.

---

## 1. Overview of the Ingestion Flow

The ingestion pipeline for ibridaDB is designed to:
- **Initialize the Database:** Create a new database using a template (typically a PostGIS-enabled template).
- **Import Data:** Load CSV data for observations, photos, taxa, and observers from iNaturalist data dumps.
- **Compute Geometries:** Generate geospatial geometry columns from the latitude and longitude fields.
- **Update Metadata:** Populate additional columns such as `origin`, `version`, and `release` on each table.
- **Integrate Elevation (Optional):**  
  - **Fresh Ingestion Scenario:** If you are initializing a new database and want to include elevation data, the main ingestion script (in `common/main.sh`) can trigger the elevation pipeline automatically when `ENABLE_ELEVATION=true` is set.
  - **Existing Database Scenario:** If your database already exists and lacks elevation data, you can run the elevation wrapper (in `utils/elevation/wrapper.sh`) to update the observations with DEM-derived elevation values.

After the ingestion steps are complete, the database is ready for further processing or for exporting subsets of observations using the export pipeline.

---

## 2. Environment Variables

The ingestion pipeline is controlled by several environment variables. The key ones include:

### Database and General Settings
- **`DB_USER`**  
  PostgreSQL user (typically `"postgres"`).

- **`DB_TEMPLATE`**  
  Template database name (e.g., `"template_postgis"`) used to create the new database.

- **`NUM_PROCESSES`**  
  Number of parallel processes to use for tasks such as geometry calculations and metadata updates.

- **`BASE_DIR`**  
  Root directory of the ingestion tools (e.g., `/home/caleb/repo/ibridaDB/dbTools/ingest/v0`).

- **`SOURCE`**  
  Source identifier for the iNaturalist data (e.g., `"Dec2024"`, `"Feb2025"`).

- **`METADATA_PATH`**  
  Path to the CSV files containing iNaturalist data (e.g., `/datasets/ibrida-data/intake/Dec2024`).

### Versioning and Release
- **`ORIGIN_VALUE`**  
  Describes the data provenance (e.g., `"iNat-Dec2024"`).

- **`VERSION_VALUE`**  
  Database version identifier (e.g., `"v0"`).

- **`RELEASE_VALUE`**  
  Data release identifier (e.g., `"r1"`).

- **`DB_NAME`**  
  Name of the new database (e.g., `"ibrida-v0-r1"`).

- **`DB_CONTAINER`**  
  Name of the Docker container running PostgreSQL (e.g., `"ibridaDB"`).

### Elevation Integration Settings
- **`ENABLE_ELEVATION`**  
  If set to `"true"`, the ingestion process will invoke the elevation pipeline.  
  - *Fresh Ingestion:* When creating a new database, the main ingestion script calls the elevation pipeline after geometry calculation.  
  - *Existing DB Update:* You may also run the elevation wrapper separately to add or update elevation values.
- **`DEM_DIR`**  
  Path to the directory containing the MERIT DEM `.tar` files (e.g., `"/datasets/dem/merit"`).
- **`EPSG`**  
  EPSG code for the DEM data (default: `"4326"`).
- **`TILE_SIZE`**  
  Tile size for processing DEM data (default: `"100x100"`).

---

## 3. Ingestion Flow Details

### A. Database Initialization and CSV Import

1. **Database Creation:**  
   - The ingestion pipeline starts by dropping any existing database with the target name and creating a fresh database using the specified template.
2. **Table Creation:**  
   - Tables are created using a provided structure SQL file (e.g., `r1/structure.sql`).
3. **Data Import:**  
   - The pipeline imports CSV files for observations, photos, taxa, and observers into the respective tables using PostgreSQL’s `COPY` command.
4. **Index Creation:**  
   - Key indexes are created to optimize spatial and text-based queries (including geospatial indexes on the geometry column).

### B. Geometry Calculation

- The script `common/geom.sh` is executed in parallel to compute the `geom` column on the `observations` table from the `latitude` and `longitude` fields.
- A PostGIS GIST index is then created on the new geometry column to speed up spatial queries.

### C. Metadata Update

- The script `common/vers_origin.sh` is run to add and populate the `origin`, `version`, and `release` columns on all tables.
- This process is executed in parallel across tables to speed up the update.

### D. Elevation Integration

The elevation pipeline can be integrated in one of two ways:

#### 1. During Fresh Ingestion
- **Integration via Main Ingestion Script:**  
  When the environment variable `ENABLE_ELEVATION` is set to `"true"`, the main ingestion script (`common/main.sh`) calls the elevation pipeline after geometry calculation. This pipeline performs the following steps:
  - **Create Elevation Table:**  
    The script `utils/elevation/create_elevation_table.sh` ensures that the `elevation_raster` table exists.
  - **Load DEM Data:**  
    The script `utils/elevation/load_dem.sh` extracts DEM tiles from `.tar` archives and loads them into the `elevation_raster` table using `raster2pgsql` (which requires the custom Docker image).
  - **Update Elevation Values:**  
    Finally, `utils/elevation/update_elevation.sh` updates the `observations.elevation_meters` column for each observation based on a spatial join with the DEM data.

#### 2. Updating an Existing Database
- **Separate Elevation Wrapper:**  
  If you have an existing database (e.g., from a previous release) and you need to add or update elevation values, you can run the elevation wrapper script located at `utils/elevation/wrapper.sh`. This script sets the appropriate environment variables and calls the elevation main script to update the database.

---

## 4. Example Wrapper Usage

Below is an example wrapper script for a fresh ingestion (with elevation enabled):

```bash
#!/bin/bash
# Example: dbTools/ingest/v0/r1/wrapper.sh

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/wrapper_$(date +%Y%m%d_%H%M%S).log"
echo "Starting ingestion at $(date)" > "${LOG_FILE}"

# Export environment variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"
export SOURCE="Feb2025"
export METADATA_PATH="/datasets/ibrida-data/intake/Feb2025"
export ORIGIN_VALUE="iNat-Feb2025"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r2"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r2/structure.sql"

# Enable elevation integration in this ingestion
export ENABLE_ELEVATION=true
export DEM_DIR="/datasets/dem/merit"
export EPSG="4326"
export TILE_SIZE="100x100"

# Execute the main ingestion script
/home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/main.sh

# End of wrapper
echo "Ingestion process complete at $(date)" >> "${LOG_FILE}"
```

For an existing database update, simply run the elevation wrapper:

```bash
#!/bin/bash
# Example: dbTools/ingest/v0/utils/elevation/wrapper.sh

export DB_NAME="ibrida-v0-r1"
export DB_USER="postgres"
export DB_CONTAINER="ibridaDB"
export DEM_DIR="/datasets/dem/merit"
export NUM_PROCESSES=16
export EPSG="4326"
export TILE_SIZE="100x100"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

./utils/elevation/main.sh "$DB_NAME" "$DB_USER" "$DB_CONTAINER" "$DEM_DIR" "$NUM_PROCESSES" "$EPSG" "$TILE_SIZE"
```

---

## 5. Verifying Success

After running the ingestion process, verify that:

- The database is created with all required tables and indexes.
- The CSV files have been successfully imported into the tables.
- The `geom` column on `observations` is correctly computed and indexed.
- If elevation was enabled, the `elevation_raster` table exists and the `observations.elevation_meters` column is populated (for rows with valid DEM coverage).
- Log messages and notifications (if configured) confirm each step’s completion.

---

## Final Notes

- This document covers the ingestion side of ibridaDB. For exporting subsets of the data, please see [export.md](../export/export.md) for detailed instructions on the export pipeline.
- Ensure that you use the custom Docker image (with `raster2pgsql` installed) when running ingestion with elevation data.
- For new releases, adjust your wrapper scripts as needed and consider setting up separate directories (e.g., r1, r2) to maintain versioning consistency.

Happy Ingesting!
      </file>
      <file path="docs/roadmap.md">

**positional_accuracy** usage
- Thought: perhaps 'smudge' the elevation_meters value; weighted average of elevation tiles within the circle whose radius is the positional accuracy.
      </file>
      <file path="docs/schemas.md">
# ibridaDB Schemas Reference

This document provides a detailed reference for all of the schemas used in ibridaDB. It covers:

1. The core tables imported directly from iNaturalist.
2. The **expanded_taxa** table generated from the iNaturalist `taxa` table.
3. The final export table produced by the export pipeline.
4. The ColDP integration tables for taxonomic data from Catalog of Life.
5. Supplementary information on data types, indexing, and version-specific differences.
6. An appendix with sample SQL output (\d results) for quick reference.

This reference is intended to help developers and maintainers quickly understand the structure, data types, and intended usage of each table without needing to manually inspect the database.

---

## Table of Contents

1. [Core iNaturalist Tables](#core-inaturalist-tables)  
   1.1. [Observations](#observations)  
   1.2. [Photos](#photos)  
   1.3. [Observers](#observers)  
   1.4. [Taxa](#taxa)

2. [Expanded Taxa Table](#expanded-taxa-table)  
   2.1. [Purpose and Generation](#purpose-and-generation)  
   2.2. [Schema Details](#schema-details)  
   2.3. [Indexing and Performance Considerations](#indexing-and-performance-considerations)  
   2.4. [Rank-Level Mapping](#rank-level-mapping)

3. [Final Export Table Schema](#final-export-table-schema)  
   3.1. [Overview](#overview)  
   3.2. [Explicit Column List and Descriptions](#explicit-column-list-and-descriptions)  
   3.3. [Conditional Columns: elevation_meters and anomaly_score](#conditional-columns-elevation_meters-and-anomaly_score)  
   3.4. [Example Row / CSV Layout](#example-row--csv-layout)

4. [ColDP Integration Tables](#coldp-integration-tables)  
   4.1. [ColDP Staging Tables](#coldp-staging-tables)  
   4.2. [Taxonomy Mapping Table](#taxonomy-mapping-table)  
   4.3. [Relationships and Data Flow](#relationships-and-data-flow)

5. [Supplementary Information](#supplementary-information)  
   5.1. [Data Types and Precision](#data-types-and-precision)  
   5.2. [Indices and Their Purposes](#indices-and-their-purposes)  
   5.3. [Version-Specific Schema Differences](#version-specific-schema-differences)

6. [Elevation Data Tables](#elevation-data-tables)  
   6.1. [Elevation_Raster Table](#elevation_raster-table)

7. [Appendix: SQL Dumps and \d Outputs](#appendix-sql-dumps-and-d-output)

---

## 1. Core iNaturalist Tables

These tables are imported directly from iNaturalist open data dumps.

### Observations

**Description:**  
Contains each observation record with geospatial and temporal data.

**Key Columns:**

| Column               | Type              | Description |
|----------------------|-------------------|-------------|
| observation_uuid     | uuid              | Unique identifier for each observation. This identifier is used to link photos and can be found on iNaturalist.org. |
| observer_id          | integer           | Identifier for the observer who recorded the observation. |
| latitude             | numeric(15,10)    | Latitude of the observation. High precision (up to 10 digits after the decimal) ensures accuracy. |
| longitude            | numeric(15,10)    | Longitude of the observation. |
| positional_accuracy  | integer           | Uncertainty in meters for the location. |
| taxon_id             | integer           | Identifier linking the observation to a taxon. |
| quality_grade        | varchar(255)      | Observation quality, e.g., "research", "casual", or "needs_id". |
| observed_on          | date              | Date when the observation was made. |
| anomaly_score        | numeric(15,6)     | A computed metric for anomaly detection; available in releases r1 and later. |
| geom                 | geometry          | Geospatial column computed from latitude and longitude. |
| origin               | varchar(255)      | Metadata field populated during ingestion. |
| version              | varchar(255)      | Database structure version. |
| release              | varchar(255)      | Data release identifier. |
| **elevation_meters** | **numeric(10,2)** | *Optional:* Elevation value in meters (if elevation processing is enabled). |

### Photos

**Description:**  
Contains metadata for photos associated with observations.

**Key Columns:**

| Column           | Type           | Description |
|------------------|----------------|-------------|
| photo_uuid       | uuid           | Unique identifier for each photo. |
| photo_id         | integer        | iNaturalist photo ID. |
| observation_uuid | uuid           | Identifier linking the photo to an observation. |
| observer_id      | integer        | Identifier of the observer who took the photo. |
| extension        | varchar(5)     | Image file format (e.g., "jpeg"). |
| license          | varchar(255)   | Licensing information (e.g., Creative Commons). |
| width            | smallint       | Photo width in pixels. |
| height           | smallint       | Photo height in pixels. |
| position         | smallint       | Indicates the order of photos for an observation (position 0 indicates primary photo). |
| origin           | varchar(255)   | Metadata field. |
| version          | varchar(255)   | Database structure version. |
| release          | varchar(255)   | Data release identifier. |

### Observers

**Description:**  
Contains information about the users (observers) who record observations.

**Key Columns:**

| Column      | Type         | Description |
|-------------|--------------|-------------|
| observer_id | integer      | Unique identifier for each observer. |
| login       | varchar(255) | Unique login/username. |
| name        | varchar(255) | Observer's personal name (if provided). |
| origin      | varchar(255) | Metadata field. |
| version     | varchar(255) | Database structure version. |
| release     | varchar(255) | Data release identifier. |

### Taxa

**Description:**  
Contains the taxonomy as provided by iNaturalist.

**Key Columns:**

| Column     | Type              | Description |
|------------|-------------------|-------------|
| taxon_id   | integer           | Unique taxon identifier. |
| ancestry   | varchar(255)      | Encoded ancestral hierarchy (delimited by backslashes). |
| rank_level | double precision  | Numeric level indicating taxonomic rank. |
| rank       | varchar(255)      | Taxonomic rank (e.g., "species", "genus"). |
| name       | varchar(255)      | Scientific name of the taxon. |
| active     | boolean           | Indicates if the taxon is active in the taxonomy. |
| origin     | varchar(255)      | Metadata field. |
| version    | varchar(255)      | Database structure version. |
| release    | varchar(255)      | Data release identifier. |

---

## 2. Expanded Taxa Table

### Purpose and Generation

The **expanded_taxa** table is generated from the iNaturalist `taxa` table by the `expand_taxa.sh` script. Its purpose is to "unpack" the single-column ancestry string into discrete columns (e.g., `L5_taxonID`, `L5_name`, `L5_commonName`, etc.) so that clade-based filtering and ancestor lookups can be performed efficiently without resorting to recursive string parsing.

### Schema Details

**Core Columns:**

| Column      | Type              | Description |
|-------------|-------------------|-------------|
| taxonID     | integer           | Primary key; unique taxon identifier. |
| rankLevel   | double precision  | Numeric indicator of the taxonomic rank. |
| rank        | varchar(255)      | Taxonomic rank label. |
| name        | varchar(255)      | Scientific name of the taxon. |
| commonName        | varchar(255)      | Common name of the taxon. |
| taxonActive | boolean           | Indicates whether the taxon is active. |

**Expanded Columns:**

For each rank level in the set `{5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30, 32, 33, 33.5, 34, 34.5, 35, 37, 40, 43, 44, 45, 47, 50, 53, 57, 60, 67, 70}`, the following columns are added:

- `L{level}_taxonID` (integer)
- `L{level}_name` (varchar(255))
- `L{level}_commonName` (varchar(255))

For example, for rank level 10:
- `L10_taxonID`
- `L10_name`
- `L10_commonName`

### Indexing and Performance Considerations

Indexes are created on the most frequently queried expanded columns (typically on `L10_taxonID`, `L20_taxonID`, …, `L70_taxonID`) as well as on the base columns (`taxonID`, `rankLevel`, and `name`). These indexes help to optimize the clade filtering and ancestor lookups performed by the export pipeline.

### Rank-Level Mapping

A supplemental mapping (provided in `clade_helpers.sh`) maps the column prefixes to human-readable rank names. For example:

| Prefix | Rank         |
|--------|--------------|
| L5     | subspecies   |
| L10    | species      |
| L20    | genus        |
| L40    | order        |
| L50    | class        |
| L70    | kingdom      |

A complete mapping is maintained in the code to facilitate any dynamic filtering or display of taxonomic information.

---

## 3. Final Export Table Schema

### Overview

The final export table is generated by the export pipeline (primarily via `cladistic.sh`) and is used for downstream applications such as training specimen identification models. This table is created by joining observations with photo metadata and taxonomic data from `expanded_taxa`. It includes additional computed columns for quality filtering and sampling.

### Explicit Column List and Descriptions

The final export table (named `<EXPORT_GROUP>_observations`) contains the following columns:

#### From the Observations Table

| Column               | Type              | Description |
|----------------------|-------------------|-------------|
| observation_uuid     | uuid              | Unique observation identifier. |
| observer_id          | integer           | Observer identifier. |
| latitude             | numeric(15,10)    | Latitude of the observation. |
| longitude            | numeric(15,10)    | Longitude of the observation. |
| **elevation_meters** | **numeric(10,2)** | *Optional:* Elevation in meters (included if `INCLUDE_ELEVATION_EXPORT=true`). |
| positional_accuracy  | integer           | Location uncertainty in meters. |
| taxon_id             | integer           | Identifier linking to the taxon. |
| quality_grade        | varchar(255)      | Quality grade (e.g., "research"). |
| observed_on          | date              | Date of observation. |
| anomaly_score        | numeric(15,6)     | Anomaly score (only available in r1 and later). |
| in_region            | boolean           | Computed flag indicating if the observation lies within the region bounding box. |
| expanded_taxonID     | integer           | Taxon ID from the expanded_taxa table. |
| expanded_rankLevel   | double precision  | Rank level from expanded_taxa. |
| expanded_name        | varchar(255)      | Taxon name from expanded_taxa. |
| L5_taxonID – L70_taxonID | integer       | A series of columns representing the taxonomic ancestry at various rank levels (e.g., L5_taxonID, L10_taxonID, …, L70_taxonID). |

#### From the Photos Table

| Column       | Type           | Description |
|--------------|----------------|-------------|
| photo_uuid   | uuid           | Unique photo identifier. |
| photo_id     | integer        | Photo identifier (from iNaturalist). |
| extension    | varchar(5)     | Image file format (e.g., "jpeg"). |
| license      | varchar(255)   | Licensing information. |
| width        | smallint       | Photo width (in pixels). |
| height       | smallint       | Photo height (in pixels). |
| position     | smallint       | Photo order indicator (primary photo has position 0). |

#### Additional Computed Column

| Column | Type    | Description |
|--------|---------|-------------|
| rn     | bigint  | Row number (per species partition based on `L10_taxonID`) used to cap the number of research-grade observations per species (controlled by `MAX_RN`). |

### Conditional Columns: elevation_meters and anomaly_score

- **elevation_meters:**  
  This column is included in the final export if the environment variable `INCLUDE_ELEVATION_EXPORT` is set to true and if the current release is not `"r0"`. It is positioned immediately after the `longitude` column.

- **anomaly_score:**  
  Present only in releases where it has been added (e.g., `r1` onward).

### Example Row / CSV Layout

An exported CSV row (tab-delimited) might be structured as follows:

```
observation_uuid    observer_id    latitude    longitude    elevation_meters    positional_accuracy    taxon_id    quality_grade    observed_on    anomaly_score    in_region    expanded_taxonID    expanded_rankLevel    expanded_name    L5_taxonID    L10_taxonID    ...    L70_taxonID    photo_uuid    photo_id    extension    license    width    height    position    rn
```

Each observation row is linked to one or more photo rows; the export process uses a partition-based random sampling (per species) so that only a maximum of `MAX_RN` research-grade observations per species are included.

---

## 4. ColDP Integration Tables

These tables store taxonomic data from the Catalog of Life Data Package (ColDP) and provide mappings between iNaturalist and Catalog of Life taxonomies. ColDP data is used to enrich the existing taxonomy with common names, distribution information, and other valuable data.

### ColDP Staging Tables

#### ColdpNameUsage

**Description:**  
Stores scientific names and taxonomic information from the ColDP NameUsage.tsv file.

**Key Columns:**

| Column               | Type          | Description |
|----------------------|---------------|-------------|
| ID                   | varchar(64)   | Primary key; unique taxon identifier from Catalog of Life. |
| scientificName       | text          | Full scientific name including authorship. |
| authorship           | text          | Taxonomic authorship information. |
| rank                 | varchar(64)   | Taxonomic rank (e.g., "species", "genus"). |
| status               | varchar(64)   | Status of the name (e.g., "accepted", "synonym"). |
| parentID             | varchar(64)   | Reference to the parent taxon in the hierarchy. |
| uninomial            | text          | For genus or higher rank names. |
| genericName          | text          | Genus part of the scientific name. |
| specificEpithet      | text          | Species part of the scientific name. |
| infraspecificEpithet | text          | Subspecies or variety part of the name. |
| family               | text          | Family name (for homonym resolution). |
| order                | text          | Order name (for homonym resolution). |
| class_               | text          | Class name (for homonym resolution). |
| phylum               | text          | Phylum name (for homonym resolution). |
| kingdom              | text          | Kingdom name (for homonym resolution). |

#### ColdpVernacularName

**Description:**  
Stores common names for taxa in various languages.

**Key Columns:**

| Column           | Type          | Description |
|------------------|---------------|-------------|
| id               | integer       | Auto-incrementing primary key. |
| taxonID          | varchar(64)   | Reference to ColdpNameUsage.ID. |
| name             | text          | The vernacular/common name. |
| language         | varchar(3)    | ISO 639-3 language code (e.g., "eng" for English). |
| preferred        | boolean       | Flag indicating if this is the preferred common name. |
| country          | varchar(10)   | ISO 3166-1-alpha-2 country code. |
| area             | text          | Geographic area where the name is used. |

#### ColdpDistribution

**Description:**  
Contains geographic distribution information for taxa.

**Key Columns:**

| Column       | Type          | Description |
|--------------|---------------|-------------|
| id           | integer       | Auto-incrementing primary key. |
| taxonID      | varchar(64)   | Reference to ColdpNameUsage.ID. |
| area         | text          | Geographic area description. |
| status       | varchar(64)   | Distribution status (e.g., "native", "introduced"). |

#### ColdpMedia

**Description:**  
Links to images, sounds, videos, and other media for taxa.

**Key Columns:**

| Column       | Type          | Description |
|--------------|---------------|-------------|
| id           | integer       | Auto-incrementing primary key. |
| taxonID      | varchar(64)   | Reference to ColdpNameUsage.ID. |
| url          | text          | URL to the media resource. |
| type         | varchar(64)   | Media type (e.g., "stillImage", "sound", "video"). |
| format       | varchar(64)   | MIME type or file suffix. |
| license      | varchar(128)  | License information for the media. |

#### ColdpReference

**Description:**  
Stores bibliographic references for taxonomic information.

**Key Columns:**

| Column        | Type          | Description |
|---------------|---------------|-------------|
| ID            | varchar(255)  | Primary key; reference identifier. |
| citation      | text          | Full citation text. |
| author        | text          | Author(s) of the reference. |
| title         | text          | Title of the reference. |
| issued        | text          | Date issued. |
| doi           | text          | Digital Object Identifier. |

#### ColdpTypeMaterial

**Description:**  
Information about type specimens for taxonomic names.

**Key Columns:**

| Column           | Type          | Description |
|------------------|---------------|-------------|
| ID               | varchar(64)   | Primary key; unique identifier. |
| nameID           | varchar(64)   | Reference to name in ColdpNameUsage. |
| citation         | text          | Citation for the type material. |
| status           | varchar(64)   | Type status (e.g., "holotype", "paratype"). |
| institutionCode  | varchar(64)   | Code for the holding institution. |
| catalogNumber    | varchar(64)   | Specimen catalog number. |
| latitude         | numeric(9,5)  | Latitude of the collection site. |
| longitude        | numeric(9,5)  | Longitude of the collection site. |

### Taxonomy Mapping Table

#### InatToColdpMap

**Description:**  
Cross-reference between iNaturalist taxa and Catalog of Life taxa, enabling the integration of ColDP data with iNaturalist observations.

**Key Columns:**

| Column               | Type          | Description |
|----------------------|---------------|-------------|
| inat_taxon_id        | integer       | iNaturalist taxon ID (references expanded_taxa.taxonID). |
| col_taxon_id         | varchar(64)   | Catalog of Life taxon ID (references ColdpNameUsage.ID). |
| match_type           | varchar(64)   | Type of match (e.g., "exact_name_rank", "fuzzy_name"). |
| match_score          | float         | Match confidence score (1.0 for exact matches, <1.0 for fuzzy). |
| inat_scientific_name | text          | Scientific name from iNaturalist. |
| col_scientific_name  | text          | Scientific name from Catalog of Life. |

### Relationships and Data Flow

The ColDP integration follows this data flow:

1. Raw TSV files from ColDP are loaded into the staging tables (`coldp_*`).
2. iNaturalist taxa from `expanded_taxa` are mapped to ColDP taxa in `coldp_name_usage_staging`.
3. The mapping is stored in `inat_to_coldp_taxon_map`.
4. Common names and other data are transferred from ColDP tables to `expanded_taxa` using the mapping.

**Key Relationships:**
- `inat_to_coldp_taxon_map.inat_taxon_id` → `expanded_taxa.taxonID`
- `inat_to_coldp_taxon_map.col_taxon_id` → `coldp_name_usage_staging.ID`
- `coldp_vernacular_name.taxonID` → `coldp_name_usage_staging.ID`
- `coldp_distribution.taxonID` → `coldp_name_usage_staging.ID`
- `coldp_media.taxonID` → `coldp_name_usage_staging.ID`

---

## 5. Supplementary Information

### Data Types and Precision

- **Latitude and Longitude:** Stored as `numeric(15,10)`, which provides high precision (up to 10 digits after the decimal) ensuring accurate geolocation.
- **elevation_meters:** Stored as `numeric(10,2)`, capturing elevation with two decimal places.
- **anomaly_score:** Stored as `numeric(15,6)` for precise anomaly measurements.
- Standard PostgreSQL data types are used for other columns as specified.

### Indices and Their Purposes

- **Core Tables:**  
  Primary keys and indexes are created on identifiers (e.g., `observation_uuid`, `photo_uuid`, `taxon_id`) and frequently queried columns.
- **Observations Geometry:**  
  A GIST index is created on the `geom` column for fast spatial queries.
- **Expanded_Taxa:**  
  Additional indexes are created on key expanded ancestry columns (e.g., `L10_taxonID`, `L20_taxonID`, …, `L70_taxonID`) to optimize clade-based filtering.
- **ColDP Tables:**  
  Indexes on `taxonID` fields to optimize joins between different ColDP tables.
- **Mapping Table:**  
  Indexes on both `inat_taxon_id` and `col_taxon_id` for efficient lookups in both directions.

### Version-Specific Schema Differences

- **Releases prior to r1:**  
  May not include `anomaly_score` and `elevation_meters`.
- **Current and Future Releases:**  
  Include these columns. Future schema changes will be documented here as needed.

---

## 6. Elevation Data Tables

### Elevation_Raster Table

**Description:**  
Stores Digital Elevation Model (DEM) raster data from MERIT DEM dataset. This table uses the PostGIS raster type to store elevation data in tiled format, which is used to provide elevation values for observations based on their geospatial coordinates.

**Key Columns:**

| Column   | Type    | Description |
|----------|---------|-------------|
| rid      | integer | Primary key; unique identifier for each raster tile. |
| rast     | raster  | PostGIS raster data type containing elevation data. Each raster is tiled to 100x100 pixels with 32-bit float values. |
| filename | text    | Original filename of the DEM data source (may be empty). |

**Indices:**
- Primary key on `rid`
- GIST index on `ST_ConvexHull(rast)` for spatial queries

**Usage Notes:**
1. The elevation_raster table is populated during the ingestion process by the `load_dem.sh` script, which processes and loads MERIT DEM data files.
2. The raster data is used to populate the `observations.elevation_meters` column using `ST_Value(rast, geom)` PostGIS function.
3. Each raster covers a specific geographic area, and the GIST indices on `ST_ConvexHull(rast)` allow for efficient spatial lookup.
4. The raster data type uses 32-bit float (-9999 as NODATA value), providing precise elevation values in meters.

**Query Example:**
```sql
-- Get elevation for a specific point
SELECT ST_Value(rast, ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)) AS elevation_meters
FROM elevation_raster
WHERE ST_Intersects(rast, ST_SetSRID(ST_MakePoint(longitude, latitude), 4326))
LIMIT 1;
```

**Python API Considerations:**
For SDK development, the recommended approach is to provide a function that:
1. Takes latitude and longitude as input
2. Queries the elevation_raster table using spatial intersection with the point
3. Returns the elevation value from the raster at that exact point
4. Gracefully handles points outside coverage areas (returns None/null)

## 7. Appendix: SQL Dumps and \d Outputs

Below are example outputs from PostgreSQL's `\d` command for key tables. These serve as a quick reference for the column names and types.

### Observations Table

```sql
-- \d observations
       Column        |          Type          
---------------------+------------------------
 observation_uuid    | uuid                  
 observer_id         | integer               
 latitude            | numeric(15,10)        
 longitude           | numeric(15,10)        
 positional_accuracy | integer               
 taxon_id            | integer               
 quality_grade       | varchar(255)          
 observed_on         | date                  
 anomaly_score       | numeric(15,6)         
 geom                | geometry              
 origin              | varchar(255)          
 version             | varchar(255)          
 release             | varchar(255)
 elevation_meters    | numeric(10,2)         -- Present if enabled
```

### Photos Table

```sql
-- \d photos
      Column      |          Type          
------------------+------------------------
 photo_uuid       | uuid                   
 photo_id         | integer                
 observation_uuid | uuid                   
 observer_id      | integer                
 extension        | varchar(5)             
 license          | varchar(255)           
 width            | smallint               
 height           | smallint               
 position         | smallint               
 origin           | varchar(255)           
 version          | varchar(255)           
 release          | varchar(255)
```

### Observers Table

```sql
-- \d observers
   Column    |          Type          
-------------+------------------------
 observer_id | integer                
 login       | varchar(255)           
 name        | varchar(255)           
 origin      | varchar(255)           
 version     | varchar(255)           
 release     | varchar(255)
```

### Taxa Table

```sql
-- \d taxa
   Column   |          Type          
------------+------------------------
 taxon_id   | integer                
 ancestry   | varchar(255)           
 rank_level | double precision       
 rank       | varchar(255)           
 name       | varchar(255)           
 active     | boolean                
 origin     | varchar(255)           
 version    | varchar(255)           
 release    | varchar(255)
```

### Expanded_Taxa Table

```sql
-- \d "expanded_taxa"
      Column      |          Type          
------------------+------------------------
 taxonID          | integer    (PK)
 rankLevel        | double precision       
 rank             | varchar(255)           
 name             | varchar(255)           
 taxonActive      | boolean                
 commonName       | varchar(255)           
 L5_taxonID       | integer                
 L5_name          | varchar(255)           
 L5_commonName    | varchar(255)           
 L10_taxonID      | integer                
 L10_name         | varchar(255)           
 L10_commonName   | varchar(255)           
 ...              | ...                    
 L70_taxonID      | integer                
```

### Final Export Table (Example)

Assuming the export group is named `amphibia_all_exc_nonrg_sp_oor_elev`, an example output is:

```sql
-- \d "amphibia_all_exc_nonrg_sp_oor_elev_observations"
       Column              |          Type          
-----------------------------+------------------------
 observation_uuid            | uuid                  
 observer_id                 | integer               
 latitude                    | numeric(15,10)        
 longitude                   | numeric(15,10)        
 elevation_meters            | numeric(10,2)         -- Only if enabled
 positional_accuracy         | integer               
 taxon_id                    | integer               
 quality_grade               | varchar(255)          
 observed_on                 | date                  
 anomaly_score               | numeric(15,6)         -- Only for r1 and later
 in_region                   | boolean               
 expanded_taxonID            | integer               
 expanded_rankLevel          | double precision       
 expanded_name               | varchar(255)           
 L5_taxonID                  | integer               
 L10_taxonID                 | integer               
 ...                         | ...                   
 L70_taxonID                 | integer               
 photo_uuid                  | uuid                  
 photo_id                    | integer               
 extension                   | varchar(5)            
 license                     | varchar(255)          
 width                       | smallint              
 height                      | smallint              
 position                    | smallint              
 rn                          | bigint                -- For internal sampling
```

### ColDP Tables

```sql
-- \d coldp_name_usage_staging
       Column           |          Type          
------------------------+------------------------
 ID                     | varchar(64)  PRIMARY KEY
 scientificName         | text                   
 authorship             | text                   
 rank                   | varchar(64)            
 status                 | varchar(64)            
 parentID               | varchar(64)            
 uninomial              | text                   
 genericName            | text                   
 infragenericEpithet    | text                   
 specificEpithet        | text                   
 infraspecificEpithet   | text                   
 family                 | text                   
 order                  | text                   
 class_                 | text                   
 phylum                 | text                   
 kingdom                | text                   

-- \d coldp_vernacular_name
     Column        |          Type          
-------------------+------------------------
 id                | integer  PRIMARY KEY
 taxonID           | varchar(64)            
 sourceID          | varchar(64)            
 name              | text                   
 transliteration   | text                   
 language          | varchar(3)             
 preferred         | boolean                
 country           | varchar(10)            
 area              | text                   

-- \d inat_to_coldp_taxon_map
        Column        |          Type          
----------------------+------------------------
 inat_taxon_id        | integer                
 col_taxon_id         | varchar(64)            
 match_type           | varchar(64)            
 match_score          | double precision       
 inat_scientific_name | text                   
 col_scientific_name  | text                   
```

---

## Final Notes

- This document serves as the definitive reference for all table schemas within ibridaDB (other than intermediate tables).  
- It is essential for developers working on downstream processing, migration, or debugging tasks.  
- As the system evolves (new releases, additional columns, or modifications to processing logic), please update this document to maintain an accurate reference.
  - NOTE: 'regional base' tables are not documented here but quite likely should be. Necessary for debugging and understanding advances features like ancestor-aware (ancestor search), out-of-region (oor) observations of in-region taxa, etc.
      </file>
      <dir path="docs/sample_data">
        <file path="docs/sample_data/README.md">
# Sample Data for Testing

This directory contains sample data files that can be used for unit testing with SQLite or other lightweight database solutions. These samples are extracted from the ibridaDB database and provide realistic data structures and relationships for testing.

## Files Included

### ColDP Tables

These files contain samples from the Catalog of Life Data Package (ColDP) tables:

- `coldp_name_usage.tsv`: Scientific names and taxonomic information
- `coldp_vernacular_name.tsv`: Common names in various languages
- `coldp_distribution.tsv`: Geographic distribution information
- `coldp_media.tsv`: Links to images, sounds, and other media
- `coldp_reference.tsv`: Bibliographic references
- `coldp_type_material.tsv`: Type specimen information

### Mapping and Taxa Tables

- `inat_to_coldp_taxon_map.tsv`: Crosswalk between iNaturalist taxa and Catalog of Life taxa
- `expanded_taxa_sample.tsv`: Basic sample of the expanded_taxa table with taxonomic hierarchies and common names
- `expanded_taxa_lca_sample.tsv`: Specialized sample containing the full taxonomy for bees (Anthophila) and wasps (Vespoidea), designed for testing Lowest Common Ancestor (LCA) algorithms and taxonomic distance calculations

## Usage for SQLite Testing

These files can be used to create a lightweight SQLite database for unit testing, particularly for the typus SDK. Here's an example of how to load them into SQLite:

```python
import sqlite3
import pandas as pd

# Create SQLite connection
conn = sqlite3.connect('test_db.sqlite')

# Load each TSV file into a table
for table_name in ['coldp_name_usage', 'coldp_vernacular_name', 'coldp_distribution', 
                  'coldp_media', 'coldp_reference', 'coldp_type_material',
                  'inat_to_coldp_taxon_map', 'expanded_taxa_sample']:
    df = pd.read_csv(f'{table_name}.tsv', sep='\t')
    df.to_sql(table_name, conn, if_exists='replace', index=False)
    
# Optionally load the specialized LCA sample data
df_lca = pd.read_csv('expanded_taxa_lca_sample.tsv', sep='\t')
df_lca.to_sql('expanded_taxa', conn, if_exists='replace', index=False)

# Create view for expanded_taxa_cmn if needed
conn.execute('''
CREATE VIEW IF NOT EXISTS expanded_taxa_cmn AS
SELECT * FROM expanded_taxa_sample
''')

conn.commit()
conn.close()
```

## Table Relationships

The relationships between these tables mirror those in the full database:

- `inat_to_coldp_taxon_map.inat_taxon_id` → `expanded_taxa_sample.taxonID`
- `inat_to_coldp_taxon_map.col_taxon_id` → `coldp_name_usage.ID`
- `coldp_vernacular_name.taxonID` → `coldp_name_usage.ID`
- `coldp_distribution.taxonID` → `coldp_name_usage.ID`
- `coldp_media.taxonID` → `coldp_name_usage.ID`
- `coldp_reference.ID` is referenced by other tables' `referenceID` fields

## Notes

- These samples contain a limited subset of records for testing purposes
- All data relationships are preserved to allow for testing of joins and foreign key relationships
- The expanded_taxa_sample table includes full taxonomic hierarchies with common names for testing
- These files can be easily imported into a SQLite database or any other database system

## Lowest Common Ancestor (LCA) Testing

The `expanded_taxa_lca_sample.tsv` file contains a carefully selected set of taxa from the honey bee (Apis) and wasp (Vespa/Vespula) lineages, which can be used to test:

1. **Lowest Common Ancestor (LCA) functionality** - For example:
   - LCA of Apis mellifera (47219) and Vespa crabro (54327) is Hymenoptera (47201)
   - LCA of Bombus (52775) and Apis (47220) is Apidae (47221)
   - LCA of Vespula (61356) and Vespa (54328) is Vespidae (52747)

2. **Taxonomic Distance Calculation** - For calculating both inclusive and exclusive distance:
   - Distance from Apis mellifera to Vespa crabro (inclusive of rank boundaries)
   - Distance from Bombus to Apis (excluding minor ranks)

3. **Rank-Level Navigation** - For traversing up and down the taxonomy at specific rank levels:
   - Finding all species under Apidae
   - Finding the order of any given species
   - Extracting the full lineage of a taxon

When using this sample for LCA testing, the taxonID field (630955 in the error message) is correctly included in the dataset.

### Regenerating the LCA Sample Data

To regenerate or update the LCA sample data, a SQL query file is provided in `generate_lca_sample.sql`. This file contains the following query that exports ALL columns from the expanded_taxa table for selected taxa:

```sql
-- SQL query for generating LCA sample data
-- Export all columns for the specified taxa

COPY (
    SELECT *
    FROM expanded_taxa t
    WHERE
        -- Include targeted taxa IDs and names for bees, wasps, and their ancestors
        t."taxonID" IN (630955, 47201, 47216, 47337, 47338, 184884, 47369, 47218, 52747, 52775, 295935)
        OR t."name" IN ('Hymenoptera', 'Anthophila', 'Apis', 'Apis mellifera', 'Apidae', 'Animalia', 'Arthropoda', 'Insecta',
                        'Vespidae', 'Vespa', 'Vespa mandarinia', 'Vespoidea', 'Scoliidae', 'Vespa crabro', 'Vespinae', 'Vespula', 'Vespula vulgaris')
) TO '/tmp/expanded_taxa_lca_sample.tsv' WITH (FORMAT CSV, DELIMITER E'\t', HEADER TRUE);
```

To run this query and generate the sample data:

```bash
# Copy the SQL file to the Docker container
docker cp docs/sample_data/generate_lca_sample.sql ibridaDB:/tmp/

# Execute the SQL query
docker exec ibridaDB psql -U postgres -d ibrida-v0-r1 -f /tmp/generate_lca_sample.sql

# Copy the results back from the container
docker cp ibridaDB:/tmp/expanded_taxa_lca_sample.tsv docs/sample_data/expanded_taxa_lca_sample.tsv
```

This process ensures the sample includes all columns from the expanded_taxa table, including all rank levels (L5 through L70) with their taxonID, name, and commonName fields, which is critical for accurate LCA testing.
        </file>
        <file path="docs/sample_data/generate_lca_sample.sql">
-- SQL query for generating LCA sample data
-- Export ALL columns for the selected taxa (bees, wasps, key ancestors)

COPY (
    SELECT *
    FROM expanded_taxa t
    WHERE
        /* explicit IDs */
        t."taxonID" IN (
            630955,   -- Anthophila
            52747,    -- Vespidae
            47201,    -- Hymenoptera
            326777,   -- Aculeata           <-- new
            124417,   -- Apocrita           <-- new
            47216, 47337, 47338, 47369, 47218, 52775, 184884, 295935
        )
        /* or matching by scientific name (belt-and-braces in case IDs drift) */
        OR t."name" IN (
            'Animalia', 'Arthropoda', 'Insecta',
            'Hymenoptera', 'Aculeata',            -- added
            'Apocrita',                           -- added
            'Anthophila', 'Apidae', 'Apis', 'Apis mellifera',
            'Vespoidea', 'Vespidae', 'Vespinae', 'Vespa', 'Vespa mandarinia',
            'Vespa crabro', 'Vespula', 'Vespula vulgaris', 'Scoliidae'
        )
) TO '/tmp/expanded_taxa_lca_sample.tsv'
  WITH (FORMAT CSV, DELIMITER E'\t', HEADER TRUE);

-- Reminder for the fixture generator:
--   mv /tmp/expanded_taxa_lca_sample.tsv tests/sample_tsv/expanded_taxa.tsv
        </file>
      </dir>
    </dir>
    <dir path="models">
      <file path="models/__init__.py">
from .base import Base
from .expanded_taxa import ExpandedTaxa
from .expanded_taxa_cmn import ExpandedTaxaCmn
from .coldp_models import (
    ColdpVernacularName,
    ColdpDistribution, 
    ColdpMedia,
    ColdpReference,
    ColdpTypeMaterial
)

__all__ = [
    'Base',
    'ExpandedTaxa',
    'ExpandedTaxaCmn',
    'ColdpVernacularName',
    'ColdpDistribution',
    'ColdpMedia',
    'ColdpReference',
    'ColdpTypeMaterial'
]
      </file>
      <file path="models/base.py">
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()
      </file>
      <file path="models/coldp_models.py">
from sqlalchemy import (
    Column, String, Text, Boolean, Date, Numeric, Integer
)
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base

from .base import Base

class ColdpVernacularName(Base):
    __tablename__ = "coldp_vernacular_name"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(64), index=True, nullable=False)
    sourceID       = Column(String(10))
    name           = Column(Text, nullable=False)
    transliteration= Column(Text)
    language       = Column(String(3))      # ISO‑639‑3
    preferred      = Column(Boolean)
    country        = Column(String(10))     # ISO‑3166‑1‑alpha‑2, increased size for data compatibility
    area           = Column(Text)
    sex            = Column(String(20))
    referenceID    = Column(String(64))
    remarks        = Column(Text)

class ColdpDistribution(Base):
    __tablename__ = "coldp_distribution"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(64), index=True)
    sourceID       = Column(String(64))
    areaID         = Column(String(64))
    area           = Column(Text)
    gazetteer      = Column(String(64))
    status         = Column(String(64))     # e.g. native, introduced
    referenceID    = Column(String(64))
    remarks        = Column(Text)

class ColdpMedia(Base):
    __tablename__ = "coldp_media"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(64), index=True)
    sourceID       = Column(String(64))
    url            = Column(Text, nullable=False)
    type           = Column(String(64))     # stillImage, sound, video …
    format         = Column(String(64))     # MIME type or file suffix
    title          = Column(Text)
    created        = Column(Date)
    creator        = Column(Text)
    license        = Column(String(128))
    link           = Column(Text)           # landing page
    remarks        = Column(Text)

class ColdpReference(Base):
    __tablename__ = "coldp_reference"
    ID             = Column(String(255), primary_key=True)   # UUID or short key
    alternativeID  = Column(Text)
    sourceID       = Column(Text)
    citation       = Column(Text)
    type           = Column(Text)
    author         = Column(Text)
    editor         = Column(Text)
    title          = Column(Text)
    titleShort     = Column(Text)
    containerAuthor= Column(Text)
    containerTitle = Column(Text)
    containerTitleShort = Column(Text)
    issued         = Column(Text)
    accessed       = Column(Text)
    collectionTitle= Column(Text)
    collectionEditor= Column(Text)
    volume         = Column(Text)
    issue          = Column(Text)
    edition        = Column(Text)
    page           = Column(Text)
    publisher      = Column(Text)
    publisherPlace = Column(Text)
    version        = Column(Text)
    isbn           = Column(Text)
    issn           = Column(Text)
    doi            = Column(Text)
    link           = Column(Text)
    remarks        = Column(Text)

class ColdpTypeMaterial(Base):
    """
    ColDP entity `TypeMaterial` (called TypeSpecimen in the user request).
    """
    __tablename__ = "coldp_type_material"
    ID              = Column(String(64), primary_key=True)
    nameID          = Column(String(64), index=True)
    sourceID        = Column(String(64))
    citation        = Column(Text)
    status          = Column(String(64))
    referenceID     = Column(String(64))
    page            = Column(String(64))
    country         = Column(String(10))
    locality        = Column(Text)
    latitude        = Column(Numeric(9,5))
    longitude       = Column(Numeric(9,5))
    altitude        = Column(String(64))
    sex             = Column(String(20))
    host            = Column(Text)
    associatedSequences = Column(Text)
    date            = Column(Date)
    collector       = Column(Text)
    institutionCode = Column(String(64))
    catalogNumber   = Column(String(64))
    link            = Column(Text)
    remarks         = Column(Text)

# ColdpNameUsage class for staging with all the necessary fields for mapping
class ColdpNameUsage(Base):
    """
    Represents the ColDP NameUsage.tsv table with fields needed for mapping to iNaturalist taxa.
    
    This model includes fields needed for:
    1. Basic identification and matching (ID, scientificName, rank, status)
    2. Taxonomic hierarchy fields for resolving homonyms during fuzzy matching
    3. Name components for more detailed matching
    """
    __tablename__ = "coldp_name_usage_staging"
    ID = Column(String(64), primary_key=True)
    
    # Basic identification
    scientificName = Column(Text, index=True)
    authorship = Column(Text)
    rank = Column(String(64), index=True)
    status = Column(String(64), index=True)
    parentID = Column(String(64))
    
    # Name components
    uninomial = Column(Text)  # For genus or higher rank names
    genericName = Column(Text)  # Note: ColDP uses 'genus' for genus part of binomial
    infragenericEpithet = Column(Text)
    specificEpithet = Column(Text)
    infraspecificEpithet = Column(Text)
    basionymID = Column(String(64))
    
    # Taxonomic hierarchy fields for homonym resolution
    # These simplify matching across taxonomic hierarchies
    # In ColDP NameUsage they appear at the end of the TSV
    family = Column(Text)
    order = Column(Text) 
    class_ = Column(Text)  # Using class_ to avoid Python keyword conflict
    phylum = Column(Text)
    kingdom = Column(Text)
      </file>
      <file path="models/expanded_taxa.py">
from sqlalchemy import Boolean, Column, Float, Index, Integer, String
from .base import Base


class ExpandedTaxa(Base):
    """
    ORM model for the expanded_taxa table.
    
    This table provides a wide, denormalized view of taxonomic data with:
    - Core taxon information (ID, rank, names)
    - Immediate ancestor columns for efficient parent lookups
    - Full ancestral hierarchy columns (L* columns) for each taxonomic rank
    - Common names from Catalog of Life integration
    
    Immediate Ancestor Columns (Added 2025-05-26):
    - immediateAncestor_taxonID: Direct parent taxon, regardless of rank
    - immediateAncestor_rankLevel: Rank level of the immediate parent
    - immediateMajorAncestor_taxonID: Nearest ancestor at a major rank (multiple of 10)
    - immediateMajorAncestor_rankLevel: Rank level of the immediate major ancestor
    
    These columns enable O(1) parent lookups instead of scanning all L* columns.
    When immediate parent is already at a major rank, both sets of columns will have the same values.
    Root taxa (e.g., Animalia) will have NULL values for all ancestor columns.
    """
    __tablename__ = "expanded_taxa"

    taxonID = Column(Integer, primary_key=True, nullable=False)
    rankLevel = Column(Float, index=True)
    rank = Column(String(255))
    name = Column(String(255), index=True)
    commonName = Column(String(255))  # NEW - populated from ColDP integration
    taxonActive = Column(Boolean, index=True)
    
    # Immediate ancestor columns for efficient parent lookups # NEW
    immediateMajorAncestor_taxonID = Column(Integer, index=True)  # NEW
    immediateMajorAncestor_rankLevel = Column(Float)  # NEW
    immediateAncestor_taxonID = Column(Integer, index=True)  # NEW
    immediateAncestor_rankLevel = Column(Float)  # NEW

    # Ancestral columns
    # Each rank level has three columns: taxonID, name, and commonName
    # The commonName columns are NEW - populated from ColDP integration
    L5_taxonID = Column(Integer)
    L5_name = Column(String(255))
    L5_commonName = Column(String(255))  # NEW - from ColDP
    L10_taxonID = Column(Integer)
    L10_name = Column(String(255))
    L10_commonName = Column(String(255))  # NEW - from ColDP
    L11_taxonID = Column(Integer)
    L11_name = Column(String(255))
    L11_commonName = Column(String(255))  # NEW - from ColDP
    L12_taxonID = Column(Integer)
    L12_name = Column(String(255))
    L12_commonName = Column(String(255))  # NEW - from ColDP
    L13_taxonID = Column(Integer)
    L13_name = Column(String(255))
    L13_commonName = Column(String(255))  # NEW - from ColDP
    L15_taxonID = Column(Integer)
    L15_name = Column(String(255))
    L15_commonName = Column(String(255))  # NEW - from ColDP
    L20_taxonID = Column(Integer)
    L20_name = Column(String(255))
    L20_commonName = Column(String(255))  # NEW - from ColDP
    L24_taxonID = Column(Integer)
    L24_name = Column(String(255))
    L24_commonName = Column(String(255))  # NEW - from ColDP
    L25_taxonID = Column(Integer)
    L25_name = Column(String(255))
    L25_commonName = Column(String(255))  # NEW - from ColDP
    L26_taxonID = Column(Integer)
    L26_name = Column(String(255))
    L26_commonName = Column(String(255))  # NEW - from ColDP
    L27_taxonID = Column(Integer)
    L27_name = Column(String(255))
    L27_commonName = Column(String(255))  # NEW - from ColDP
    L30_taxonID = Column(Integer)
    L30_name = Column(String(255))
    L30_commonName = Column(String(255))  # NEW - from ColDP
    L32_taxonID = Column(Integer)
    L32_name = Column(String(255))
    L32_commonName = Column(String(255))  # NEW - from ColDP
    L33_taxonID = Column(Integer)
    L33_name = Column(String(255))
    L33_commonName = Column(String(255))  # NEW - from ColDP
    L33_5_taxonID = Column(Integer)
    L33_5_name = Column(String(255))
    L33_5_commonName = Column(String(255))  # NEW - from ColDP
    L34_taxonID = Column(Integer)
    L34_name = Column(String(255))
    L34_commonName = Column(String(255))  # NEW - from ColDP
    L34_5_taxonID = Column(Integer)
    L34_5_name = Column(String(255))
    L34_5_commonName = Column(String(255))  # NEW - from ColDP
    L35_taxonID = Column(Integer)
    L35_name = Column(String(255))
    L35_commonName = Column(String(255))  # NEW - from ColDP
    L37_taxonID = Column(Integer)
    L37_name = Column(String(255))
    L37_commonName = Column(String(255))  # NEW - from ColDP
    L40_taxonID = Column(Integer)
    L40_name = Column(String(255))
    L40_commonName = Column(String(255))  # NEW - from ColDP
    L43_taxonID = Column(Integer)
    L43_name = Column(String(255))
    L43_commonName = Column(String(255))  # NEW - from ColDP
    L44_taxonID = Column(Integer)
    L44_name = Column(String(255))
    L44_commonName = Column(String(255))  # NEW - from ColDP
    L45_taxonID = Column(Integer)
    L45_name = Column(String(255))
    L45_commonName = Column(String(255))  # NEW - from ColDP
    L47_taxonID = Column(Integer)
    L47_name = Column(String(255))
    L47_commonName = Column(String(255))  # NEW - from ColDP
    L50_taxonID = Column(Integer)
    L50_name = Column(String(255))
    L50_commonName = Column(String(255))  # NEW - from ColDP
    L53_taxonID = Column(Integer)
    L53_name = Column(String(255))
    L53_commonName = Column(String(255))  # NEW - from ColDP
    L57_taxonID = Column(Integer)
    L57_name = Column(String(255))
    L57_commonName = Column(String(255))  # NEW - from ColDP
    L60_taxonID = Column(Integer)
    L60_name = Column(String(255))
    L60_commonName = Column(String(255))  # NEW - from ColDP
    L67_taxonID = Column(Integer)
    L67_name = Column(String(255))
    L67_commonName = Column(String(255))  # NEW - from ColDP
    L70_taxonID = Column(Integer)
    L70_name = Column(String(255))
    L70_commonName = Column(String(255))  # NEW - from ColDP


# Important indexes for lookups
Index("idx_expanded_taxa_L10_taxonID", ExpandedTaxa.L10_taxonID)
Index("idx_immediate_ancestor_taxon_id", ExpandedTaxa.immediateAncestor_taxonID)  # NEW
Index("idx_immediate_major_ancestor_taxon_id", ExpandedTaxa.immediateMajorAncestor_taxonID)  # NEW

      </file>
      <file path="models/expanded_taxa_cmn.py">
from sqlalchemy import (
    Column, Integer, String, Text, Boolean, Float, Index
)
from .base import Base

class ExpandedTaxaCmn(Base):
    """
    Copy of expanded_taxa with additional common‑name columns.
    Only the *preferred English* common name is stored per taxon.
    """
    __tablename__ = "expanded_taxa_cmn"

    taxonID       = Column(Integer, primary_key=True, nullable=False)
    rankLevel     = Column(Float, index=True)
    rank          = Column(String(255))
    name          = Column(String(255), index=True)
    commonName    = Column(String(255))               # NEW
    taxonActive   = Column(Boolean, index=True)

    # Ancestral columns – dots converted to underscores to match SQL
    L5_taxonID    = Column(Integer)
    L5_name       = Column(Text)
    L5_commonName = Column(String(255))
    L10_taxonID    = Column(Integer)
    L10_name       = Column(Text)
    L10_commonName = Column(String(255))
    L11_taxonID    = Column(Integer)
    L11_name       = Column(Text)
    L11_commonName = Column(String(255))
    L12_taxonID    = Column(Integer)
    L12_name       = Column(Text)
    L12_commonName = Column(String(255))
    L13_taxonID    = Column(Integer)
    L13_name       = Column(Text)
    L13_commonName = Column(String(255))
    L15_taxonID    = Column(Integer)
    L15_name       = Column(Text)
    L15_commonName = Column(String(255))
    L20_taxonID    = Column(Integer)
    L20_name       = Column(Text)
    L20_commonName = Column(String(255))
    L24_taxonID    = Column(Integer)
    L24_name       = Column(Text)
    L24_commonName = Column(String(255))
    L25_taxonID    = Column(Integer)
    L25_name       = Column(Text)
    L25_commonName = Column(String(255))
    L26_taxonID    = Column(Integer)
    L26_name       = Column(Text)
    L26_commonName = Column(String(255))
    L27_taxonID    = Column(Integer)
    L27_name       = Column(Text)
    L27_commonName = Column(String(255))
    L30_taxonID    = Column(Integer)
    L30_name       = Column(Text)
    L30_commonName = Column(String(255))
    L32_taxonID    = Column(Integer)
    L32_name       = Column(Text)
    L32_commonName = Column(String(255))
    L33_taxonID    = Column(Integer)
    L33_name       = Column(Text)
    L33_commonName = Column(String(255))
    L33_5_taxonID    = Column(Integer)
    L33_5_name       = Column(Text)
    L33_5_commonName = Column(String(255))
    L34_taxonID    = Column(Integer)
    L34_name       = Column(Text)
    L34_commonName = Column(String(255))
    L34_5_taxonID    = Column(Integer)
    L34_5_name       = Column(Text)
    L34_5_commonName = Column(String(255))
    L35_taxonID    = Column(Integer)
    L35_name       = Column(Text)
    L35_commonName = Column(String(255))
    L37_taxonID    = Column(Integer)
    L37_name       = Column(Text)
    L37_commonName = Column(String(255))
    L40_taxonID    = Column(Integer)
    L40_name       = Column(Text)
    L40_commonName = Column(String(255))
    L43_taxonID    = Column(Integer)
    L43_name       = Column(Text)
    L43_commonName = Column(String(255))
    L44_taxonID    = Column(Integer)
    L44_name       = Column(Text)
    L44_commonName = Column(String(255))
    L45_taxonID    = Column(Integer)
    L45_name       = Column(Text)
    L45_commonName = Column(String(255))
    L47_taxonID    = Column(Integer)
    L47_name       = Column(Text)
    L47_commonName = Column(String(255))
    L50_taxonID    = Column(Integer)
    L50_name       = Column(Text)
    L50_commonName = Column(String(255))
    L53_taxonID    = Column(Integer)
    L53_name       = Column(Text)
    L53_commonName = Column(String(255))
    L57_taxonID    = Column(Integer)
    L57_name       = Column(Text)
    L57_commonName = Column(String(255))
    L60_taxonID    = Column(Integer)
    L60_name       = Column(Text)
    L60_commonName = Column(String(255))
    L67_taxonID    = Column(Integer)
    L67_name       = Column(Text)
    L67_commonName = Column(String(255))
    L70_taxonID    = Column(Integer)
    L70_name       = Column(Text)
    L70_commonName = Column(String(255))

# Helpful composite index for frequent ancestor look‑ups
Index("idx_expanded_taxa_cmn_L10_taxonID", "L10_taxonID")
      </file>
    </dir>
    <dir path="scripts">
      <file path="scripts/__init__.py">
# Scripts package initialization
      </file>
      <file path="scripts/add_immediate_ancestors.py" line_interval="25">
#!/usr/bin/env python3

import argparse
import os
import logging
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
import pandas as pd
import time

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def get_db_engine(db_user, db_password, db_host, db_port, db_name):
    connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    return create_engine(connection_string)

def add_columns_if_not_exists(session):
    """Add the four new immediate ancestor columns if they don't already exist."""
    logger.info("Checking if immediate ancestor columns exist...")
    
    # Check if columns already exist
    check_sql = text("""
        SELECT column_name 
#|LN|25|
        FROM information_schema.columns 
        WHERE table_name = 'expanded_taxa' 
        AND column_name IN ('immediateMajorAncestor_taxonID', 'immediateMajorAncestor_rankLevel', 
                           'immediateAncestor_taxonID', 'immediateAncestor_rankLevel');
    """)
    
    existing_columns = [row[0] for row in session.execute(check_sql)]
    
    if len(existing_columns) == 4:
        logger.info("All immediate ancestor columns already exist.")
        return False
    
    # Add columns
    logger.info("Adding immediate ancestor columns to expanded_taxa table...")
    
    alter_statements = []
    if 'immediateMajorAncestor_taxonID' not in existing_columns:
        alter_statements.append('ADD COLUMN "immediateMajorAncestor_taxonID" INTEGER')
    if 'immediateMajorAncestor_rankLevel' not in existing_columns:
        alter_statements.append('ADD COLUMN "immediateMajorAncestor_rankLevel" DOUBLE PRECISION')
    if 'immediateAncestor_taxonID' not in existing_columns:
        alter_statements.append('ADD COLUMN "immediateAncestor_taxonID" INTEGER')
    if 'immediateAncestor_rankLevel' not in existing_columns:
        alter_statements.append('ADD COLUMN "immediateAncestor_rankLevel" DOUBLE PRECISION')
    
#|LN|50|
    if alter_statements:
        alter_sql = text(f"ALTER TABLE expanded_taxa {', '.join(alter_statements)};")
        session.execute(alter_sql)
        session.commit()
        logger.info("Successfully added immediate ancestor columns.")
    
    return True

def compute_immediate_ancestors(row, all_rank_levels, major_rank_levels):
    """Compute immediate ancestors for a single taxon."""
    taxon_rank = row['rankLevel']
    
    # Find immediate ancestor (any rank)
    immediate_ancestor_id = None
    immediate_ancestor_rank = None
    min_rank_diff = float('inf')
    
    for rank_level in all_rank_levels:
        col_name = f"L{str(rank_level).replace('.', '_')}_taxonID"
        if col_name in row:
            ancestor_id = row[col_name]
            
            if pd.notna(ancestor_id) and rank_level > taxon_rank:
                if rank_level - taxon_rank < min_rank_diff:
                    min_rank_diff = rank_level - taxon_rank
#|LN|75|
                    immediate_ancestor_id = int(ancestor_id)
                    immediate_ancestor_rank = rank_level
    
    # Find immediate major-rank ancestor
    immediate_major_ancestor_id = None
    immediate_major_ancestor_rank = None
    
    for rank_level in major_rank_levels:
        col_name = f"L{rank_level}_taxonID"
        if col_name in row:
            ancestor_id = row[col_name]
            
            if pd.notna(ancestor_id) and rank_level > taxon_rank:
                immediate_major_ancestor_id = int(ancestor_id)
                immediate_major_ancestor_rank = float(rank_level)
                break
    
    return {
        'immediateAncestor_taxonID': immediate_ancestor_id,
        'immediateAncestor_rankLevel': immediate_ancestor_rank,
        'immediateMajorAncestor_taxonID': immediate_major_ancestor_id,
        'immediateMajorAncestor_rankLevel': immediate_major_ancestor_rank
    }

def populate_immediate_ancestors(session, engine, batch_size=50000):
#|LN|100|
    """Populate the immediate ancestor columns for all taxa."""
    logger.info("Starting to populate immediate ancestor columns...")
    
    # Define all rank levels in order
    all_rank_levels = [5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30, 32, 33, 33.5, 34, 34.5, 35, 37, 40, 43, 44, 45, 47, 50, 53, 57, 60, 67, 70]
    major_rank_levels = [10, 20, 30, 40, 50, 60, 70]
    
    # Build list of columns to select
    base_cols = ['"taxonID"', '"rankLevel"']
    ancestor_cols = []
    for rank_level in all_rank_levels:
        col_name = f'"L{str(rank_level).replace(".", "_")}_taxonID"'
        ancestor_cols.append(col_name)
    
    select_cols = ', '.join(base_cols + ancestor_cols)
    
    # Get total count
    count_result = session.execute(text("SELECT COUNT(*) FROM expanded_taxa"))
    total_count = count_result.scalar()
    logger.info(f"Total taxa to process: {total_count}")
    
    # Process in batches
    processed = 0
    start_time = time.time()
    
#|LN|125|
    for offset in range(0, total_count, batch_size):
        # Read batch
        query = f"""
            SELECT {select_cols}
            FROM expanded_taxa
            ORDER BY "taxonID"
            LIMIT {batch_size} OFFSET {offset}
        """
        
        df = pd.read_sql_query(query, engine)
        
        # Compute immediate ancestors for each row
        updates = []
        for _, row in df.iterrows():
            result = compute_immediate_ancestors(row, all_rank_levels, major_rank_levels)
            updates.append({
                'taxonID': row['taxonID'],
                **result
            })
        
        # Update database
        if updates:
            update_df = pd.DataFrame(updates)
            
            # Create temporary table for bulk update
#|LN|150|
            temp_table = f"temp_immediate_ancestors_{int(time.time())}"
            update_df.to_sql(temp_table, engine, if_exists='replace', index=False)
            
            # Perform bulk update
            update_sql = text(f"""
                UPDATE expanded_taxa et
                SET 
                    "immediateAncestor_taxonID" = t."immediateAncestor_taxonID",
                    "immediateAncestor_rankLevel" = t."immediateAncestor_rankLevel",
                    "immediateMajorAncestor_taxonID" = t."immediateMajorAncestor_taxonID",
                    "immediateMajorAncestor_rankLevel" = t."immediateMajorAncestor_rankLevel"
                FROM {temp_table} t
                WHERE et."taxonID" = t."taxonID"
            """)
            
            session.execute(update_sql)
            session.commit()
            
            # Drop temporary table
            session.execute(text(f"DROP TABLE {temp_table}"))
            session.commit()
        
        processed += len(df)
        elapsed = time.time() - start_time
        rate = processed / elapsed if elapsed > 0 else 0
#|LN|175|
        eta = (total_count - processed) / rate if rate > 0 else 0
        
        logger.info(f"Processed {processed}/{total_count} taxa "
                   f"({100 * processed / total_count:.1f}%) "
                   f"Rate: {rate:.0f} taxa/sec, ETA: {eta/60:.1f} minutes")

def create_indexes(session):
    """Create indexes on the new taxonID columns for efficient lookups."""
    logger.info("Creating indexes on immediate ancestor columns...")
    
    index_statements = [
        'CREATE INDEX IF NOT EXISTS idx_immediate_ancestor_taxon_id ON expanded_taxa("immediateAncestor_taxonID")',
        'CREATE INDEX IF NOT EXISTS idx_immediate_major_ancestor_taxon_id ON expanded_taxa("immediateMajorAncestor_taxonID")'
    ]
    
    for stmt in index_statements:
        session.execute(text(stmt))
    
    session.commit()
    logger.info("Indexes created successfully.")

def verify_results(session):
    """Verify that the immediate ancestor columns were populated correctly."""
    logger.info("Verifying results...")
    
#|LN|200|
    # Check a few examples
    sample_sql = text("""
        SELECT 
            "taxonID", 
            name, 
            "rankLevel",
            "immediateAncestor_taxonID",
            "immediateAncestor_rankLevel",
            "immediateMajorAncestor_taxonID",
            "immediateMajorAncestor_rankLevel"
        FROM expanded_taxa 
        WHERE "immediateAncestor_taxonID" IS NOT NULL
        LIMIT 10;
    """)
    
    results = session.execute(sample_sql)
    logger.info("Sample of populated immediate ancestors:")
    for row in results:
        logger.info(f"  Taxon {row[0]} ({row[1]}, rank {row[2]}): "
                   f"immediate={row[3]} (rank {row[4]}), "
                   f"major={row[5]} (rank {row[6]})")
    
    # Check specific examples - verify ancestors are correct
    test_sql = text("""
        SELECT 
#|LN|225|
            et."taxonID",
            et.name,
            et."rankLevel",
            et."immediateAncestor_taxonID",
            ia.name as immediate_name,
            et."immediateAncestor_rankLevel",
            et."immediateMajorAncestor_taxonID",
            ima.name as major_name,
            et."immediateMajorAncestor_rankLevel"
        FROM expanded_taxa et
        LEFT JOIN expanded_taxa ia ON et."immediateAncestor_taxonID" = ia."taxonID"
        LEFT JOIN expanded_taxa ima ON et."immediateMajorAncestor_taxonID" = ima."taxonID"
        WHERE et.name IN ('Apis mellifera', 'Vespa mandarinia', 'Anthophila')
    """)
    
    test_results = session.execute(test_sql)
    logger.info("\nVerifying specific taxa:")
    for row in test_results:
        logger.info(f"  {row[1]} (rank {row[2]}): immediate={row[4]} (rank {row[5]}), major={row[7]} (rank {row[8]})")
    
    # Check counts
    count_sql = text("""
        SELECT 
            COUNT(*) as total,
            COUNT("immediateAncestor_taxonID") as with_immediate,
#|LN|250|
            COUNT("immediateMajorAncestor_taxonID") as with_major
        FROM expanded_taxa;
    """)
    
    count_result = session.execute(count_sql).fetchone()
    logger.info(f"\nTotal taxa: {count_result[0]}")
    logger.info(f"Taxa with immediate ancestor: {count_result[1]} ({100*count_result[1]/count_result[0]:.1f}%)")
    logger.info(f"Taxa with immediate major ancestor: {count_result[2]} ({100*count_result[2]/count_result[0]:.1f}%)")

def main():
    parser = argparse.ArgumentParser(description="Add and populate immediate ancestor columns in expanded_taxa.")
    parser.add_argument("--db-user", default=os.getenv("DB_USER", "postgres"))
    parser.add_argument("--db-password", default=os.getenv("DB_PASSWORD", "password"))
    parser.add_argument("--db-host", default=os.getenv("DB_HOST", "localhost"))
    parser.add_argument("--db-port", default=os.getenv("DB_PORT", "5432"))
    parser.add_argument("--db-name", default=os.getenv("DB_NAME", "ibrida-v0-r1"))
    parser.add_argument("--batch-size", type=int, default=50000, help="Batch size for updates")
    parser.add_argument("--skip-add-columns", action="store_true", help="Skip adding columns if they already exist")
    parser.add_argument("--skip-populate", action="store_true", help="Skip populating the columns")
    parser.add_argument("--skip-indexes", action="store_true", help="Skip creating indexes")
    parser.add_argument("--verify-only", action="store_true", help="Only verify existing data")
    
    args = parser.parse_args()
    
    engine = get_db_engine(args.db_user, args.db_password, args.db_host, args.db_port, args.db_name)
#|LN|275|
    Session = sessionmaker(bind=engine)
    session = Session()
    
    try:
        if args.verify_only:
            verify_results(session)
        else:
            # Add columns
            if not args.skip_add_columns:
                add_columns_if_not_exists(session)
            
            # Populate data
            if not args.skip_populate:
                populate_immediate_ancestors(session, engine, args.batch_size)
            
            # Create indexes
            if not args.skip_indexes:
                create_indexes(session)
            
            # Verify results
            verify_results(session)
            
        logger.info("Process completed successfully!")
        
    except Exception as e:
#|LN|300|
        logger.error(f"An error occurred: {e}")
        session.rollback()
        raise
    finally:
        session.close()

if __name__ == "__main__":
    main()
      </file>
      <dir path="scripts/ingest_coldp">
        <file path="scripts/ingest_coldp/__init__.py">
# Catalog of Life Data Package (ColDP) ingest scripts

        </file>
        <file path="scripts/ingest_coldp/load_tables.py" line_interval="25">
#!/usr/bin/env python3

import argparse
import os
import pandas as pd
import csv
import logging
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# Import models from the top-level models directory
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from models.base import Base
from models.coldp_models import (
    ColdpNameUsage,
    ColdpVernacularName,
    ColdpDistribution,
    ColdpMedia,
    ColdpReference,
    ColdpTypeMaterial
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
#|LN|25|
logger = logging.getLogger(__name__)

# --- Column Mappings (from 'col:FieldName' in TSV to 'fieldName' in ORM) ---
COLDP_TSV_FILES_AND_MODELS = {
    "NameUsage.tsv": ColdpNameUsage,
    "VernacularName.tsv": ColdpVernacularName,
    "Distribution.tsv": ColdpDistribution,
    "Media.tsv": ColdpMedia,
    "Reference.tsv": ColdpReference,
    "TypeMaterial.tsv": ColdpTypeMaterial,
}

def get_db_engine(db_user, db_password, db_host, db_port, db_name):
    connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    return create_engine(connection_string)

def create_schemas(engine):
    logger.info("Creating ColDP tables in the database (if they don't exist)...")
    tables_to_create = [
        model.__table__ for model in COLDP_TSV_FILES_AND_MODELS.values()
    ]
    Base.metadata.create_all(engine, tables=tables_to_create, checkfirst=True)
    logger.info("ColDP table schemas ensured.")

def clean_column_name(col_name, prefix="col:"):
#|LN|50|
    if col_name.startswith(prefix):
        return col_name[len(prefix):]
    return col_name

def safe_bool_convert(value):
    if isinstance(value, str):
        val_lower = value.lower()
        if val_lower == 'true':
            return True
        elif val_lower == 'false':
            return False
    elif isinstance(value, bool):
        return value
    return None # Or raise error, or return default

def load_table_from_tsv(session, model_class, tsv_path, col_prefix="col:"):
    logger.info(f"Loading data for {model_class.__tablename__} from {tsv_path}...")
    if not os.path.exists(tsv_path):
        logger.error(f"TSV file not found: {tsv_path}")
        return

    try:
        # Read with dtype=str to handle various inputs, convert specific columns later
        df = pd.read_csv(
            tsv_path,
#|LN|75|
            sep='\t',
            header=0,
            quoting=csv.QUOTE_NONE,
            dtype=str, # Read all as string initially
            keep_default_na=False,
            na_values=['', 'NA', 'N/A', '#N/A'] # Define what pandas should see as NaN
        )

        # Clean column names (e.g., 'col:taxonID' -> 'taxonID')
        df.columns = [clean_column_name(col, prefix=col_prefix) for col in df.columns]

        # Convert to None where appropriate (pandas reads empty strings as '', not NaN with keep_default_na=False)
        df = df.replace({ '': None })


        # --- Specific Column Type Conversions ---
        # Example for boolean columns (adjust based on your actual ColDP files/models)
        if model_class == ColdpVernacularName:
            if 'preferred' in df.columns:
                df['preferred'] = df['preferred'].apply(safe_bool_convert)
        
        # Example for numeric types (float, integer) - pandas might infer some, but explicit is safer
        # For SQLAlchemy, None will be interpreted as SQL NULL.
        # SQLAlchemy will handle type conversion for basic types like int, float if pandas df has them as objects
        # that can be cast. For stricter control, cast in pandas:
#|LN|100|
        # if 'latitude' in df.columns and model_class == ColdpTypeMaterial:
        #     df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce') # Coerce will turn errors to NaT/NaN

        # Clear existing data from the table
        logger.info(f"Clearing existing data from {model_class.__tablename__}...")
        session.query(model_class).delete(synchronize_session=False)
        
        records = df.to_dict(orient='records')
        
        if records:
            logger.info(f"Bulk inserting {len(records)} records into {model_class.__tablename__}...")
            session.bulk_insert_mappings(model_class, records)
            session.commit()
            logger.info(f"Successfully loaded data into {model_class.__tablename__}.")
        else:
            logger.info(f"No records to load for {model_class.__tablename__}.")

    except Exception as e:
        session.rollback()
        logger.error(f"Error loading data for {model_class.__tablename__} from {tsv_path}: {e}")
        raise

def verify_schema_field_lengths(engine):
    """
    Verifies that columns in the database have sufficient field length.
#|LN|125|
    """
    # Define the expected length for fields and tables to check
    column_specs = [
        # TaxonID and related fields
        ('coldp_vernacular_name', 'taxonID', 64),
        ('coldp_distribution', 'taxonID', 64),
        ('coldp_media', 'taxonID', 64),
        ('coldp_type_material', 'nameID', 64)
    ]
    
    # Note: We don't check reference fields anymore since they're all TEXT type now
    
    # The tables might not exist yet, so we'll handle exceptions
    tables_checked = False
    for table_name, column_name, expected_length in column_specs:
        try:
            query = text(f"""
                SELECT character_maximum_length 
                FROM information_schema.columns 
                WHERE table_name='{table_name}' AND column_name='{column_name}'
            """)
            
            with engine.connect() as conn:
                result = conn.execute(query).scalar()
                tables_checked = True
#|LN|150|
            
            if result is not None and result < expected_length:
                logger.warning(f"WARNING: {table_name}.{column_name} has length {result}, expected {expected_length}")
                return False
        except Exception as e:
            # Table might not exist yet, which is fine since we'll create it
            logger.info(f"Table {table_name} not found or other error checking schema: {e}")
    
    return tables_checked

def main():
    parser = argparse.ArgumentParser(description="Load ColDP TSV data into PostgreSQL database.")
    parser.add_argument("--coldp-dir", required=True, help="Path to the unzipped ColDP directory.")
    parser.add_argument("--db-user", default=os.getenv("DB_USER", "postgres"), help="Database user.")
    parser.add_argument("--db-password", default=os.getenv("DB_PASSWORD", "password"), help="Database password.")
    parser.add_argument("--db-host", default=os.getenv("DB_HOST", "localhost"), help="Database host.")
    parser.add_argument("--db-port", default=os.getenv("DB_PORT", "5432"), help="Database port.")
    parser.add_argument("--db-name", default=os.getenv("DB_NAME", "ibrida-v0-r1"), help="Database name.")
    args = parser.parse_args()

    engine = get_db_engine(args.db_user, args.db_password, args.db_host, args.db_port, args.db_name)
    
    # Verify schema field lengths (for existing tables)
    tables_exist = verify_schema_field_lengths(engine)
    if tables_exist:
#|LN|175|
        logger.info("Schema validation completed. Field lengths are sufficient or tables don't exist yet.")
    
    # 1. Create table schemas
    create_schemas(engine)

    Session = sessionmaker(bind=engine)
    session = Session()

    try:
        # 2. Load data for each table
        for tsv_file, model_cls in COLDP_TSV_FILES_AND_MODELS.items():
            full_tsv_path = os.path.join(args.coldp_dir, tsv_file)
            load_table_from_tsv(session, model_cls, full_tsv_path)
        logger.info("All ColDP tables loaded successfully.")
    except Exception as e:
        logger.error(f"An error occurred during the loading process: {e}")
    finally:
        session.close()

if __name__ == "__main__":
    main()
        </file>
        <file path="scripts/ingest_coldp/map_taxa.py" line_interval="25">
#!/usr/bin/env python3

import argparse
import os
import pandas as pd
import logging
from sqlalchemy import create_engine, Column, Integer, String, Float, Index, Text, ForeignKey
from sqlalchemy.orm import sessionmaker, relationship
from sqlalchemy.ext.declarative import declarative_base
from rapidfuzz import process, fuzz
import time

# Import models from the top-level models directory
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from models.base import Base 
from models.expanded_taxa import ExpandedTaxa # Target for common names
from models.coldp_models import ColdpNameUsage # Staging table for ColDP names

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- ORM for the crosswalk table ---
class InatToColdpMap(Base):
#|LN|25|
    __tablename__ = "inat_to_coldp_taxon_map"
    inat_taxon_id = Column(Integer, ForeignKey(f'{ExpandedTaxa.__tablename__}.taxonID'), primary_key=True)
    col_taxon_id = Column(String(64), ForeignKey(f'{ColdpNameUsage.__tablename__}.ID'), primary_key=True) # From ColDP NameUsage.ID
    
    match_type = Column(String(50), nullable=False) # e.g., 'exact_name_rank', 'exact_name_only', 'fuzzy_name'
    match_score = Column(Float, nullable=True)
    inat_scientific_name = Column(Text)
    col_scientific_name = Column(Text)

    # Relationships (optional but good practice)
    # inat_taxon = relationship("ExpandedTaxa") # If ExpandedTaxa is the ORM for your main taxa table
    # col_name_usage = relationship("ColdpNameUsage")


def get_db_engine(db_user, db_password, db_host, db_port, db_name):
    connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    return create_engine(connection_string)

def normalize_name(name):
    if pd.isna(name) or name is None:
        return None
    return ' '.join(str(name).lower().split()) # Lowercase and normalize whitespace

def create_crosswalk_table(engine):
    logger.info("Creating/ensuring 'inat_to_coldp_taxon_map' table exists...")
#|LN|50|
    Base.metadata.create_all(engine, tables=[InatToColdpMap.__table__], checkfirst=True)
    logger.info("'inat_to_coldp_taxon_map' table ensured.")

def get_taxon_ancestor_info(session, inat_taxa_df):
    """
    Retrieve ancestor information for resolving homonyms during fuzzy matching.
    """
    # Get a full list of taxa including ancestor levels for unmatched iNat taxa
    taxon_ids = inat_taxa_df['inat_taxon_id'].tolist()
    
    # Retrieve the full expanded_taxa rows including ancestor info
    ancestor_data = pd.read_sql_query(
        session.query(
            ExpandedTaxa.taxonID,
            ExpandedTaxa.L20_taxonID, ExpandedTaxa.L20_name,   # Genus
            ExpandedTaxa.L30_taxonID, ExpandedTaxa.L30_name,   # Family
            ExpandedTaxa.L40_taxonID, ExpandedTaxa.L40_name,   # Order 
            ExpandedTaxa.L50_taxonID, ExpandedTaxa.L50_name,   # Class
            ExpandedTaxa.L60_taxonID, ExpandedTaxa.L60_name    # Phylum
        )
        .filter(ExpandedTaxa.taxonID.in_(taxon_ids))
        .statement,
        session.bind
    )
    
#|LN|75|
    ancestor_data.set_index('taxonID', inplace=True)
    return ancestor_data

def resolve_homonyms(row, matches, coldp_names_df, ancestor_data, ancestor_map):
    """
    Resolve homonyms by comparing ancestor taxonomy.
    Returns the most likely match or None if no good match can be determined.
    """
    matches_df = pd.DataFrame(matches, columns=['col_name', 'score', 'idx'])
    
    # Filter matches to only include those above threshold
    matches_df = matches_df[matches_df['score'] > 89.0]  # Adjust threshold as needed
    
    if matches_df.empty:
        return None
    
    # If only one match, return it
    if len(matches_df) == 1:
        match_idx = matches_df.iloc[0]['idx']
        return {
            'inat_taxon_id': row['inat_taxon_id'],
            'col_taxon_id': coldp_names_df.iloc[match_idx]['col_taxon_id'],
            'match_type': 'fuzzy_name_single_match',
            'match_score': matches_df.iloc[0]['score'] / 100.0,
            'inat_scientific_name': row['inat_scientific_name'],
#|LN|100|
            'col_scientific_name': coldp_names_df.iloc[match_idx]['col_scientific_name']
        }
    
    # Multiple matches - try to use ancestor data to disambiguate
    inat_taxon_id = row['inat_taxon_id']
    if inat_taxon_id not in ancestor_data.index:
        # No ancestry data available for this taxon
        # Take highest score match
        matches_df = matches_df.sort_values('score', ascending=False)
        match_idx = matches_df.iloc[0]['idx']
        return {
            'inat_taxon_id': row['inat_taxon_id'],
            'col_taxon_id': coldp_names_df.iloc[match_idx]['col_taxon_id'],
            'match_type': 'fuzzy_name_highest_score',
            'match_score': matches_df.iloc[0]['score'] / 100.0,
            'inat_scientific_name': row['inat_scientific_name'],
            'col_scientific_name': coldp_names_df.iloc[match_idx]['col_scientific_name']
        }
    
    # Get iNat ancestor info
    inat_ancestors = ancestor_data.loc[inat_taxon_id]
    
    # Score each candidate match by comparing ancestors
    ancestor_scores = []
    for _, match in matches_df.iterrows():
#|LN|125|
        match_idx = match['idx']
        col_taxon_id = coldp_names_df.iloc[match_idx]['col_taxon_id']
        
        if col_taxon_id not in ancestor_map:
            # No ancestor data for this COL taxon - just use the fuzzy match score
            ancestor_scores.append((match_idx, 0, match['score']))
            continue
        
        col_ancestors = ancestor_map[col_taxon_id]
        
        # Check for matching ancestors at different ranks (genus, family, order, class, phylum)
        ancestor_matches = 0
        
        # Check genus
        inat_genus = normalize_name(inat_ancestors.get('L20_name'))
        col_genus = normalize_name(col_ancestors.get('genus'))
        if inat_genus and col_genus and inat_genus == col_genus:
            ancestor_matches += 2
        
        # Check family
        inat_family = normalize_name(inat_ancestors.get('L30_name'))
        col_family = normalize_name(col_ancestors.get('family'))
        if inat_family and col_family and inat_family == col_family:
            ancestor_matches += 1
        
#|LN|150|
        # Check order
        inat_order = normalize_name(inat_ancestors.get('L40_name'))
        col_order = normalize_name(col_ancestors.get('order'))
        if inat_order and col_order and inat_order == col_order:
            ancestor_matches += 1
        
        # Check class
        inat_class = normalize_name(inat_ancestors.get('L50_name'))
        col_class = normalize_name(col_ancestors.get('class'))
        if inat_class and col_class and inat_class == col_class:
            ancestor_matches += 1
        
        # Check phylum
        inat_phylum = normalize_name(inat_ancestors.get('L60_name'))
        col_phylum = normalize_name(col_ancestors.get('phylum'))
        if inat_phylum and col_phylum and inat_phylum == col_phylum:
            ancestor_matches += 1
        
        # Store the total score (combines fuzzy match score and ancestor matches)
        ancestor_scores.append((match_idx, ancestor_matches, match['score']))
    
    if not ancestor_scores:
        return None
        
    # Find the best match by prioritizing ancestor matches, then fuzzy score
#|LN|175|
    ancestor_scores.sort(key=lambda x: (x[1], x[2]), reverse=True)
    best_match_idx, ancestor_match_count, fuzzy_score = ancestor_scores[0]
    
    match_type = 'fuzzy_name_with_ancestors' if ancestor_match_count > 0 else 'fuzzy_name_no_ancestors'
    return {
        'inat_taxon_id': row['inat_taxon_id'],
        'col_taxon_id': coldp_names_df.iloc[best_match_idx]['col_taxon_id'],
        'match_type': match_type,
        'match_score': fuzzy_score / 100.0,
        'inat_scientific_name': row['inat_scientific_name'],
        'col_scientific_name': coldp_names_df.iloc[best_match_idx]['col_scientific_name']
    }

def build_col_ancestors_map(coldp_names_df):
    """
    Create a map of ColDP taxon IDs to their ancestor information from the NameUsage data
    """
    ancestor_map = {}
    
    # Extract and organize ancestor data
    for _, row in coldp_names_df.iterrows():
        col_taxon_id = row['col_taxon_id']
        ancestor_info = {
            'genus': row.get('genericName'),
            'family': row.get('family'),
#|LN|200|
            'order': row.get('order'),
            'class': row.get('class'),
            'phylum': row.get('phylum')
        }
        ancestor_map[col_taxon_id] = ancestor_info
    
    return ancestor_map

def perform_mapping(session, fuzzy_match=True, fuzzy_threshold=90):
    logger.info("Starting iNaturalist to ColDP taxon mapping process...")

    # 0. Clear existing mapping data
    logger.info("Clearing existing data from 'inat_to_coldp_taxon_map'...")
    session.query(InatToColdpMap).delete(synchronize_session=False)
    session.commit()

    # 1. Load iNat taxa data from expanded_taxa
    logger.info("Loading iNaturalist taxa from 'expanded_taxa' table...")
    inat_taxa_df = pd.read_sql_query(
        session.query(ExpandedTaxa.taxonID, ExpandedTaxa.name, ExpandedTaxa.rank)
               .filter(ExpandedTaxa.taxonActive == True) # Only map active iNat taxa
               .statement,
        session.bind
    )
    inat_taxa_df.rename(columns={'taxonID': 'inat_taxon_id', 'name': 'inat_scientific_name', 'rank': 'inat_rank'}, inplace=True)
#|LN|225|
    inat_taxa_df['norm_inat_name'] = inat_taxa_df['inat_scientific_name'].apply(normalize_name)
    inat_taxa_df['norm_inat_rank'] = inat_taxa_df['inat_rank'].apply(normalize_name)
    logger.info(f"Loaded {len(inat_taxa_df)} active iNaturalist taxa.")

    # 2. Load ColDP NameUsage data (from coldp_name_usage_staging)
    logger.info("Loading ColDP NameUsage data from 'coldp_name_usage_staging' table...")
    
    # For fuzzy matching, we need to load more columns to help with ancestor comparison
    if fuzzy_match:
        coldp_names_df = pd.read_sql_query(
            session.query(
                ColdpNameUsage.ID, 
                ColdpNameUsage.scientificName, 
                ColdpNameUsage.rank, 
                ColdpNameUsage.status,
                ColdpNameUsage.genericName,
                ColdpNameUsage.specificEpithet,
                ColdpNameUsage.family,
                ColdpNameUsage.order,
                ColdpNameUsage.class_,
                ColdpNameUsage.phylum
            )
            .statement, 
            session.bind
        )
#|LN|250|
    else:
        coldp_names_df = pd.read_sql_query(
            session.query(ColdpNameUsage.ID, ColdpNameUsage.scientificName, ColdpNameUsage.rank, ColdpNameUsage.status)
                   .statement,
            session.bind
        )
    
    coldp_names_df.rename(columns={'ID': 'col_taxon_id', 'scientificName': 'col_scientific_name', 
                              'rank': 'col_rank', 'status': 'col_status'}, inplace=True)
    coldp_names_df['norm_col_name'] = coldp_names_df['col_scientific_name'].apply(normalize_name)
    coldp_names_df['norm_col_rank'] = coldp_names_df['col_rank'].apply(normalize_name)
    logger.info(f"Loaded {len(coldp_names_df)} ColDP NameUsage entries.")

    all_mappings = []

    # --- Step 3: Exact Match (Name + Rank), prioritize 'accepted' ColDP status ---
    logger.info("Attempting exact match on scientific name and rank...")
    merged_exact_rank = pd.merge(
        inat_taxa_df,
        coldp_names_df,
        left_on=['norm_inat_name', 'norm_inat_rank'],
        right_on=['norm_col_name', 'norm_col_rank'],
        how='inner'
    )
    # Prioritize 'accepted' status
#|LN|275|
    merged_exact_rank.sort_values(by=['inat_taxon_id', 'col_status'], ascending=[True, True], inplace=True) # 'accepted' often comes first alphabetically
    merged_exact_rank_unique = merged_exact_rank.drop_duplicates(subset=['inat_taxon_id'], keep='first')

    for _, row in merged_exact_rank_unique.iterrows():
        all_mappings.append({
            'inat_taxon_id': row['inat_taxon_id'],
            'col_taxon_id': row['col_taxon_id'],
            'match_type': 'exact_name_rank_accepted' if row['col_status'] == 'accepted' else 'exact_name_rank_other_status',
            'match_score': 1.0,
            'inat_scientific_name': row['inat_scientific_name'],
            'col_scientific_name': row['col_scientific_name']
        })
    logger.info(f"Found {len(merged_exact_rank_unique)} matches on name and rank.")
    
    # Update iNat taxa df to exclude matched items
    inat_taxa_df = inat_taxa_df[~inat_taxa_df['inat_taxon_id'].isin(merged_exact_rank_unique['inat_taxon_id'])]

    # --- Step 4: Exact Match (Name only), prioritize 'accepted' ---
    if not inat_taxa_df.empty:
        logger.info(f"Attempting exact match on scientific name only for {len(inat_taxa_df)} remaining iNat taxa...")
        merged_exact_name_only = pd.merge(
            inat_taxa_df,
            coldp_names_df, # Could filter coldp_names_df for status='accepted' first for efficiency
            left_on=['norm_inat_name'],
            right_on=['norm_col_name'],
#|LN|300|
            how='inner'
        )
        merged_exact_name_only.sort_values(by=['inat_taxon_id', 'col_status'], ascending=[True, True], inplace=True)
        merged_exact_name_only_unique = merged_exact_name_only.drop_duplicates(subset=['inat_taxon_id'], keep='first')

        for _, row in merged_exact_name_only_unique.iterrows():
             all_mappings.append({
                'inat_taxon_id': row['inat_taxon_id'],
                'col_taxon_id': row['col_taxon_id'],
                'match_type': 'exact_name_only_accepted' if row['col_status'] == 'accepted' else 'exact_name_only_other_status',
                'match_score': 0.95, # Slightly lower score than name+rank
                'inat_scientific_name': row['inat_scientific_name'],
                'col_scientific_name': row['col_scientific_name']
            })
        logger.info(f"Found {len(merged_exact_name_only_unique)} matches on name only.")
        inat_taxa_df = inat_taxa_df[~inat_taxa_df['inat_taxon_id'].isin(merged_exact_name_only_unique['inat_taxon_id'])]

    # --- Step 5: Fuzzy Match ---
    fuzzy_match_count = 0
    if fuzzy_match and not inat_taxa_df.empty:
        logger.info(f"Attempting fuzzy matching for {len(inat_taxa_df)} remaining iNat taxa...")
        
        # Prepare for homonym resolution: get ancestor information
        ancestor_data = get_taxon_ancestor_info(session, inat_taxa_df)
        logger.info(f"Retrieved ancestor data for {len(ancestor_data)} unmatched taxa")
#|LN|325|
        
        # Build a map of ColDP taxon IDs to their ancestor information
        ancestor_map = build_col_ancestors_map(coldp_names_df)
        logger.info(f"Built ancestor map for {len(ancestor_map)} ColDP taxa")
        
        # Get list of normalized ColDP names for fuzzy matching
        coldp_names_list = coldp_names_df['norm_col_name'].dropna().tolist()
        
        # Separate the accepted names for preferential matching
        accepted_coldp_df = coldp_names_df[coldp_names_df['col_status'] == 'accepted']
        accepted_coldp_names = accepted_coldp_df['norm_col_name'].dropna().tolist()
        
        # Filter out null/None values before fuzzy matching
        inat_taxa_filtered = inat_taxa_df[inat_taxa_df['norm_inat_name'].notna()]
        
        # Use batch size to process large dataframes in chunks
        batch_size = 1000
        fuzzy_matches = []
        
        for start_idx in range(0, len(inat_taxa_filtered), batch_size):
            end_idx = min(start_idx + batch_size, len(inat_taxa_filtered))
            batch = inat_taxa_filtered.iloc[start_idx:end_idx]
            
            logger.info(f"Processing fuzzy match batch {start_idx}-{end_idx} of {len(inat_taxa_filtered)}")
            batch_start_time = time.time()
#|LN|350|
            
            for _, row in batch.iterrows():
                # First try to match against accepted names only
                if accepted_coldp_names:  # Skip if empty
                    matches = process.extract(
                        query=row['norm_inat_name'],
                        choices=accepted_coldp_names, 
                        scorer=fuzz.WRatio,  # WRatio is good for scientific names with different word orders
                        score_cutoff=fuzzy_threshold,
                        limit=5
                    )
                    
                    # If no good matches in accepted names, try all names
                    if not matches and coldp_names_list:
                        matches = process.extract(
                            query=row['norm_inat_name'],
                            choices=coldp_names_list,
                            scorer=fuzz.WRatio,
                            score_cutoff=fuzzy_threshold,
                            limit=5
                        )
                    
                    # Find the index of each match in the original DataFrame
                    if matches:
                        matches_with_indices = []
#|LN|375|
                        for match_tuple in matches:
                            # Handle both (string, score) and (string, score, index) formats from rapidfuzz
                            if len(match_tuple) == 2:
                                match_name, score = match_tuple
                            else:
                                match_name, score, _ = match_tuple
                            # For accepted_coldp_names matches
                            indices = accepted_coldp_df[accepted_coldp_df['norm_col_name'] == match_name].index.tolist()
                            if indices:
                                for idx in indices:
                                    matches_with_indices.append((match_name, score, idx))
                            else:
                                # For coldp_names_list matches
                                indices = coldp_names_df[coldp_names_df['norm_col_name'] == match_name].index.tolist()
                                for idx in indices:
                                    matches_with_indices.append((match_name, score, idx))
                        
                        # Resolve homonyms and get the best match
                        match_result = resolve_homonyms(row, matches_with_indices, coldp_names_df, ancestor_data, ancestor_map)
                        if match_result:
                            fuzzy_matches.append(match_result)
            
            batch_end_time = time.time()
            logger.info(f"Batch processed in {batch_end_time - batch_start_time:.2f} seconds")
        
#|LN|400|
        fuzzy_match_count = len(fuzzy_matches)
        all_mappings.extend(fuzzy_matches)
        logger.info(f"Found {fuzzy_match_count} fuzzy matches.")
    
    # Calculate how many unmatched taxa remain
    total_exact_matches = len(all_mappings) - fuzzy_match_count
    total_unmatched = len(inat_taxa_df) - fuzzy_match_count
    logger.info(f"Summary: {total_exact_matches} exact matches, {fuzzy_match_count} fuzzy matches, {total_unmatched} remaining unmatched taxa.")
    
    # Save match statistics by type
    match_types = {}
    for mapping in all_mappings:
        match_type = mapping['match_type']
        if match_type not in match_types:
            match_types[match_type] = 0
        match_types[match_type] += 1
    
    logger.info("Match statistics by type:")
    for match_type, count in match_types.items():
        logger.info(f"  {match_type}: {count}")

    # --- Step 6: Persist mappings ---
    if all_mappings:
        logger.info(f"Bulk inserting {len(all_mappings)} mappings into 'inat_to_coldp_taxon_map'...")
        session.bulk_insert_mappings(InatToColdpMap, all_mappings)
#|LN|425|
        session.commit()
        logger.info("Successfully populated 'inat_to_coldp_taxon_map'.")
    else:
        logger.info("No mappings found to insert.")


def main():
    parser = argparse.ArgumentParser(description="Map iNaturalist taxa to ColDP taxa.")
    parser.add_argument("--db-user", default=os.getenv("DB_USER", "postgres"))
    parser.add_argument("--db-password", default=os.getenv("DB_PASSWORD", "password"))
    parser.add_argument("--db-host", default=os.getenv("DB_HOST", "localhost"))
    parser.add_argument("--db-port", default=os.getenv("DB_PORT", "5432"))
    parser.add_argument("--db-name", default=os.getenv("DB_NAME", "ibrida-v0-r1"))
    parser.add_argument("--fuzzy-match", action="store_true", help="Enable fuzzy matching for unmatched taxa")
    parser.add_argument("--fuzzy-threshold", type=int, default=90, help="Threshold score (0-100) for fuzzy matching")
    parser.add_argument("--debug", action="store_true", help="Enable debug output")
    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    engine = get_db_engine(args.db_user, args.db_password, args.db_host, args.db_port, args.db_name)
    create_crosswalk_table(engine) # Ensure table exists

    Session = sessionmaker(bind=engine)
#|LN|450|
    session = Session()

    try:
        perform_mapping(session, fuzzy_match=args.fuzzy_match, fuzzy_threshold=args.fuzzy_threshold)
    except Exception as e:
        logger.error(f"An error occurred during the mapping process: {e}")
        session.rollback()
    finally:
        session.close()

if __name__ == "__main__":
    main()
        </file>
        <file path="scripts/ingest_coldp/map_taxa_parallel.py" line_interval="25">
#!/usr/bin/env python3

import argparse
import os
import pandas as pd
import logging
import time
import multiprocessing
from functools import partial
from sqlalchemy import create_engine, Column, Integer, String, Float, Text, ForeignKey
from sqlalchemy.orm import sessionmaker
from rapidfuzz import process, fuzz
import tempfile
import json

# Import models from the top-level models directory
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from models.base import Base 
from models.expanded_taxa import ExpandedTaxa # Target for common names
from models.coldp_models import ColdpNameUsage # Staging table for ColDP names

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
#|LN|25|

# --- ORM for the crosswalk table ---
class InatToColdpMap(Base):
    __tablename__ = "inat_to_coldp_taxon_map"
    inat_taxon_id = Column(Integer, ForeignKey(f'{ExpandedTaxa.__tablename__}.taxonID'), primary_key=True)
    col_taxon_id = Column(String(64), ForeignKey(f'{ColdpNameUsage.__tablename__}.ID'), primary_key=True) # From ColDP NameUsage.ID
    
    match_type = Column(String(50), nullable=False) # e.g., 'exact_name_rank', 'exact_name_only', 'fuzzy_name'
    match_score = Column(Float, nullable=True)
    inat_scientific_name = Column(Text)
    col_scientific_name = Column(Text)


def get_db_engine(db_user, db_password, db_host, db_port, db_name):
    connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    return create_engine(connection_string)

def normalize_name(name):
    if pd.isna(name) or name is None:
        return None
    return ' '.join(str(name).lower().split()) # Lowercase and normalize whitespace

def create_crosswalk_table(engine):
    logger.info("Creating/ensuring 'inat_to_coldp_taxon_map' table exists...")
    Base.metadata.create_all(engine, tables=[InatToColdpMap.__table__], checkfirst=True)
#|LN|50|
    logger.info("'inat_to_coldp_taxon_map' table ensured.")

def get_taxon_ancestor_info(session, inat_taxa_df):
    """
    Retrieve ancestor information for resolving homonyms during fuzzy matching.
    """
    # Get a full list of taxa including ancestor levels for unmatched iNat taxa
    taxon_ids = inat_taxa_df['inat_taxon_id'].tolist()
    
    # Retrieve the full expanded_taxa rows including ancestor info
    ancestor_data = pd.read_sql_query(
        session.query(
            ExpandedTaxa.taxonID,
            ExpandedTaxa.L20_taxonID, ExpandedTaxa.L20_name,   # Genus
            ExpandedTaxa.L30_taxonID, ExpandedTaxa.L30_name,   # Family
            ExpandedTaxa.L40_taxonID, ExpandedTaxa.L40_name,   # Order 
            ExpandedTaxa.L50_taxonID, ExpandedTaxa.L50_name,   # Class
            ExpandedTaxa.L60_taxonID, ExpandedTaxa.L60_name    # Phylum
        )
        .filter(ExpandedTaxa.taxonID.in_(taxon_ids))
        .statement,
        session.bind
    )
    
    # Convert to dict for faster lookups in the worker processes
#|LN|75|
    ancestor_dict = {}
    for _, row in ancestor_data.iterrows():
        taxon_id = row['taxonID']
        ancestor_dict[taxon_id] = {
            'L20_name': row.get('L20_name'),
            'L30_name': row.get('L30_name'),
            'L40_name': row.get('L40_name'),
            'L50_name': row.get('L50_name'),
            'L60_name': row.get('L60_name')
        }
    
    return ancestor_dict

def build_col_ancestors_map(coldp_names_df):
    """
    Create a map of ColDP taxon IDs to their ancestor information from the NameUsage data
    """
    ancestor_map = {}
    
    # Extract and organize ancestor data
    for _, row in coldp_names_df.iterrows():
        col_taxon_id = row['col_taxon_id']
        ancestor_info = {
            'genus': row.get('genericName'),
            'family': row.get('family'),
#|LN|100|
            'order': row.get('order'),
            'class': row.get('class'),
            'phylum': row.get('phylum')
        }
        ancestor_map[col_taxon_id] = ancestor_info
    
    return ancestor_map

def resolve_homonyms_lightweight(row, matches_with_taxon_info, ancestor_data, ancestor_map):
    """
    Lightweight version of resolve_homonyms that works with taxon info dictionaries instead of DataFrames.
    """
    # Filter matches to only include those above threshold
    good_matches = [(name, score, taxon_info) for name, score, taxon_info in matches_with_taxon_info if score > 89.0]
    
    if not good_matches:
        return None
    
    # If only one match, return it
    if len(good_matches) == 1:
        match_name, score, taxon_info = good_matches[0]
        return {
            'inat_taxon_id': row['inat_taxon_id'],
            'col_taxon_id': taxon_info['col_taxon_id'],
            'match_type': 'fuzzy_name_single_match',
#|LN|125|
            'match_score': score / 100.0,
            'inat_scientific_name': row['inat_scientific_name'],
            'col_scientific_name': taxon_info['col_scientific_name']
        }
    
    # Multiple matches - try to use ancestor data to disambiguate
    inat_taxon_id = row['inat_taxon_id']
    if inat_taxon_id not in ancestor_data:
        # No ancestry data available for this taxon - take highest score match
        good_matches.sort(key=lambda x: x[1], reverse=True)
        match_name, score, taxon_info = good_matches[0]
        return {
            'inat_taxon_id': row['inat_taxon_id'],
            'col_taxon_id': taxon_info['col_taxon_id'],
            'match_type': 'fuzzy_name_highest_score',
            'match_score': score / 100.0,
            'inat_scientific_name': row['inat_scientific_name'],
            'col_scientific_name': taxon_info['col_scientific_name']
        }
    
    # Get iNat ancestor info
    inat_ancestors = ancestor_data[inat_taxon_id]
    
    # Score each candidate match by comparing ancestors
    ancestor_scores = []
#|LN|150|
    for match_name, score, taxon_info in good_matches:
        col_taxon_id = taxon_info['col_taxon_id']
        
        if col_taxon_id not in ancestor_map:
            # No ancestor data for this COL taxon - just use the fuzzy match score
            ancestor_scores.append((taxon_info, 0, score))
            continue
        
        col_ancestors = ancestor_map[col_taxon_id]
        
        # Check for matching ancestors at different ranks (genus, family, order, class, phylum)
        ancestor_matches = 0
        
        # Check genus
        inat_genus = normalize_name(inat_ancestors.get('L20_name'))
        col_genus = normalize_name(col_ancestors.get('genus'))
        if inat_genus and col_genus and inat_genus == col_genus:
            ancestor_matches += 2
        
        # Check family
        inat_family = normalize_name(inat_ancestors.get('L30_name'))
        col_family = normalize_name(col_ancestors.get('family'))
        if inat_family and col_family and inat_family == col_family:
            ancestor_matches += 1
        
#|LN|175|
        # Check order
        inat_order = normalize_name(inat_ancestors.get('L40_name'))
        col_order = normalize_name(col_ancestors.get('order'))
        if inat_order and col_order and inat_order == col_order:
            ancestor_matches += 1
        
        # Check class
        inat_class = normalize_name(inat_ancestors.get('L50_name'))
        col_class = normalize_name(col_ancestors.get('class'))
        if inat_class and col_class and inat_class == col_class:
            ancestor_matches += 1
        
        # Check phylum
        inat_phylum = normalize_name(inat_ancestors.get('L60_name'))
        col_phylum = normalize_name(col_ancestors.get('phylum'))
        if inat_phylum and col_phylum and inat_phylum == col_phylum:
            ancestor_matches += 1
        
        # Store the total score (combines fuzzy match score and ancestor matches)
        ancestor_scores.append((taxon_info, ancestor_matches, score))
    
    if not ancestor_scores:
        return None
        
    # Find the best match by prioritizing ancestor matches, then fuzzy score
#|LN|200|
    ancestor_scores.sort(key=lambda x: (x[1], x[2]), reverse=True)
    best_taxon_info, ancestor_match_count, fuzzy_score = ancestor_scores[0]
    
    match_type = 'fuzzy_name_with_ancestors' if ancestor_match_count > 0 else 'fuzzy_name_no_ancestors'
    return {
        'inat_taxon_id': row['inat_taxon_id'],
        'col_taxon_id': best_taxon_info['col_taxon_id'],
        'match_type': match_type,
        'match_score': fuzzy_score / 100.0,
        'inat_scientific_name': row['inat_scientific_name'],
        'col_scientific_name': best_taxon_info['col_scientific_name']
    }

def resolve_homonyms(row, matches, coldp_names_df, ancestor_data, ancestor_map):
    """
    Resolve homonyms by comparing ancestor taxonomy.
    Returns the most likely match or None if no good match can be determined.
    """
    matches_df = pd.DataFrame(matches, columns=['col_name', 'score', 'idx'])
    
    # Filter matches to only include those above threshold
    matches_df = matches_df[matches_df['score'] > 89.0]  # Adjust threshold as needed
    
    if matches_df.empty:
        return None
#|LN|225|
    
    # If only one match, return it
    if len(matches_df) == 1:
        match_idx = matches_df.iloc[0]['idx']
        return {
            'inat_taxon_id': row['inat_taxon_id'],
            'col_taxon_id': coldp_names_df.iloc[match_idx]['col_taxon_id'],
            'match_type': 'fuzzy_name_single_match',
            'match_score': matches_df.iloc[0]['score'] / 100.0,
            'inat_scientific_name': row['inat_scientific_name'],
            'col_scientific_name': coldp_names_df.iloc[match_idx]['col_scientific_name']
        }
    
    # Multiple matches - try to use ancestor data to disambiguate
    inat_taxon_id = row['inat_taxon_id']
    if inat_taxon_id not in ancestor_data:
        # No ancestry data available for this taxon
        # Take highest score match
        matches_df = matches_df.sort_values('score', ascending=False)
        match_idx = matches_df.iloc[0]['idx']
        return {
            'inat_taxon_id': row['inat_taxon_id'],
            'col_taxon_id': coldp_names_df.iloc[match_idx]['col_taxon_id'],
            'match_type': 'fuzzy_name_highest_score',
            'match_score': matches_df.iloc[0]['score'] / 100.0,
#|LN|250|
            'inat_scientific_name': row['inat_scientific_name'],
            'col_scientific_name': coldp_names_df.iloc[match_idx]['col_scientific_name']
        }
    
    # Get iNat ancestor info
    inat_ancestors = ancestor_data[inat_taxon_id]
    
    # Score each candidate match by comparing ancestors
    ancestor_scores = []
    for _, match in matches_df.iterrows():
        match_idx = match['idx']
        col_taxon_id = coldp_names_df.iloc[match_idx]['col_taxon_id']
        
        if col_taxon_id not in ancestor_map:
            # No ancestor data for this COL taxon - just use the fuzzy match score
            ancestor_scores.append((match_idx, 0, match['score']))
            continue
        
        col_ancestors = ancestor_map[col_taxon_id]
        
        # Check for matching ancestors at different ranks (genus, family, order, class, phylum)
        ancestor_matches = 0
        
        # Check genus
        inat_genus = normalize_name(inat_ancestors.get('L20_name'))
#|LN|275|
        col_genus = normalize_name(col_ancestors.get('genus'))
        if inat_genus and col_genus and inat_genus == col_genus:
            ancestor_matches += 2
        
        # Check family
        inat_family = normalize_name(inat_ancestors.get('L30_name'))
        col_family = normalize_name(col_ancestors.get('family'))
        if inat_family and col_family and inat_family == col_family:
            ancestor_matches += 1
        
        # Check order
        inat_order = normalize_name(inat_ancestors.get('L40_name'))
        col_order = normalize_name(col_ancestors.get('order'))
        if inat_order and col_order and inat_order == col_order:
            ancestor_matches += 1
        
        # Check class
        inat_class = normalize_name(inat_ancestors.get('L50_name'))
        col_class = normalize_name(col_ancestors.get('class'))
        if inat_class and col_class and inat_class == col_class:
            ancestor_matches += 1
        
        # Check phylum
        inat_phylum = normalize_name(inat_ancestors.get('L60_name'))
        col_phylum = normalize_name(col_ancestors.get('phylum'))
#|LN|300|
        if inat_phylum and col_phylum and inat_phylum == col_phylum:
            ancestor_matches += 1
        
        # Store the total score (combines fuzzy match score and ancestor matches)
        ancestor_scores.append((match_idx, ancestor_matches, match['score']))
    
    if not ancestor_scores:
        return None
        
    # Find the best match by prioritizing ancestor matches, then fuzzy score
    ancestor_scores.sort(key=lambda x: (x[1], x[2]), reverse=True)
    best_match_idx, ancestor_match_count, fuzzy_score = ancestor_scores[0]
    
    match_type = 'fuzzy_name_with_ancestors' if ancestor_match_count > 0 else 'fuzzy_name_no_ancestors'
    return {
        'inat_taxon_id': row['inat_taxon_id'],
        'col_taxon_id': coldp_names_df.iloc[best_match_idx]['col_taxon_id'],
        'match_type': match_type,
        'match_score': fuzzy_score / 100.0,
        'inat_scientific_name': row['inat_scientific_name'],
        'col_scientific_name': coldp_names_df.iloc[best_match_idx]['col_scientific_name']
    }

def process_fuzzy_batch(batch_data, fuzzy_threshold=90):
    """
#|LN|325|
    Process a batch of taxa for fuzzy matching in a separate process.
    
    Args:
        batch_data: Dictionary containing all data needed for processing:
            - batch: DataFrame of iNat taxa to process
            - accepted_coldp_names: List of accepted ColDP normalized names
            - all_coldp_names: List of all ColDP normalized names
            - coldp_lookup: Dictionary for name -> taxon info lookup
            - ancestor_data: Dictionary of iNat ancestor data
            - ancestor_map: Dictionary of ColDP ancestor data
    
    Returns:
        List of match results
    """
    batch = batch_data['batch']
    accepted_coldp_names = batch_data['accepted_coldp_names']
    all_coldp_names = batch_data['all_coldp_names']
    coldp_lookup = batch_data['coldp_lookup']
    ancestor_data = batch_data['ancestor_data']
    ancestor_map = batch_data['ancestor_map']
    
    fuzzy_matches = []
    start_time = time.time()
    
    batch_size = len(batch)
#|LN|350|
    batch_id = batch_data.get('batch_id', 0)
    
    # Process each taxon in the batch
    for i, (_, row) in enumerate(batch.iterrows()):
        if i % 100 == 0 and i > 0:
            elapsed = time.time() - start_time
            progress = i / batch_size * 100
            logger.info(f"Batch {batch_id}: Processed {i}/{batch_size} ({progress:.1f}%) in {elapsed:.2f}s")
        
        # First try to match against accepted names only
        if accepted_coldp_names:  # Skip if empty
            matches = process.extract(
                query=row['norm_inat_name'],
                choices=accepted_coldp_names, 
                scorer=fuzz.WRatio,  # WRatio is good for scientific names with different word orders
                score_cutoff=fuzzy_threshold,
                limit=5
            )
            
            # If no good matches in accepted names, try all names
            if not matches and all_coldp_names:
                matches = process.extract(
                    query=row['norm_inat_name'],
                    choices=all_coldp_names,
                    scorer=fuzz.WRatio,
#|LN|375|
                    score_cutoff=fuzzy_threshold,
                    limit=5
                )
            
            # Convert matches to format expected by resolve_homonyms
            if matches:
                matches_with_taxon_info = []
                for match_tuple in matches:
                    # Handle both (string, score) and (string, score, index) formats from rapidfuzz
                    if len(match_tuple) == 2:
                        match_name, score = match_tuple
                    else:
                        match_name, score, _ = match_tuple
                    
                    # Look up taxon info using our lookup dictionary
                    if match_name in coldp_lookup:
                        for taxon_info in coldp_lookup[match_name]:
                            matches_with_taxon_info.append((match_name, score, taxon_info))
                
                # Resolve homonyms and get the best match
                match_result = resolve_homonyms_lightweight(row, matches_with_taxon_info, ancestor_data, ancestor_map)
                if match_result:
                    fuzzy_matches.append(match_result)
    
    elapsed = time.time() - start_time
#|LN|400|
    logger.info(f"Batch {batch_id} completed: Processed {batch_size} taxa in {elapsed:.2f}s")
    
    return fuzzy_matches

def perform_mapping_parallel(session, num_processes=None, fuzzy_match=True, fuzzy_threshold=90):
    """
    Perform the mapping process with parallel processing for the fuzzy matching.
    """
    if num_processes is None:
        num_processes = multiprocessing.cpu_count() - 1  # Leave one core free
        
    logger.info(f"Will use {num_processes} parallel processes for fuzzy matching")
    
    logger.info("Starting iNaturalist to ColDP taxon mapping process...")

    # 0. Clear existing mapping data
    logger.info("Clearing existing data from 'inat_to_coldp_taxon_map'...")
    session.query(InatToColdpMap).delete(synchronize_session=False)
    session.commit()

    # 1. Load iNat taxa data from expanded_taxa
    logger.info("Loading iNaturalist taxa from 'expanded_taxa' table...")
    inat_taxa_df = pd.read_sql_query(
        session.query(ExpandedTaxa.taxonID, ExpandedTaxa.name, ExpandedTaxa.rank)
               .filter(ExpandedTaxa.taxonActive == True) # Only map active iNat taxa
#|LN|425|
               .statement,
        session.bind
    )
    inat_taxa_df.rename(columns={'taxonID': 'inat_taxon_id', 'name': 'inat_scientific_name', 'rank': 'inat_rank'}, inplace=True)
    inat_taxa_df['norm_inat_name'] = inat_taxa_df['inat_scientific_name'].apply(normalize_name)
    inat_taxa_df['norm_inat_rank'] = inat_taxa_df['inat_rank'].apply(normalize_name)
    logger.info(f"Loaded {len(inat_taxa_df)} active iNaturalist taxa.")

    # 2. Load ColDP NameUsage data (from coldp_name_usage_staging)
    logger.info("Loading ColDP NameUsage data from 'coldp_name_usage_staging' table...")
    
    # For fuzzy matching, we need to load more columns to help with ancestor comparison
    if fuzzy_match:
        coldp_names_df = pd.read_sql_query(
            session.query(
                ColdpNameUsage.ID, 
                ColdpNameUsage.scientificName, 
                ColdpNameUsage.rank, 
                ColdpNameUsage.status,
                ColdpNameUsage.genericName,
                ColdpNameUsage.specificEpithet,
                ColdpNameUsage.family,
                ColdpNameUsage.order,
                ColdpNameUsage.class_,
                ColdpNameUsage.phylum
#|LN|450|
            )
            .statement, 
            session.bind
        )
    else:
        coldp_names_df = pd.read_sql_query(
            session.query(ColdpNameUsage.ID, ColdpNameUsage.scientificName, ColdpNameUsage.rank, ColdpNameUsage.status)
                   .statement,
            session.bind
        )
    
    coldp_names_df.rename(columns={'ID': 'col_taxon_id', 'scientificName': 'col_scientific_name', 
                          'rank': 'col_rank', 'status': 'col_status'}, inplace=True)
    coldp_names_df['norm_col_name'] = coldp_names_df['col_scientific_name'].apply(normalize_name)
    coldp_names_df['norm_col_rank'] = coldp_names_df['col_rank'].apply(normalize_name)
    logger.info(f"Loaded {len(coldp_names_df)} ColDP NameUsage entries.")

    all_mappings = []

    # --- Step 3: Exact Match (Name + Rank), prioritize 'accepted' ColDP status ---
    logger.info("Attempting exact match on scientific name and rank...")
    merged_exact_rank = pd.merge(
        inat_taxa_df,
        coldp_names_df,
        left_on=['norm_inat_name', 'norm_inat_rank'],
#|LN|475|
        right_on=['norm_col_name', 'norm_col_rank'],
        how='inner'
    )
    # Prioritize 'accepted' status
    merged_exact_rank.sort_values(by=['inat_taxon_id', 'col_status'], ascending=[True, True], inplace=True) # 'accepted' often comes first alphabetically
    merged_exact_rank_unique = merged_exact_rank.drop_duplicates(subset=['inat_taxon_id'], keep='first')

    for _, row in merged_exact_rank_unique.iterrows():
        all_mappings.append({
            'inat_taxon_id': row['inat_taxon_id'],
            'col_taxon_id': row['col_taxon_id'],
            'match_type': 'exact_name_rank_accepted' if row['col_status'] == 'accepted' else 'exact_name_rank_other_status',
            'match_score': 1.0,
            'inat_scientific_name': row['inat_scientific_name'],
            'col_scientific_name': row['col_scientific_name']
        })
    logger.info(f"Found {len(merged_exact_rank_unique)} matches on name and rank.")
    
    # Update iNat taxa df to exclude matched items
    inat_taxa_df = inat_taxa_df[~inat_taxa_df['inat_taxon_id'].isin(merged_exact_rank_unique['inat_taxon_id'])]

    # --- Step 4: Exact Match (Name only), prioritize 'accepted' ---
    if not inat_taxa_df.empty:
        logger.info(f"Attempting exact match on scientific name only for {len(inat_taxa_df)} remaining iNat taxa...")
        merged_exact_name_only = pd.merge(
#|LN|500|
            inat_taxa_df,
            coldp_names_df, # Could filter coldp_names_df for status='accepted' first for efficiency
            left_on=['norm_inat_name'],
            right_on=['norm_col_name'],
            how='inner'
        )
        merged_exact_name_only.sort_values(by=['inat_taxon_id', 'col_status'], ascending=[True, True], inplace=True)
        merged_exact_name_only_unique = merged_exact_name_only.drop_duplicates(subset=['inat_taxon_id'], keep='first')

        for _, row in merged_exact_name_only_unique.iterrows():
             all_mappings.append({
                'inat_taxon_id': row['inat_taxon_id'],
                'col_taxon_id': row['col_taxon_id'],
                'match_type': 'exact_name_only_accepted' if row['col_status'] == 'accepted' else 'exact_name_only_other_status',
                'match_score': 0.95, # Slightly lower score than name+rank
                'inat_scientific_name': row['inat_scientific_name'],
                'col_scientific_name': row['col_scientific_name']
            })
        logger.info(f"Found {len(merged_exact_name_only_unique)} matches on name only.")
        inat_taxa_df = inat_taxa_df[~inat_taxa_df['inat_taxon_id'].isin(merged_exact_name_only_unique['inat_taxon_id'])]

    # --- Step 5: Fuzzy Match ---
    fuzzy_match_count = 0
    if fuzzy_match and not inat_taxa_df.empty:
        logger.info(f"Attempting fuzzy matching for {len(inat_taxa_df)} remaining iNat taxa...")
#|LN|525|
        
        # Prepare for homonym resolution: get ancestor information
        ancestor_data = get_taxon_ancestor_info(session, inat_taxa_df)
        logger.info(f"Retrieved ancestor data for {len(ancestor_data)} unmatched taxa")
        
        # Build a map of ColDP taxon IDs to their ancestor information
        ancestor_map = build_col_ancestors_map(coldp_names_df)
        logger.info(f"Built ancestor map for {len(ancestor_map)} ColDP taxa")
        
        # Filter out null/None values before fuzzy matching
        inat_taxa_filtered = inat_taxa_df[inat_taxa_df['norm_inat_name'].notna()]
        
        # Use a conservative batch size to manage memory usage
        batch_size = 1000
        parallel_batch_size = batch_size  # Keep batch size at 1000 to prevent OOM
        
        # Create lightweight batch data - only pass essential data to reduce memory copying
        # Extract just the normalized names and IDs to minimize data transfer
        accepted_coldp_names = coldp_names_df[coldp_names_df['col_status'] == 'accepted']['norm_col_name'].dropna().tolist()
        all_coldp_names = coldp_names_df['norm_col_name'].dropna().tolist()
        
        # Create lookup dictionaries for faster access (instead of copying full DataFrames)
        coldp_lookup = {}
        for _, row in coldp_names_df.iterrows():
            norm_name = row['norm_col_name']
#|LN|550|
            if pd.notna(norm_name):
                if norm_name not in coldp_lookup:
                    coldp_lookup[norm_name] = []
                coldp_lookup[norm_name].append({
                    'col_taxon_id': row['col_taxon_id'],
                    'col_scientific_name': row['col_scientific_name'],
                    'col_status': row['col_status']
                })
        
        # Split the data into batches for parallel processing
        all_batches = []
        for start_idx in range(0, len(inat_taxa_filtered), parallel_batch_size):
            end_idx = min(start_idx + parallel_batch_size, len(inat_taxa_filtered))
            batch = inat_taxa_filtered.iloc[start_idx:end_idx].copy()
            
            batch_data = {
                'batch': batch,
                'accepted_coldp_names': accepted_coldp_names,
                'all_coldp_names': all_coldp_names,
                'coldp_lookup': coldp_lookup,
                'ancestor_data': ancestor_data,
                'ancestor_map': ancestor_map,
                'batch_id': len(all_batches) + 1
            }
            all_batches.append(batch_data)
#|LN|575|
            
        logger.info(f"Split data into {len(all_batches)} large batches for parallel processing")
        
        # Create a multiprocessing pool
        with multiprocessing.Pool(processes=num_processes) as pool:
            process_func = partial(process_fuzzy_batch, fuzzy_threshold=fuzzy_threshold)
            
            # Process batches in parallel
            logger.info(f"Processing {len(all_batches)} batches with {num_processes} workers...")
            results = pool.map(process_func, all_batches)
            
            # Combine results
            fuzzy_matches = []
            for batch_results in results:
                fuzzy_matches.extend(batch_results)
            
            fuzzy_match_count = len(fuzzy_matches)
            all_mappings.extend(fuzzy_matches)
            logger.info(f"Found {fuzzy_match_count} fuzzy matches across all batches.")
    
    # Calculate how many unmatched taxa remain
    total_exact_matches = len(all_mappings) - fuzzy_match_count
    total_unmatched = len(inat_taxa_df) - fuzzy_match_count
    logger.info(f"Summary: {total_exact_matches} exact matches, {fuzzy_match_count} fuzzy matches, {total_unmatched} remaining unmatched taxa.")
    
#|LN|600|
    # Save match statistics by type
    match_types = {}
    for mapping in all_mappings:
        match_type = mapping['match_type']
        if match_type not in match_types:
            match_types[match_type] = 0
        match_types[match_type] += 1
    
    logger.info("Match statistics by type:")
    for match_type, count in match_types.items():
        logger.info(f"  {match_type}: {count}")

    # --- Step 6: Persist mappings ---
    if all_mappings:
        logger.info(f"Bulk inserting {len(all_mappings)} mappings into 'inat_to_coldp_taxon_map'...")
        # Insert in chunks to avoid memory issues
        chunk_size = 10000
        for i in range(0, len(all_mappings), chunk_size):
            chunk = all_mappings[i:i+chunk_size]
            session.bulk_insert_mappings(InatToColdpMap, chunk)
            session.commit()
            logger.info(f"Inserted chunk {i//chunk_size + 1}/{(len(all_mappings) + chunk_size - 1)//chunk_size}")
        
        logger.info("Successfully populated 'inat_to_coldp_taxon_map'.")
    else:
#|LN|625|
        logger.info("No mappings found to insert.")


def main():
    parser = argparse.ArgumentParser(description="Map iNaturalist taxa to ColDP taxa using parallel processing.")
    parser.add_argument("--db-user", default=os.getenv("DB_USER", "postgres"))
    parser.add_argument("--db-password", default=os.getenv("DB_PASSWORD", "password"))
    parser.add_argument("--db-host", default=os.getenv("DB_HOST", "localhost"))
    parser.add_argument("--db-port", default=os.getenv("DB_PORT", "5432"))
    parser.add_argument("--db-name", default=os.getenv("DB_NAME", "ibrida-v0-r1"))
    parser.add_argument("--fuzzy-match", action="store_true", help="Enable fuzzy matching for unmatched taxa")
    parser.add_argument("--fuzzy-threshold", type=int, default=90, help="Threshold score (0-100) for fuzzy matching")
    parser.add_argument("--processes", type=int, default=None, help="Number of parallel processes to use (default: CPU count - 1)")
    parser.add_argument("--debug", action="store_true", help="Enable debug output")
    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    engine = get_db_engine(args.db_user, args.db_password, args.db_host, args.db_port, args.db_name)
    create_crosswalk_table(engine) # Ensure table exists

    Session = sessionmaker(bind=engine)
    session = Session()

#|LN|650|
    try:
        perform_mapping_parallel(
            session, 
            num_processes=args.processes, 
            fuzzy_match=args.fuzzy_match, 
            fuzzy_threshold=args.fuzzy_threshold
        )
    except Exception as e:
        logger.error(f"An error occurred during the mapping process: {e}")
        session.rollback()
    finally:
        session.close()

if __name__ == "__main__":
    main()
        </file>
        <file path="scripts/ingest_coldp/populate_common_names.py">
#!/usr/bin/env python3

import argparse
import os
import logging
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# Import models from the top-level models directory
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from models.expanded_taxa import ExpandedTaxa  # ORM for expanded_taxa

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

TARGET_TABLE_NAME = "expanded_taxa" # The table created by expand_taxa.sh

def get_db_engine(db_user, db_password, db_host, db_port, db_name):
    connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    return create_engine(connection_string)

def clear_existing_common_names(session):
    logger.info(f"Clearing existing common name data from '{TARGET_TABLE_NAME}'...")
    
    # Base commonName
    stmt_base = text(f"""UPDATE "{TARGET_TABLE_NAME}" SET "commonName" = NULL;""")
    session.execute(stmt_base)

    # Ancestral commonNames
    ancestral_ranks = [5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30, 32, 33, '33_5', 34, '34_5', 35, 37, 40, 43, 44, 45, 47, 50, 53, 57, 60, 67, 70]
    for level in ancestral_ranks:
        safe_level = str(level).replace('.', '_')
        lxx_common_name_col = f"L{safe_level}_commonName"
        stmt_ancestor = text(f"""UPDATE "{TARGET_TABLE_NAME}" SET "{lxx_common_name_col}" = NULL;""")
        session.execute(stmt_ancestor)
    
    session.commit()
    logger.info("Finished clearing existing common names.")


def populate_common_names_in_expanded_taxa(session):
    logger.info(f"Populating 'commonName' in '{TARGET_TABLE_NAME}'...")
    
    # SQL to update the main commonName for each taxon in expanded_taxa
    # It joins expanded_taxa (aliased as et) with the crosswalk (xmap),
    # then with coldp_vernacular_name (cvn)
    # It selects the preferred English common name.
    update_main_common_name_sql = text(f"""
    UPDATE "{TARGET_TABLE_NAME}" AS et
    SET "commonName" = cvn.name
    FROM inat_to_coldp_taxon_map AS xmap
    JOIN coldp_vernacular_name AS cvn ON xmap.col_taxon_id = cvn."taxonID"
    WHERE et."taxonID" = xmap.inat_taxon_id
      AND cvn.language = 'eng' 
      AND (cvn.preferred = TRUE OR cvn.preferred IS NULL);
    """)
    
    result = session.execute(update_main_common_name_sql)
    session.commit()
    logger.info(f"Populated 'commonName' for {result.rowcount} direct taxa in '{TARGET_TABLE_NAME}'.")

    logger.info(f"Populating ancestral 'LXX_commonName' fields in '{TARGET_TABLE_NAME}'...")
    ancestral_ranks = [5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30, 32, 33, '33_5', 34, '34_5', 35, 37, 40, 43, 44, 45, 47, 50, 53, 57, 60, 67, 70]
    
    total_ancestor_updates = 0
    for level in ancestral_ranks:
        safe_level = str(level).replace('.', '_')
        lxx_taxon_id_col = f"L{safe_level}_taxonID"
        lxx_common_name_col = f"L{safe_level}_commonName"

        update_ancestor_common_name_sql = text(f"""
        UPDATE "{TARGET_TABLE_NAME}" AS et
        SET "{lxx_common_name_col}" = cvn.name
        FROM inat_to_coldp_taxon_map AS xmap
        JOIN coldp_vernacular_name AS cvn ON xmap.col_taxon_id = cvn."taxonID"
        WHERE et."{lxx_taxon_id_col}" = xmap.inat_taxon_id
          AND cvn.language = 'eng'
          AND (cvn.preferred = TRUE OR cvn.preferred IS NULL)
          AND et."{lxx_taxon_id_col}" IS NOT NULL; 
        """)
        # The 'et."{lxx_taxon_id_col}" IS NOT NULL' is important as that column can be NULL.
        
        logger.debug(f"Executing update for {lxx_common_name_col}...")
        result_ancestor = session.execute(update_ancestor_common_name_sql)
        session.commit() # Commit after each rank level for large tables
        logger.info(f"Populated '{lxx_common_name_col}' for {result_ancestor.rowcount} ancestral links.")
        total_ancestor_updates += result_ancestor.rowcount
        
    logger.info(f"Finished populating ancestral common names. Total updates made: {total_ancestor_updates}.")


def main():
    parser = argparse.ArgumentParser(description="Populate common names in the expanded_taxa table.")
    parser.add_argument("--db-user", default=os.getenv("DB_USER", "postgres"))
    parser.add_argument("--db-password", default=os.getenv("DB_PASSWORD", "password"))
    parser.add_argument("--db-host", default=os.getenv("DB_HOST", "localhost"))
    parser.add_argument("--db-port", default=os.getenv("DB_PORT", "5432"))
    parser.add_argument("--db-name", default=os.getenv("DB_NAME", "ibrida-v0-r1"))
    parser.add_argument("--clear-first", action="store_true", help="Clear existing common names before populating.")

    args = parser.parse_args()

    engine = get_db_engine(args.db_user, args.db_password, args.db_host, args.db_port, args.db_name)
    Session = sessionmaker(bind=engine)
    session = Session()

    try:
        if args.clear_first:
            clear_existing_common_names(session)
        populate_common_names_in_expanded_taxa(session)
        logger.info("Common name population process completed successfully.")
    except Exception as e:
        logger.error(f"An error occurred: {e}")
        session.rollback()
    finally:
        session.close()

if __name__ == "__main__":
    main()

        </file>
        <file path="scripts/ingest_coldp/snips.txt">
```bash
chmod +x /home/caleb/repo/ibridaDB/scripts/ingest_coldp/wrapper_ingest_coldp_parallel.sh
NUM_PROCESSES=12 ./scripts/ingest_coldp/wrapper_ingest_coldp_parallel.sh
```
        </file>
        <file path="scripts/ingest_coldp/wrapper_ingest_coldp.sh">
#!/bin/bash
# ibridaDB/scripts/ingest_coldp/wrapper_ingest_coldp.sh

# This wrapper orchestrates the ingestion of Catalogue of Life Data Package (ColDP)
# data into the ibridaDB. It loads raw ColDP tables, maps iNaturalist taxon IDs
# to ColDP taxon IDs, and then populates common name fields in the expanded_taxa table.

set -euo pipefail # Exit on error, undefined variable, or pipe failure

SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/wrapper_ingest_coldp_$(date +%Y%m%d_%H%M%S).log"

# --- Configuration ---
# These can be overridden by environment variables if needed
DB_USER="${DB_USER:-postgres}"
DB_PASSWORD="${DB_PASSWORD:-ooglyboogly69}" # Hardcoded password for private repo
DB_HOST="${DB_HOST:-localhost}"
DB_PORT="${DB_PORT:-5432}"
DB_NAME="${DB_NAME:-ibrida-v0-r1}" # Target ibridaDB database

COLDP_DATA_DIR="${COLDP_DATA_DIR:-/datasets/taxa/catalogue_of_life/2024/ColDP}" # Path to unzipped ColDP files

PYTHON_EXE="${PYTHON_EXE:-/home/caleb/repo/ibridaDB/.venv/bin/python}" # Use venv interpreter

# Enable/disable fuzzy matching (default: enabled)
ENABLE_FUZZY_MATCH="${ENABLE_FUZZY_MATCH:-true}"
FUZZY_THRESHOLD="${FUZZY_THRESHOLD:-90}" # Match threshold (0-100)

# Enable/disable steps (for debugging or incremental runs)
RUN_LOAD_TABLES="${RUN_LOAD_TABLES:-true}"
RUN_MAP_TAXA="${RUN_MAP_TAXA:-true}"
RUN_POPULATE_COMMON_NAMES="${RUN_POPULATE_COMMON_NAMES:-true}"

# --- Logging ---
exec > >(tee -a "${LOG_FILE}") 2>&1
echo "Starting ColDP Ingestion Wrapper at $(date)"
echo "--------------------------------------------------"
echo "Configuration:"
echo "  DB User: ${DB_USER}"
echo "  DB Host: ${DB_HOST}"
echo "  DB Port: ${DB_PORT}"
echo "  DB Name: ${DB_NAME}"
echo "  ColDP Data Dir: ${COLDP_DATA_DIR}"
echo "  Python Executable: ${PYTHON_EXE}"
echo "  Enable Fuzzy Match: ${ENABLE_FUZZY_MATCH}"
echo "  Fuzzy Threshold: ${FUZZY_THRESHOLD}"
echo "  Log File: ${LOG_FILE}"
echo "--------------------------------------------------"
echo "Steps Configuration:"
echo "  Load Tables: ${RUN_LOAD_TABLES}"
echo "  Map Taxa: ${RUN_MAP_TAXA}"
echo "  Populate Common Names: ${RUN_POPULATE_COMMON_NAMES}"
echo "--------------------------------------------------"

# --- Helper function to run Python scripts ---
run_python_script() {
    local script_name="$1"
    shift  # Remove first argument (script_name) so $@ contains only the remaining args
    local script_path="${SCRIPT_DIR}/${script_name}.py"
    
    echo ""
    echo ">>> Running ${script_name}.py..."
    if [ ! -f "${script_path}" ]; then
        echo "ERROR: Python script not found: ${script_path}"
        exit 1
    fi

    # Pass database connection details as arguments
    "${PYTHON_EXE}" "${script_path}" \
        --db-user "${DB_USER}" \
        --db-password "${DB_PASSWORD}" \
        --db-host "${DB_HOST}" \
        --db-port "${DB_PORT}" \
        --db-name "${DB_NAME}" \
        "$@" # Pass through any additional arguments for the specific script
    
    if [ $? -ne 0 ]; then
        echo "ERROR: ${script_name}.py failed. Check logs above."
        exit 1
    fi
    echo ">>> Finished ${script_name}.py successfully."
}

# --- Main Orchestration ---

# Step 1: Load raw ColDP tables
# The load_tables.py script handles table creation and data loading from TSVs.
if [ "${RUN_LOAD_TABLES}" = "true" ]; then
    run_python_script "load_tables" --coldp-dir "${COLDP_DATA_DIR}"
else
    echo "Skipping load_tables.py (RUN_LOAD_TABLES=${RUN_LOAD_TABLES})"
fi

# Step 2: Map iNaturalist taxon IDs to ColDP taxon IDs
# This creates and populates the 'inat_to_coldp_taxon_map' table.
if [ "${RUN_MAP_TAXA}" = "true" ]; then
    # Build the arguments list based on whether fuzzy matching is enabled
    MAP_TAXA_ARGS=()
    if [ "${ENABLE_FUZZY_MATCH}" = "true" ]; then
        MAP_TAXA_ARGS+=("--fuzzy-match" "--fuzzy-threshold" "${FUZZY_THRESHOLD}")
    fi
    
    run_python_script "map_taxa" "${MAP_TAXA_ARGS[@]}"
else
    echo "Skipping map_taxa.py (RUN_MAP_TAXA=${RUN_MAP_TAXA})"
fi

# Step 3: Populate common names in the expanded_taxa table
# This uses the mapping table and vernacular names to update expanded_taxa.
if [ "${RUN_POPULATE_COMMON_NAMES}" = "true" ]; then
    run_python_script "populate_common_names" --clear-first
else
    echo "Skipping populate_common_names.py (RUN_POPULATE_COMMON_NAMES=${RUN_POPULATE_COMMON_NAMES})"
fi

echo "--------------------------------------------------"
echo "ColDP Ingestion Wrapper finished successfully at $(date)."
echo "--------------------------------------------------"

exit 0
        </file>
        <file path="scripts/ingest_coldp/wrapper_ingest_coldp_parallel.sh" line_interval="25">
#!/bin/bash

# Get the directory of this script
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

# Default settings
ENABLE_FUZZY_MATCH=${ENABLE_FUZZY_MATCH:-true}
FUZZY_THRESHOLD=${FUZZY_THRESHOLD:-90}
NUM_PROCESSES=${NUM_PROCESSES:-12}  # Use 12 processes by default
TIMESTAMP=$(date "+%Y%m%d_%H%M%S")
LOG_FILE="${SCRIPT_DIR}/wrapper_ingest_coldp_parallel_${TIMESTAMP}.log"
PYTHON_EXECUTABLE=${PYTHON_EXECUTABLE:-"${SCRIPT_DIR}/../../.venv/bin/python"}
COLDP_DIR=${COLDP_DIR:-"/datasets/taxa/catalogue_of_life/2024/ColDP"}

# Database config
DB_USER=${DB_USER:-"postgres"}
DB_PASSWORD=${DB_PASSWORD:-"ooglyboogly69"}
DB_HOST=${DB_HOST:-"localhost"}
DB_PORT=${DB_PORT:-"5432"}
DB_NAME=${DB_NAME:-"ibrida-v0-r1"}

# Step flags
DO_LOAD_TABLES=${DO_LOAD_TABLES:-true}
DO_MAP_TAXA=${DO_MAP_TAXA:-true}
#|LN|25|
DO_POPULATE_COMMON_NAMES=${DO_POPULATE_COMMON_NAMES:-true}

# Create a function to log messages
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# Start the log file
echo "Starting ColDP Ingestion Parallel Wrapper at $(date)" | tee "$LOG_FILE"
echo "--------------------------------------------------" | tee -a "$LOG_FILE"
echo "Configuration:" | tee -a "$LOG_FILE"
echo "  DB User: $DB_USER" | tee -a "$LOG_FILE"
echo "  DB Host: $DB_HOST" | tee -a "$LOG_FILE"
echo "  DB Port: $DB_PORT" | tee -a "$LOG_FILE"
echo "  DB Name: $DB_NAME" | tee -a "$LOG_FILE"
echo "  ColDP Data Dir: $COLDP_DIR" | tee -a "$LOG_FILE"
echo "  Python Executable: $PYTHON_EXECUTABLE" | tee -a "$LOG_FILE"
echo "  Enable Fuzzy Match: $ENABLE_FUZZY_MATCH" | tee -a "$LOG_FILE"
echo "  Fuzzy Threshold: $FUZZY_THRESHOLD" | tee -a "$LOG_FILE"
echo "  Number of Processes: $NUM_PROCESSES" | tee -a "$LOG_FILE"
echo "  Log File: $LOG_FILE" | tee -a "$LOG_FILE"
echo "--------------------------------------------------" | tee -a "$LOG_FILE"
echo "Steps Configuration:" | tee -a "$LOG_FILE"
echo "  Load Tables: $DO_LOAD_TABLES" | tee -a "$LOG_FILE"
echo "  Map Taxa: $DO_MAP_TAXA" | tee -a "$LOG_FILE"
#|LN|50|
echo "  Populate Common Names: $DO_POPULATE_COMMON_NAMES" | tee -a "$LOG_FILE"
echo "--------------------------------------------------" | tee -a "$LOG_FILE"
echo "" | tee -a "$LOG_FILE"

# Clean up the ColDP tables before loading (but NOT expanded_taxa)
if [[ "$DO_LOAD_TABLES" == "true" ]]; then
    log "Dropping ColDP tables to ensure correct schema..."
    
    # Use Docker to run psql commands since we're running the database in a container
    docker exec ibridaDB psql -U "$DB_USER" -d "$DB_NAME" -c "
    -- Drop all ColDP tables (but NOT expanded_taxa) so they can be recreated with the correct schema
    DROP TABLE IF EXISTS coldp_vernacular_name, coldp_distribution, coldp_media, 
                          coldp_reference, coldp_type_material, coldp_name_usage_staging CASCADE;
    " 2>&1 | tee -a "$LOG_FILE"
    
    if [ $? -ne 0 ]; then
        log "Failed to drop ColDP tables"
        exit 1
    else
        log "Successfully dropped ColDP tables - they will be recreated with the correct schema"
    fi
fi

# Step 1: Load ColDP tables
if [[ "$DO_LOAD_TABLES" == "true" ]]; then
#|LN|75|
    log ">>> Running load_tables.py..."
    
    # Run the loading script
    "$PYTHON_EXECUTABLE" "${SCRIPT_DIR}/load_tables.py" \
        --coldp-dir="$COLDP_DIR" \
        --db-user="$DB_USER" \
        --db-password="$DB_PASSWORD" \
        --db-host="$DB_HOST" \
        --db-port="$DB_PORT" \
        --db-name="$DB_NAME" \
        2>&1 | tee -a "$LOG_FILE"
    
    SCRIPT_EXIT_CODE=${PIPESTATUS[0]}
    if [ $SCRIPT_EXIT_CODE -eq 0 ]; then
        log ">>> Finished load_tables.py successfully."
    else
        log ">>> load_tables.py failed with exit code $SCRIPT_EXIT_CODE."
        exit 1
    fi
fi

# Step 2: Map taxa (with parallelization)
if [[ "$DO_MAP_TAXA" == "true" ]]; then
    log ">>> Running map_taxa_parallel.py..."
    
#|LN|100|
    FUZZY_ARGS=""
    if [[ "$ENABLE_FUZZY_MATCH" == "true" ]]; then
        FUZZY_ARGS="--fuzzy-match --fuzzy-threshold=$FUZZY_THRESHOLD"
    fi
    
    # Run the parallel mapping script
    "$PYTHON_EXECUTABLE" "${SCRIPT_DIR}/map_taxa_parallel.py" \
        --db-user="$DB_USER" \
        --db-password="$DB_PASSWORD" \
        --db-host="$DB_HOST" \
        --db-port="$DB_PORT" \
        --db-name="$DB_NAME" \
        --processes="$NUM_PROCESSES" \
        $FUZZY_ARGS \
        2>&1 | tee -a "$LOG_FILE"
    
    SCRIPT_EXIT_CODE=${PIPESTATUS[0]}
    if [ $SCRIPT_EXIT_CODE -eq 0 ]; then
        log ">>> Finished map_taxa_parallel.py successfully."
    else
        log ">>> map_taxa_parallel.py failed with exit code $SCRIPT_EXIT_CODE."
        exit 1
    fi
fi

#|LN|125|
# Step 3: Populate common names
if [[ "$DO_POPULATE_COMMON_NAMES" == "true" ]]; then
    log ">>> Running populate_common_names.py..."
    
    # Run the common names population script
    "$PYTHON_EXECUTABLE" "${SCRIPT_DIR}/populate_common_names.py" \
        --db-user="$DB_USER" \
        --db-password="$DB_PASSWORD" \
        --db-host="$DB_HOST" \
        --db-port="$DB_PORT" \
        --db-name="$DB_NAME" \
        --clear-first \
        2>&1 | tee -a "$LOG_FILE"
    
    SCRIPT_EXIT_CODE=${PIPESTATUS[0]}
    if [ $SCRIPT_EXIT_CODE -eq 0 ]; then
        log ">>> Finished populate_common_names.py successfully."
    else
        log ">>> populate_common_names.py failed with exit code $SCRIPT_EXIT_CODE."
        exit 1
    fi
fi

log "All steps completed successfully!"
echo "=========================================================================================" | tee -a "$LOG_FILE"
#|LN|150|
log "ColDP integration process completed. Log file: $LOG_FILE"
        </file>
      </dir>
    </dir>
  </files>
</codebase_context>