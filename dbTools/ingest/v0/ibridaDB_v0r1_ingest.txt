<codebase_context>

<dirtree: /home/caleb/repo/ibridaDB/dbTools/ingest/v0>
|-- common (420 lines)
|   |-- functions.sh (46)
|   |-- geom.sh (50)
|   |-- main.sh (172)
|   \-- vers_origin.sh (152)
|-- r1 (102)
|   |-- structure.sql (49)
|   \-- wrapper.sh (53)
|-- readme.md (100)
\-- utils (61)
    |-- add_release.sh (55)
    \-- elevation (6)
        |-- create_elevation_table.sh (1)
        |-- create_elevation_table.sql (1)
        |-- load_dem.sh (1)
        |-- main.sh (1)
        |-- update_elevation.sh (1)
        \-- wrapper.sh (1)
</dirtree: /home/caleb/repo/ibridaDB/dbTools/ingest/v0>

<file: readme.md>
```markdown
dbTools/ingest/v0/
├── common/
│   ├── geom.sh           # Geometry calculations
│   ├── vers_origin.sh    # Version/origin updates
│   └── main.sh           # Core ingestion logic
├── r0/
│   └── wrapper.sh        # r0-specific parameters
└── r1/
    └── wrapper.sh        # r1-specific parameters
```

# ibrida Database Reproduction Guide

## Overview
This guide documents the step-by-step process for reproducing the ibrida database from iNaturalist open data dumps. The database uses a versioning system with two components:
- **Version (v#)**: Indicates structural changes to the database
- **Release (r#)**: Indicates different data dumps using the same structure

Current versions:
- v0r0: June 2024 iNat data release
- v0r1: December 2024 iNat data release (adds anomaly_score column to observations table)

## Directory Structure
```
dbTools/ingest/v0/
├── common/                # Shared scripts
│   ├── geom.sh           # Geometry calculations
│   ├── vers_origin.sh    # Version/origin updates
│   └── main.sh           # Core ingestion logic
├── r0/
│   └── wrapper.sh        # June 2024 release parameters
└── r1/
    └── wrapper.sh        # December 2024 release parameters
```

## Database Initialization and Data Ingestion
The initialization and ingestion process uses a modular system with wrapper scripts for version-specific parameters and common scripts for shared logic.

### Setup Release-Specific Parameters
Each release has its own wrapper script that defines:
- Database name (e.g., `ibrida-v0r1`)
- Source information
- Version and release values
- Input/output paths

### Running the Ingestion Process
```bash
# Make scripts executable
chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/main.sh
chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/geom.sh
chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/vers_origin.sh
chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh

# Run ingest process for latest release
/home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
```

### Ingestion Process Steps
1. `wrapper.sh` sets release-specific parameters
2. `main.sh` executes the core ingestion logic:
   - Creates the database
   - Sets up tables and indexes
   - Imports data from CSV files
   - Adds and calculates geometry columns (via `geom.sh`)
   - Sets version, release, and origin information (via `vers_origin.sh`)
3. Parallel processing is used for geometry calculations and metadata updates

### Database Schema
Each table includes these metadata columns:
- `version`: Database structure version (e.g., "v0")
- `release`: Data release identifier (e.g., "r0", "r1")
- `origin`: Source and date of the data (e.g., "iNat-Dec2024")

### Important Indices
Core indices:
- Primary key indices on all tables
- Geospatial index on observations (`observations_geom`)
- Foreign key indices for joins
- Full-text search indices for metadata columns
- Composite index for version/release queries (`idx_obs_version_release`)

## Adding a New Release
To add a new release:

1. Create a new release directory and wrapper script:
```bash
mkdir -p /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r{N}
cp /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r{N}/
```

2. Update parameters in the new wrapper script:
- SOURCE
- RELEASE_VALUE
- Other release-specific paths/values

3. Run the ingestion process as described above

## Export Process
[To be added as we implement the export steps...]
</file: readme.md>

<file: common/main.sh>
#!/bin/bash

# This script expects the following variables to be set by the wrapper:
# - DB_USER
# - DB_TEMPLATE
# - NUM_PROCESSES
# - BASE_DIR
# - SOURCE
# - ORIGIN_VALUE
# - VERSION_VALUE
# - RELEASE_VALUE
# - DB_NAME
# - DB_CONTAINER
# - METADATA_PATH
# - STRUCTURE_SQL

# Validate required variables
required_vars=(
    "DB_USER" "DB_TEMPLATE" "NUM_PROCESSES" "BASE_DIR" 
    "SOURCE" "ORIGIN_VALUE" "VERSION_VALUE" "DB_NAME" 
    "DB_CONTAINER" "METADATA_PATH" "STRUCTURE_SQL"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to execute SQL commands on default postgres database
execute_sql_postgres() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

# Create database
print_progress "Creating database"
execute_sql_postgres "DROP DATABASE IF EXISTS \"$DB_NAME\";"
execute_sql_postgres "CREATE DATABASE \"$DB_NAME\" WITH TEMPLATE $DB_TEMPLATE OWNER $DB_USER;"

# Create tables from structure file
print_progress "Creating tables"
cat "${STRUCTURE_SQL}" | docker exec -i ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}"

# Import data
print_progress "Importing data"
execute_sql "
BEGIN;

COPY observations FROM '${METADATA_PATH}/observations.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY photos FROM '${METADATA_PATH}/photos.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY taxa FROM '${METADATA_PATH}/taxa.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY observers FROM '${METADATA_PATH}/observers.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;

COMMIT;
"

# Create indexes
print_progress "Creating indexes"
execute_sql "
BEGIN;

CREATE INDEX index_photos_photo_uuid ON photos USING btree (photo_uuid);
CREATE INDEX index_photos_observation_uuid ON photos USING btree (observation_uuid);
CREATE INDEX index_photos_position ON photos USING btree (position);
CREATE INDEX index_photos_photo_id ON photos USING btree (photo_id);
CREATE INDEX index_taxa_taxon_id ON taxa USING btree (taxon_id);
CREATE INDEX index_observers_observers_id ON observers USING btree (observer_id);
CREATE INDEX index_observations_observer_id ON observations USING btree (observer_id);
CREATE INDEX index_observations_quality ON observations USING btree (quality_grade);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);
CREATE INDEX index_taxa_active ON taxa USING btree (active);

COMMIT;
"

# Create conditional index for anomaly_score if it exists
execute_sql "
DO \$\$
BEGIN
    IF EXISTS (
        SELECT 1 
        FROM information_schema.columns 
        WHERE table_name = 'observations' 
        AND column_name = 'anomaly_score'
    ) THEN
        CREATE INDEX idx_observations_anomaly ON observations (anomaly_score);
    END IF;
END \$\$;"

# Add geom column
print_progress "Adding geom column"
execute_sql "ALTER TABLE observations ADD COLUMN geom public.geometry;"

# Run parallel geom calculations
print_progress "Running parallel geom calculations"
"${BASE_DIR}/common/geom.sh" "$DB_NAME" "observations" "$NUM_PROCESSES" "$BASE_DIR"

# Create geom index
print_progress "Creating geom index"
execute_sql "
BEGIN;
CREATE INDEX observations_geom ON observations USING GIST (geom);
COMMIT;
"

# Vacuum analyze
print_progress "Vacuum analyze"
execute_sql "VACUUM ANALYZE;"

# Add origin, version, and release columns in parallel
print_progress "Adding origin, version, and release columns"
execute_sql "
BEGIN;

ALTER TABLE taxa ADD COLUMN origin VARCHAR(255);
ALTER TABLE observers ADD COLUMN origin VARCHAR(255);
ALTER TABLE observations ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN version VARCHAR(255);
ALTER TABLE observations ADD COLUMN version VARCHAR(255);
ALTER TABLE observers ADD COLUMN version VARCHAR(255);
ALTER TABLE taxa ADD COLUMN version VARCHAR(255);
ALTER TABLE photos ADD COLUMN release VARCHAR(255);
ALTER TABLE observations ADD COLUMN release VARCHAR(255);
ALTER TABLE observers ADD COLUMN release VARCHAR(255);
ALTER TABLE taxa ADD COLUMN release VARCHAR(255);

COMMIT;
"

# Run parallel updates for origin, version, and release columns
print_progress "Running parallel updates for origin, version, and release columns"
"${BASE_DIR}/common/vers_origin.sh" "$DB_NAME" "$NUM_PROCESSES" "$ORIGIN_VALUE" "$VERSION_VALUE" "$RELEASE_VALUE"

# Create indexes for origin, version, and release columns
print_progress "Creating indexes for origin, version, and release columns"
execute_sql "
BEGIN;

CREATE INDEX index_taxa_origins ON taxa USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_taxa_name ON taxa USING GIN (to_tsvector('simple', name));
CREATE INDEX index_observers_origins ON observers USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_observations_origins ON observations USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_origins ON photos USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_version ON photos USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observations_version ON observations USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observers_version ON observers USING GIN (to_tsvector('simple', version));
CREATE INDEX index_taxa_version ON taxa USING GIN (to_tsvector('simple', version));
CREATE INDEX index_photos_release ON photos USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observations_release ON observations USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observers_release ON observers USING GIN (to_tsvector('simple', release));
CREATE INDEX index_taxa_release ON taxa USING GIN (to_tsvector('simple', release));

COMMIT;
"

print_progress "Database setup complete"
</file: common/main.sh>

<file: common/vers_origin.sh>
#!/bin/bash

# COMMENT: populates origin, version, and release columns in parallel
# COMMENT: these are the only columns on the base tables that are not populated by the ingest process

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/vers_origin_$(date +%Y%m%d_%H%M%S).log"
echo "Starting version/origin/release updates at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "${LOG_FILE}"
}

# Function for error logging and exit
error_exit() {
    log_message "ERROR: $1"
    exit 1
}

# Function to run the update in parallel
run_update() {
    local TABLE_NAME=$1
    local COLUMN_NAME=$2
    local VALUE=$3
    local OFFSET=$4
    local LIMIT=$5
    local DB_NAME=$6
    local DB_CONTAINER=$7
    local PROCESS_NUM=$8

    log_message "Process $PROCESS_NUM: Updating $TABLE_NAME.$COLUMN_NAME (offset: $OFFSET, limit: $LIMIT)"
    
    UPDATE_RESULT=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "
    UPDATE ${TABLE_NAME}
    SET ${COLUMN_NAME} = '${VALUE}'
    WHERE ctid IN (
        SELECT ctid
        FROM ${TABLE_NAME}
        ORDER BY ctid
        OFFSET ${OFFSET}
        LIMIT ${LIMIT}
    );")
    
    if [ $? -ne 0 ]; then
        error_exit "Failed to update ${TABLE_NAME}.${COLUMN_NAME} in process $PROCESS_NUM"
    fi
    
    log_message "Process $PROCESS_NUM: Completed update of $TABLE_NAME.$COLUMN_NAME"
}

# Validate arguments
if [ "$#" -ne 5 ]; then
    error_exit "Usage: $0 <database_name> <num_workers> <origin_value> <version_value> <release_value>"
fi

# Define arguments
DB_NAME=$1
NUM_PROCESSES=$2
ORIGIN_VALUE=$3
VERSION_VALUE=$4
RELEASE_VALUE=$5

# Validate NUM_PROCESSES is a positive integer
if ! [[ "$NUM_PROCESSES" =~ ^[1-9][0-9]*$ ]]; then
    error_exit "Number of workers must be a positive integer"
fi

# Use container name from environment or default
DB_CONTAINER=${DB_CONTAINER:-"ibridaDB"}

# Verify database exists
if ! docker exec ${DB_CONTAINER} psql -U postgres -lqt | cut -d \| -f 1 | grep -qw "${DB_NAME}"; then
    error_exit "Database ${DB_NAME} does not exist"
fi

# Tables and their columns to update
declare -A TABLES_COLUMNS
TABLES_COLUMNS=(
    ["taxa"]="origin,version,release"
    ["observers"]="origin,version,release"
    ["observations"]="origin,version,release"
    ["photos"]="origin,version,release"
)

# Function to update columns in parallel
update_columns_in_parallel() {
    local TABLE_NAME=$1
    local COLUMN_NAME=$2
    local VALUE=$3
    local TOTAL_ROWS

    # Verify table exists
    if ! docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -c "\d ${TABLE_NAME}" &>/dev/null; then
        error_exit "Table ${TABLE_NAME} does not exist in database ${DB_NAME}"
    }

    # Get total rows with error handling
    TOTAL_ROWS=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};" | tr -d ' ')
    if [ $? -ne 0 ] || ! [[ "$TOTAL_ROWS" =~ ^[0-9]+$ ]]; then
        error_exit "Failed to get row count for ${TABLE_NAME}"
    }

    log_message "Starting parallel update of ${TABLE_NAME}.${COLUMN_NAME} (${TOTAL_ROWS} total rows)"
    
    local BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES + 1))
    local PIDS=()

    for ((i=0; i<NUM_PROCESSES; i++)); do
        local OFFSET=$((i * BATCH_SIZE))
        run_update ${TABLE_NAME} ${COLUMN_NAME} ${VALUE} ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${DB_CONTAINER} $i &
        PIDS+=($!)
    done

    # Wait for all processes and check their exit status
    for pid in "${PIDS[@]}"; do
        if ! wait $pid; then
            error_exit "One of the parallel update processes failed"
        fi
    done
    
    log_message "Completed update of ${TABLE_NAME}.${COLUMN_NAME}"
}

# Main update process
log_message "Starting updates with parameters:"
log_message "Database: ${DB_NAME}"
log_message "Number of processes: ${NUM_PROCESSES}"
log_message "Origin value: ${ORIGIN_VALUE}"
log_message "Version value: ${VERSION_VALUE}"
log_message "Release value: ${RELEASE_VALUE}"

for TABLE_NAME in "${!TABLES_COLUMNS[@]}"; do
    log_message "Processing table: ${TABLE_NAME}"
    IFS=',' read -ra COLUMNS <<< "${TABLES_COLUMNS[$TABLE_NAME]}"
    for COLUMN in "${COLUMNS[@]}"; do
        case "$COLUMN" in
            "origin")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$ORIGIN_VALUE"
                ;;
            "version")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$VERSION_VALUE"
                ;;
            "release")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$RELEASE_VALUE"
                ;;
        esac
    done
done

log_message "All updates completed successfully"
</file: common/vers_origin.sh>

<file: common/functions.sh>
#!bin/bash

## dbTools/ingest/v0/common/functions.sh

# Common functions used across ingest scripts (mostly mirrored from export functions.sh)

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    mkdir -p "${dir}"
    chmod -R 777 "${dir}"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f ensure_directory
export -f send_notification
</file: common/functions.sh>

<file: common/geom.sh>
#!/bin/bash

# Function to run the update in parallel
run_update() {
  local OFFSET=$1
  local LIMIT=$2
  local DB_NAME=$3
  local TABLE_NAME=$4
  local DB_CONTAINER=$5

  docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -c "
  UPDATE ${TABLE_NAME}
  SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)::public.geometry
  WHERE observation_uuid IN (
    SELECT observation_uuid
    FROM ${TABLE_NAME}
    ORDER BY observation_uuid
    OFFSET ${OFFSET}
    LIMIT ${LIMIT}
  );"
}

# Check if correct number of arguments are provided
if [ "$#" -ne 4 ]; then
  echo "Usage: $0 <database_name> <table_name> <num_workers> <base_dir>"
  exit 1
fi

# Define arguments
DB_NAME=$1
TABLE_NAME=$2
NUM_PROCESSES=$3
BASE_DIR=$4

# Use container name from environment or default
DB_CONTAINER=${DB_CONTAINER:-"ibridaDB"}

# Calculate total rows and batch size
TOTAL_ROWS=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};")
BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES))

# Run updates in parallel
for ((i=0; i<NUM_PROCESSES; i++)); do
  OFFSET=$((i * BATCH_SIZE))
  run_update ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${TABLE_NAME} ${DB_CONTAINER} &
done

# Wait for all processes to finish
wait
echo "All updates completed."
</file: common/geom.sh>

<file: r1/structure.sql>
-- Structure for v0r1 (December 2024 release)
-- Note: anomaly_score column added in r1, not present in r0

CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date,
    anomaly_score numeric(15,6)  -- New column in r1
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

-- Note: The following columns are added by our ingestion process:
-- All tables:
--   origin VARCHAR(255)
--   version VARCHAR(255)
--   release VARCHAR(255)
-- Observations table:
--   geom public.geometry
</file: r1/structure.sql>

<file: r1/wrapper.sh>
#!/bin/bash

### REVIEW: Previous run didn't populate version/origin columns. We applied a fix to vers_origin.sh (argument mismatch) but watch logs carefully next run.

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing ingest process with configuration:"

# Database and user variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"
log_message "Database User: ${DB_USER}"
log_message "Template DB: ${DB_TEMPLATE}"
log_message "Parallel Processes: ${NUM_PROCESSES}"

# Source variable
export SOURCE="Dec2024"
export METADATA_PATH="/metadata/${SOURCE}"
log_message "Source: ${SOURCE}"
log_message "Metadata Path: ${METADATA_PATH}"

# Version and origin values
export ORIGIN_VALUE="iNat-${SOURCE}"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r1/structure.sql"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"
log_message "Origin: ${ORIGIN_VALUE}"
log_message "Structure SQL: ${STRUCTURE_SQL}"

# Execute main script
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
</file: r1/wrapper.sh>

<file: utils/add_release.sh>
#!/bin/bash

# Database variables
DB_USER="postgres"
DB_NAME="ibrida-v0"  # The existing database name
DB_CONTAINER="ibridaDB"
RELEASE_VALUE="r0"

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

# Add release column to all tables
print_progress "Adding release column to tables"
execute_sql "
BEGIN;
ALTER TABLE taxa ADD COLUMN release VARCHAR(255);
ALTER TABLE observers ADD COLUMN release VARCHAR(255);
ALTER TABLE observations ADD COLUMN release VARCHAR(255);
ALTER TABLE photos ADD COLUMN release VARCHAR(255);
COMMIT;
"

# Set release values
print_progress "Setting release values"
execute_sql "
BEGIN;
UPDATE taxa SET release = '${RELEASE_VALUE}';
UPDATE observers SET release = '${RELEASE_VALUE}';
UPDATE observations SET release = '${RELEASE_VALUE}';
UPDATE photos SET release = '${RELEASE_VALUE}';
COMMIT;
"

# Create indexes for release column
print_progress "Creating indexes for release column"
execute_sql "
BEGIN;
CREATE INDEX index_taxa_release ON taxa USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observers_release ON observers USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observations_release ON observations USING GIN (to_tsvector('simple', release));
CREATE INDEX index_photos_release ON photos USING GIN (to_tsvector('simple', release));
COMMIT;
"

print_progress "Release column added and populated successfully"
</file: utils/add_release.sh>

<file: utils/elevation/main.sh>

</file: utils/elevation/main.sh>

<file: utils/elevation/load_dem.sh>

</file: utils/elevation/load_dem.sh>

<file: utils/elevation/update_elevation.sh>

</file: utils/elevation/update_elevation.sh>

<file: utils/elevation/wrapper.sh>

</file: utils/elevation/wrapper.sh>

<file: utils/elevation/create_elevation_table.sh>

</file: utils/elevation/create_elevation_table.sh>

<file: utils/elevation/create_elevation_table.sql>

</file: utils/elevation/create_elevation_table.sql>

</codebase_context>
