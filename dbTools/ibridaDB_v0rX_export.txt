<codebase_context>
  <dirtree root="/home/caleb/repo/ibridaDB/dbTools">
dbTools (6.7k/66.2k)
||-- FLOW.md (32 lines/262 tokens)
||-- README.md (152/1.6k)
||-- __init__.py (1/0)
||-- dem (254/2.6k)
||   |-- download_merit.sh (85/1.1k)
||   \-- download_merit_parallel.sh (169/1.5k)
||-- export (3.4k/33.6k)
||   \-- v0 (3.4k/33.6k)
||       |-- common (1.5k/15.7k)
||       |   |-- clade_defns.sh (153/1.8k)
||       |   |-- clade_helpers.sh (321/3.6k)
||       |   |-- cladistic.sh (314/3.0k)
||       |   |-- functions.sh (64/441)
||       |   |-- main.sh (206/2.0k)
||       |   |-- region_defns.sh (70/774)
||       |   \-- regional_base.sh (401/3.9k)
||       |-- r0 (35/287)
||       |   \-- wrapper.sh (35/287)
||       |-- r1 (1.5k/12.5k)
||       |   |-- wrapper.sh (89/647)
||       |   |-- wrapper_amphibia_all_exc_nonrg_sp.sh (92/722)
||       |   |-- wrapper_amphibia_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/799)
||       |   |-- wrapper_amphibia_all_miniTest.sh (98/776)
||       |   |-- wrapper_angiospermae_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/779)
||       |   |-- wrapper_aves_all_exc_nonrg_sp.sh (96/759)
||       |   |-- wrapper_aves_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/771)
||       |   |-- wrapper_aves_reuse_all_sp.sh (98/791)
||       |   |-- wrapper_mammalia_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/775)
||       |   |-- wrapper_pta.sh (89/663)
||       |   |-- wrapper_pta_all_exc_nonrg_sp.sh (92/726)
||       |   |-- wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh (95/771)
||       |   |-- wrapper_pta_all_exc_nonrg_sp_inc_oor_fas_elev.sh (93/741)
||       |   |-- wrapper_pta_non_rg.sh (93/737)
||       |   |-- wrapper_reptilia.sh (62/517)
||       |   |-- wrapper_reptilia_all_exc_nonrg_sp.sh (96/763)
||       |   \-- wrapper_reptilia_all_exc_nonrg_sp_inc_oor_fas_elev.sh (98/775)
||       \-- scrap (350/5.0k)
||           |-- export_per_species_snippet.sh (96/734)
||           |-- schemas.md (233/2.9k)
||           \-- scrap.md (21/1.3k)
||-- ingest (1.4k/10.5k)
||   \-- v0 (1.4k/10.5k)
||       |-- common (507/4.0k)
||       |   |-- functions.sh (42/273)
||       |   |-- geom.sh (50/366)
||       |   |-- main.sh (263/2.1k)
||       |   \-- vers_origin.sh (152/1.2k)
||       |-- r0 (205/1.4k)
||       |   |-- ingest.sh (183/1.3k)
||       |   \-- wrapper.sh (22/153)
||       |-- r1 (102/723)
||       |   |-- structure.sql (49/283)
||       |   \-- wrapper.sh (53/440)
||       |-- r2 (108/760)
||       |   |-- structure.sql (49/283)
||       |   \-- wrapper.sh (59/477)
||       \-- utils (478/3.5k)
||           |-- add_release.sh (55/365)
||           \-- elevation (423/3.2k)
||               |-- create_elevation_table.sh (55/407)
||               |-- create_elevation_table.sql (15/96)
||               |-- load_dem.sh (97/850)
||               |-- main.sh (79/580)
||               |-- update_elevation.sh (115/849)
||               \-- wrapper.sh (62/422)
||-- schema.md (45/906)
|\-- taxa (1.4k/16.5k)
|    |-- ColDP_raw_samples.txt (205/4.8k)
|    |-- __init__.py (1/0)
|    |-- analysis_utils.py (67/888)
|    |-- analyze_diff.py (244/2.6k)
|    |-- diffs (217/1.7k)
|    |   \-- May2024 (217/1.7k)
|    |       |-- L40_analysis.txt (94/776)
|    |       |-- L50_analysis.txt (107/885)
|    |       \-- L60_analysis.txt (16/116)
|    |-- expand (260/2.5k)
|    |   \-- expand_taxa.sh (260/2.5k)
|    |-- models (350/3.3k)
|    |   |-- __init__.py (19/116)
|    |   |-- coldp_models.py (106/945)
|    |   |-- expanded_taxa.py (110/1.1k)
|    |   \-- expanded_taxa_cmn.py (115/1.1k)
|    |-- reference.md (41/374)
|    \-- tools (23/179)
|        \-- extract_ColDP_samples.sh (23/179)
  </dirtree>
  <files>
    <file path="FLOW.md">
```mermaid
flowchart TB

    subgraph Ingest["Database Initialization (ingest/)"]
        i_wrap["Ingest Wrapper<br/>(e.g. r0/wrapper.sh)"]
        i_main["Ingest Main<br/>(common/main.sh)"]
        i_other["Other Common Scripts"]
        db["(ibridaDB PostgreSQL)"]
        i_wrap --> i_main
        i_main --> i_other
        i_other --> db
    end

    subgraph Export["Data Export (export/)"]
        e_wrap["Export Wrapper<br/>(e.g. r1/my_wrapper.sh)"]
        e_main["Export Main<br/>(common/main.sh)"]
        rbase["regional_base.sh<br/>Species + Ancestors"]
        clad["cladistic.sh<br/>RG_FILTER_MODE + partial-labeled"]
        csv_out["CSV + Summary Files"]
        e_wrap --> e_main
        e_main --> rbase
        rbase --> clad
        clad --> csv_out
    end

    i_other --> db
    db --> e_wrap

    style Ingest fill:#f9f,stroke:#333,stroke-width:2px
    style Export fill:#bbf,stroke:#333,stroke-width:2px

```
    </file>
    <file path="README.md">
# ibrida Database Reproduction Guide

## Overview
This guide documents the end-to-end process for **reproducing** and **exporting** from the ibrida database, which is derived from iNaturalist open data dumps. The database uses a versioning system with two components:
- **Version (v#)**: Indicates structural changes (schema revisions) to the database.
- **Release (r#)**: Indicates distinct data releases from iNaturalist under the same schema version.

For example:
- **v0r0**: June 2024 iNat data release
- **v0r1**: December 2024 iNat data release (adds `anomaly_score` column to `observations`)

## System Architecture
The pipeline is split into two phases:
1. **Database Initialization** (`ingest/`)
2. **Data Export** (`export/`)

Each phase has:
- Common scripts for shared logic
- Release- or job-specific *wrapper scripts* that set environment variables for that particular run

## 1. Database Initialization (ingest/)
### Directory Structure

```
dbTools/ingest/v0/
├── common/
│   ├── geom.sh         # Geometry calculations
│   ├── vers_origin.sh  # Version/origin updates
│   └── main.sh         # Core ingestion logic
├── r0/
│   ├── wrapper.sh      # June 2024 release
│   └── structure.sql   # schema for r0
└── r1/
    ├── wrapper.sh      # December 2024 release
    └── structure.sql   # schema for r1 (adds anomaly_score)
```

### Running the Ingestion Process
1. **Make scripts executable**:
    ```bash
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/*.sh
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r0/wrapper.sh
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
    ```
2. **Run**:
    ```bash
    # For June 2024 (r0)
    /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r0/wrapper.sh

    # For December 2024 (r1)
    /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
    ```

## 2. Data Export (export/)
The export pipeline allows flexible subsetting of the DB by region, minimum threshold, clade, etc. For additional detail, see [export.md](export.md).

### Directory Structure
```
dbTools/export/v0/
├── common/
│   ├── main.sh            # Orchestrates creation or skipping of base tables; final summary
│   ├── regional_base.sh   # Region-based table creation, ancestor-aware logic
│   ├── cladistic.sh       # Taxonomic filtering, partial-rank wiping, CSV export
│   └── functions.sh       # Shared shell functions
├── r0/
│   └── wrapper.sh         # Example job wrapper for June 2024 export
├── r1/
│   └── wrapper.sh         # Example job wrapper for December 2024 export
└── export.md              # Detailed usage documentation (v1)
```

### Export Workflow
1. **User creates/edits a wrapper script** (e.g., `r1/my_special_wrapper.sh`) to set:
   - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`
   - Optional toggles like `INCLUDE_OUT_OF_REGION_OBS`, `RG_FILTER_MODE`, `ANCESTOR_ROOT_RANKLEVEL`, `MIN_OCCURRENCES_PER_RANK`
   - A unique `EXPORT_GROUP` name
2. **Run** that wrapper. The pipeline will:
   1. **(regional_base.sh)** Build base tables of in-threshold species + ancestors, optionally bounding to region or not, depending on `INCLUDE_OUT_OF_REGION_OBS`.
   2. **(cladistic.sh)** Filter final observations by clade or metaclade, optionally wipe partial ranks, and do a random-sample CSV export.
   3. **(main.sh)** Write a summary file enumerating environment variables, row counts, timing, etc.

3. **Check** `/datasets/ibrida-data/exports` for final CSV output (organized by `VERSION_VALUE` / `RELEASE_VALUE` / any job-specific subdirectory).

### Drafting a New Wrapper
It is **recommended** to create a separate wrapper script for each new export job. For instance:
```bash
#!/bin/bash

export WRAPPER_PATH="$0"

export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_test"

export INCLUDE_OUT_OF_REGION_OBS=false
export RG_FILTER_MODE="ALL"
export ANCESTOR_ROOT_RANKLEVEL=40
export MIN_OCCURRENCES_PER_RANK=30

# other optional vars, e.g. PROCESS_OTHER, SKIP_REGIONAL_BASE, etc.

export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/myamphibia_job"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

source "${BASE_DIR}/common/functions.sh"

/home/caleb/repo/ibridaDB/dbTools/export/v0/common/main.sh
```
Then `chmod +x` this file and run it to generate a new job.

### Example Outputs
The final CSV and summary are placed in a subdirectory (e.g. `v0/r1/myamphibia_job`). A typical summary file `amphibia_test_export_summary.txt` includes:
- Region: NAfull
- MIN_OBS: 50
- RG_FILTER_MODE: ALL
- Observations: 10,402
- Unique Taxa: 927
- Timings for each step

### Further Reading
- **[export.md](export/v0/export.md)** for a deeper parameter reference (v1).
- **clade_defns.sh** for built-in definitions of macroclades, clades, and metaclades.

## Overall Flow
Below is a schematic of the entire ingest→export pipeline. For details on the ingest side, see [Ingestion docs](#database-initialization-ingest):
```
Ingest (ingest/v0/) --> Database --> Export (export/v0/)
```
In the export sub-phase, each new wrapper script can define a distinct job. Summaries and CSVs are stored in `HOST_EXPORT_BASE_PATH` for easy retrieval and analysis.

## Notes on Schema
- **v0r1** adds the `anomaly_score numeric(15,6)` column to `observations`.
- The export scripts automatically check if that column is present based on `RELEASE_VALUE`.
- If partial-labeled data is desired (coarse ranks for rare species), see the advanced features in `regional_base.sh` (ancestor logic) and `cladistic.sh` (partial-rank wiping logic).

**Notes**:
- The ingest side is unchanged for v0→v0r1 except for adding columns and data updates.
- The export side is significantly more flexible now, supporting ancestor‐aware logic and partial-labeled data.  
- Each new export job typically has its own wrapper script referencing the relevant `VERSION_VALUE`, `RELEASE_VALUE`, region, and clade parameters.
    </file>
    <file path="__init__.py">

    </file>
    <file path="schema.md">
```markdown
### Observations
Column | Description
-------|------------
observation_uuid | A unique identifier associated with each observation also available at iNaturalist.org via URLs constructed like this https://www.inaturalist.org/observations/c075c500-b566-44aa-847c-95da8fb8b3c9
observer_id | The identifier of the associated iNaturalist user who recorded the observation
latitude | The latitude where the organism was encountered
longitude | The longitude where the organism was encountered
positional_accuracy | The uncertainty in meters around the latitude and longitude
taxon_id | The identifier of the associated axon the observation has been identified as
quality_grade | `Casual` observations are missing certain data components (e.g. latitude) or may have flags associated with them not shown here (e.g. `location appears incorrect`). Observations flagged as not wild are also considered Casual. All other observations are either `Needs ID` or `Research Grade`. Generally, Research Grade observations have more than one agreeing identifications at the species level, or if there are disagreements at least ⅔ of the identifications are in agreement a the species level
observed_on | The date at which the observation took place
<NOTE> New column added in v0/r1 'anomaly_score' </NOTE>

### Observers
Column | Description
-------|------------
observer_id | A unique identifier associated with each observer also available on https://www.inaturalist.org via URLs constructed like this: https://www.inaturalist.org/users/1
login | A unique login associated with each observer
name | Personal name of the observer, if provided

### Photos
Column | Description
-------|------------
photo_uuid | A unique identifier associated with each photo. Note that photo_uuid can be non-unique across different observations.
photo_id | A photo identifier used on iNaturalist and available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/photos/113756411
observation_uuid | The identifier of the associated observation
observer_id | The identifier of the associated observer who took the photo
extension | The image file format, e.g. `jpeg`
license | All photos in the dataset have open licenses (e.g. Creative Commons) and unlicensed (CC0 / public domain)
width | The width of the photo in pixels
height | The height of the photo in pixels
position | When observations have multiple photos the user can set the position in which the photos should appear. Lower numbers are meant to appear first
>The issue is that some observations include more than one photo, and photos associated with observations that have >1 photo share a photo_id and photo_uuid, which I did not expect. These additional photos (which have their own rows in the 'photos' table) are denoted by the 'position' field, where position ==0 indicates that the photo is the primary photo for the record. If an observation only has one photo, then the associated 'photos' record will have position == 0. Therefore. I'm pretty sure that a composite key of photo_id ++ photo_uuid ++ position will function as a primary key. 

### Taxa
Column | Description
-------|------------
taxon_id | A unique identifier associated with each node in the iNaturalist taxonomy hierarchy. Also available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/taxa/47219
ancestry | The taxon_ids of ancestry of the taxon ordered from the root of the tree to the taxon concatenated together with `\`
rank_level | A number associated with the rank. Taxon rank_levels must be less than the rank level of their parent. For example, a taxon with rank genus and rank_level 20 cannot descend from a taxon of rank species and rank_level 10
rank | A constrained set of labels associated with nodes on the hierarchy. These include the standard Linnaean ranks: Kingdom, Phylum, Class, Order, Family, Genus, Species, and a number of internodes such as Subfamily
name | The scientific name for the taxon
active | When the taxonomy changes, generally taxa aren’t deleted on iNaturalist to avoid breaking links. Instead taxa are made inactive and observations are moved to new active nodes. Occasionally, observations linger on inactive taxa which are no longer active parts of the iNaturalist taxonomy
```
    </file>
    <dir path="gest">
      <dir path="gest/t">
        <dir path="gest/t/v0">
          <dir path="gest/t/v0/common">
            <file path="ingest/v0/common/functions.sh">
#!bin/bash

## dbTools/ingest/v0/common/functions.sh

# Common functions used across ingest scripts (mostly mirrored from export functions.sh)

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    mkdir -p "${dir}"
    chmod -R 777 "${dir}"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f ensure_directory
export -f send_notification
            </file>
            <file path="ingest/v0/common/geom.sh">
#!/bin/bash

# Function to run the update in parallel
run_update() {
  local OFFSET=$1
  local LIMIT=$2
  local DB_NAME=$3
  local TABLE_NAME=$4
  local DB_CONTAINER=$5

  docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -c "
  UPDATE ${TABLE_NAME}
  SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)::public.geometry
  WHERE observation_uuid IN (
    SELECT observation_uuid
    FROM ${TABLE_NAME}
    ORDER BY observation_uuid
    OFFSET ${OFFSET}
    LIMIT ${LIMIT}
  );"
}

# Check if correct number of arguments are provided
if [ "$#" -ne 4 ]; then
  echo "Usage: $0 <database_name> <table_name> <num_workers> <base_dir>"
  exit 1
fi

# Define arguments
DB_NAME=$1
TABLE_NAME=$2
NUM_PROCESSES=$3
BASE_DIR=$4

# Use container name from environment or default
DB_CONTAINER=${DB_CONTAINER:-"ibridaDB"}

# Calculate total rows and batch size
TOTAL_ROWS=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};")
BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES))

# Run updates in parallel
for ((i=0; i<NUM_PROCESSES; i++)); do
  OFFSET=$((i * BATCH_SIZE))
  run_update ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${TABLE_NAME} ${DB_CONTAINER} &
done

# Wait for all processes to finish
wait
echo "All updates completed."
            </file>
            <file path="ingest/v0/common/main.sh" line_interval="25">
#!/bin/bash
#
# main.sh
#
# Core ingestion logic for a given database release. Creates the database,
# imports CSV data, configures geometry, version columns, etc. Now also
# optionally calls the elevation pipeline if ENABLE_ELEVATION=true.
#
# This script expects the following variables to be set by the wrapper:
#   - DB_USER
#   - DB_TEMPLATE
#   - NUM_PROCESSES
#   - BASE_DIR
#   - SOURCE
#   - ORIGIN_VALUE
#   - VERSION_VALUE
#   - RELEASE_VALUE
#   - DB_NAME
#   - DB_CONTAINER
#   - METADATA_PATH
#   - STRUCTURE_SQL
#   - ENABLE_ELEVATION (new; optional, defaults to "false" if not set)
#
# Example usage:
#|LN|25|
#   ENABLE_ELEVATION=true /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
#

set -euo pipefail

# ------------------------------------------------------------------------------
# 1. Validate required variables
# ------------------------------------------------------------------------------
required_vars=(
    "DB_USER" "DB_TEMPLATE" "NUM_PROCESSES" "BASE_DIR"
    "SOURCE" "ORIGIN_VALUE" "VERSION_VALUE" "DB_NAME"
    "DB_CONTAINER" "METADATA_PATH" "STRUCTURE_SQL"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var:-}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Default ENABLE_ELEVATION to "false" if not defined
ENABLE_ELEVATION="${ENABLE_ELEVATION:-false}"

# ------------------------------------------------------------------------------
#|LN|50|
# 2. Source shared functions
# ------------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

print_progress "Starting core ingestion for ${DB_NAME}"
send_notification "[INFO] Starting ingestion for ${DB_NAME}"

# ------------------------------------------------------------------------------
# 3. Create Database
# ------------------------------------------------------------------------------
print_progress "Creating database ${DB_NAME} from template ${DB_TEMPLATE}"
execute_sql_postgres() {
    local sql="$1"
    docker exec "${DB_CONTAINER}" psql -U "${DB_USER}" -c "$sql"
}

execute_sql_postgres "DROP DATABASE IF EXISTS \"${DB_NAME}\";"
execute_sql_postgres "CREATE DATABASE \"${DB_NAME}\" WITH TEMPLATE ${DB_TEMPLATE} OWNER ${DB_USER};"

# ------------------------------------------------------------------------------
# 4. Create tables from structure SQL
# ------------------------------------------------------------------------------
print_progress "Creating tables from ${STRUCTURE_SQL}"
if [ ! -f "${STRUCTURE_SQL}" ]; then
  echo "Error: STRUCTURE_SQL file not found: ${STRUCTURE_SQL}"
#|LN|75|
  exit 1
fi

cat "${STRUCTURE_SQL}" | docker exec -i "${DB_CONTAINER}" psql -U "${DB_USER}" -d "${DB_NAME}"

# ------------------------------------------------------------------------------
# 5. Import data
# ------------------------------------------------------------------------------
print_progress "Importing CSV data from ${METADATA_PATH}"
execute_sql "
BEGIN;

COPY observations
FROM '${METADATA_PATH}/observations.csv'
DELIMITER E'\t'
QUOTE E'\b'
CSV HEADER;

COPY photos
FROM '${METADATA_PATH}/photos.csv'
DELIMITER E'\t'
QUOTE E'\b'
CSV HEADER;

COPY taxa
#|LN|100|
FROM '${METADATA_PATH}/taxa.csv'
DELIMITER E'\t'
QUOTE E'\b'
CSV HEADER;

COPY observers
FROM '${METADATA_PATH}/observers.csv'
DELIMITER E'\t'
QUOTE E'\b'
CSV HEADER;

COMMIT;
"

# ------------------------------------------------------------------------------
# 6. Create indexes
# ------------------------------------------------------------------------------
print_progress "Creating base indexes"
execute_sql "
BEGIN;

CREATE INDEX index_photos_photo_uuid         ON photos USING btree (photo_uuid);
CREATE INDEX index_photos_observation_uuid   ON photos USING btree (observation_uuid);
CREATE INDEX index_photos_position           ON photos USING btree (position);
CREATE INDEX index_photos_photo_id           ON photos USING btree (photo_id);
#|LN|125|
CREATE INDEX index_taxa_taxon_id             ON taxa   USING btree (taxon_id);
CREATE INDEX index_observers_observers_id    ON observers USING btree (observer_id);
CREATE INDEX index_observations_observer_id  ON observations USING btree (observer_id);
CREATE INDEX index_observations_quality      ON observations USING btree (quality_grade);
CREATE INDEX index_observations_taxon_id     ON observations USING btree (taxon_id);
CREATE INDEX index_taxa_active               ON taxa USING btree (active);

COMMIT;
"

# Conditional index for anomaly_score
execute_sql "
DO \$\$
BEGIN
    IF EXISTS (
        SELECT 1
        FROM information_schema.columns
        WHERE table_name = 'observations'
        AND column_name = 'anomaly_score'
    ) THEN
        CREATE INDEX idx_observations_anomaly ON observations (anomaly_score);
    END IF;
END \$\$;"

# ------------------------------------------------------------------------------
#|LN|150|
# 7. Add geom column & compute geometry in parallel
# ------------------------------------------------------------------------------
print_progress "Adding geom column to observations"
execute_sql "ALTER TABLE observations ADD COLUMN geom public.geometry;"

print_progress "Populating geom column in parallel"
"${BASE_DIR}/common/geom.sh" "${DB_NAME}" "observations" "${NUM_PROCESSES}" "${BASE_DIR}"

# Create geom index
print_progress "Creating GIST index on geom"
execute_sql "CREATE INDEX observations_geom ON observations USING GIST (geom);"

# ------------------------------------------------------------------------------
# 8. Vacuum
# ------------------------------------------------------------------------------
print_progress "Vacuum/Analyze after geometry load"
execute_sql "VACUUM ANALYZE;"

# ------------------------------------------------------------------------------
# 9. Add origin, version, and release columns
# ------------------------------------------------------------------------------
print_progress "Adding origin/version/release columns in parallel"
execute_sql "
BEGIN;

#|LN|175|
ALTER TABLE taxa         ADD COLUMN origin   VARCHAR(255);
ALTER TABLE observers    ADD COLUMN origin   VARCHAR(255);
ALTER TABLE observations ADD COLUMN origin   VARCHAR(255);
ALTER TABLE photos       ADD COLUMN origin   VARCHAR(255);

ALTER TABLE photos       ADD COLUMN version  VARCHAR(255);
ALTER TABLE observations ADD COLUMN version  VARCHAR(255);
ALTER TABLE observers    ADD COLUMN version  VARCHAR(255);
ALTER TABLE taxa         ADD COLUMN version  VARCHAR(255);

ALTER TABLE photos       ADD COLUMN release  VARCHAR(255);
ALTER TABLE observations ADD COLUMN release  VARCHAR(255);
ALTER TABLE observers    ADD COLUMN release  VARCHAR(255);
ALTER TABLE taxa         ADD COLUMN release  VARCHAR(255);

COMMIT;
"

print_progress "Populating origin/version/release columns"
"${BASE_DIR}/common/vers_origin.sh" "${DB_NAME}" "${NUM_PROCESSES}" "${ORIGIN_VALUE}" "${VERSION_VALUE}" "${RELEASE_VALUE}"

# ------------------------------------------------------------------------------
# 10. Create GIN indexes for origin/version/release
# ------------------------------------------------------------------------------
print_progress "Creating GIN indexes for origin/version/release"
#|LN|200|
execute_sql "
BEGIN;

CREATE INDEX index_taxa_origins        ON taxa        USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_taxa_name           ON taxa        USING GIN (to_tsvector('simple', name));
CREATE INDEX index_observers_origins   ON observers   USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_observations_origins ON observations USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_origins      ON photos      USING GIN (to_tsvector('simple', origin));

CREATE INDEX index_photos_version      ON photos      USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observations_version ON observations USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observers_version   ON observers   USING GIN (to_tsvector('simple', version));
CREATE INDEX index_taxa_version        ON taxa        USING GIN (to_tsvector('simple', version));

CREATE INDEX index_photos_release      ON photos      USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observations_release ON observations USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observers_release   ON observers   USING GIN (to_tsvector('simple', release));
CREATE INDEX index_taxa_release        ON taxa        USING GIN (to_tsvector('simple', release));

COMMIT;
"

# ------------------------------------------------------------------------------
# 11. Optional Elevation Flow
# ------------------------------------------------------------------------------
#|LN|225|
if [ "${ENABLE_ELEVATION}" == "true" ]; then
  print_progress "ENABLE_ELEVATION=true, proceeding with elevation pipeline"
  send_notification "[INFO] Elevation pipeline triggered for ${DB_NAME}"

  # Either call the 'wrapper.sh' or call 'main.sh' directly.
  # We'll illustrate direct call to main.sh here: (note; makes sense to direct call here, wrapper is for standalone use)
  ELEVATION_SCRIPT="${BASE_DIR}/utils/elevation/main.sh"

  # Example: pass your dem directory, concurrency, etc. 
  # If your release wrapper already sets DEM_DIR, EPSG, etc. environment variables, you can do:
  DEM_DIR="${DEM_DIR:-"/datasets/dem/merit"}"
  EPSG="${EPSG:-"4326"}"
  TILE_SIZE="${TILE_SIZE:-"100x100"}"

  if [ -x "${ELEVATION_SCRIPT}" ]; then
    "${ELEVATION_SCRIPT}" \
      "${DB_NAME}" \
      "${DB_USER}" \
      "${DB_CONTAINER}" \
      "${DEM_DIR}" \
      "${NUM_PROCESSES}" \
      "${EPSG}" \
      "${TILE_SIZE}"
  else
    echo "Warning: Elevation script not found or not executable at ${ELEVATION_SCRIPT}"
#|LN|250|
  fi

  print_progress "Elevation pipeline complete for ${DB_NAME}"
else
  print_progress "ENABLE_ELEVATION=false, skipping elevation pipeline"
  send_notification "[INFO] Skipping elevation pipeline for ${DB_NAME}"
fi

# ------------------------------------------------------------------------------
# 12. Final notice
# ------------------------------------------------------------------------------
print_progress "Database setup complete for ${DB_NAME}"
send_notification "[OK] Ingestion (and optional elevation) complete for ${DB_NAME}"
            </file>
            <file path="ingest/v0/common/vers_origin.sh" line_interval="25">
#!/bin/bash

# COMMENT: populates origin, version, and release columns in parallel
# COMMENT: these are the only columns on the base tables that are not populated by the ingest process

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/vers_origin_$(date +%Y%m%d_%H%M%S).log"
echo "Starting version/origin/release updates at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "${LOG_FILE}"
}

# Function for error logging and exit
error_exit() {
    log_message "ERROR: $1"
    exit 1
}

# Function to run the update in parallel
run_update() {
    local TABLE_NAME=$1
#|LN|25|
    local COLUMN_NAME=$2
    local VALUE=$3
    local OFFSET=$4
    local LIMIT=$5
    local DB_NAME=$6
    local DB_CONTAINER=$7
    local PROCESS_NUM=$8

    log_message "Process $PROCESS_NUM: Updating $TABLE_NAME.$COLUMN_NAME (offset: $OFFSET, limit: $LIMIT)"
    
    UPDATE_RESULT=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "
    UPDATE ${TABLE_NAME}
    SET ${COLUMN_NAME} = '${VALUE}'
    WHERE ctid IN (
        SELECT ctid
        FROM ${TABLE_NAME}
        ORDER BY ctid
        OFFSET ${OFFSET}
        LIMIT ${LIMIT}
    );")
    
    if [ $? -ne 0 ]; then
        error_exit "Failed to update ${TABLE_NAME}.${COLUMN_NAME} in process $PROCESS_NUM"
    fi
    
#|LN|50|
    log_message "Process $PROCESS_NUM: Completed update of $TABLE_NAME.$COLUMN_NAME"
}

# Validate arguments
if [ "$#" -ne 5 ]; then
    error_exit "Usage: $0 <database_name> <num_workers> <origin_value> <version_value> <release_value>"
fi

# Define arguments
DB_NAME=$1
NUM_PROCESSES=$2
ORIGIN_VALUE=$3
VERSION_VALUE=$4
RELEASE_VALUE=$5

# Validate NUM_PROCESSES is a positive integer
if ! [[ "$NUM_PROCESSES" =~ ^[1-9][0-9]*$ ]]; then
    error_exit "Number of workers must be a positive integer"
fi

# Use container name from environment or default
DB_CONTAINER=${DB_CONTAINER:-"ibridaDB"}

# Verify database exists
if ! docker exec ${DB_CONTAINER} psql -U postgres -lqt | cut -d \| -f 1 | grep -qw "${DB_NAME}"; then
#|LN|75|
    error_exit "Database ${DB_NAME} does not exist"
fi

# Tables and their columns to update
declare -A TABLES_COLUMNS
TABLES_COLUMNS=(
    ["taxa"]="origin,version,release"
    ["observers"]="origin,version,release"
    ["observations"]="origin,version,release"
    ["photos"]="origin,version,release"
)

# Function to update columns in parallel
update_columns_in_parallel() {
    local TABLE_NAME=$1
    local COLUMN_NAME=$2
    local VALUE=$3
    local TOTAL_ROWS

    # Verify table exists
    if ! docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -c "\d ${TABLE_NAME}" &>/dev/null; then
        error_exit "Table ${TABLE_NAME} does not exist in database ${DB_NAME}"
    }

    # Get total rows with error handling
#|LN|100|
    TOTAL_ROWS=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};" | tr -d ' ')
    if [ $? -ne 0 ] || ! [[ "$TOTAL_ROWS" =~ ^[0-9]+$ ]]; then
        error_exit "Failed to get row count for ${TABLE_NAME}"
    }

    log_message "Starting parallel update of ${TABLE_NAME}.${COLUMN_NAME} (${TOTAL_ROWS} total rows)"
    
    local BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES + 1))
    local PIDS=()

    for ((i=0; i<NUM_PROCESSES; i++)); do
        local OFFSET=$((i * BATCH_SIZE))
        run_update ${TABLE_NAME} ${COLUMN_NAME} ${VALUE} ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${DB_CONTAINER} $i &
        PIDS+=($!)
    done

    # Wait for all processes and check their exit status
    for pid in "${PIDS[@]}"; do
        if ! wait $pid; then
            error_exit "One of the parallel update processes failed"
        fi
    done
    
    log_message "Completed update of ${TABLE_NAME}.${COLUMN_NAME}"
}
#|LN|125|

# Main update process
log_message "Starting updates with parameters:"
log_message "Database: ${DB_NAME}"
log_message "Number of processes: ${NUM_PROCESSES}"
log_message "Origin value: ${ORIGIN_VALUE}"
log_message "Version value: ${VERSION_VALUE}"
log_message "Release value: ${RELEASE_VALUE}"

for TABLE_NAME in "${!TABLES_COLUMNS[@]}"; do
    log_message "Processing table: ${TABLE_NAME}"
    IFS=',' read -ra COLUMNS <<< "${TABLES_COLUMNS[$TABLE_NAME]}"
    for COLUMN in "${COLUMNS[@]}"; do
        case "$COLUMN" in
            "origin")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$ORIGIN_VALUE"
                ;;
            "version")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$VERSION_VALUE"
                ;;
            "release")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$RELEASE_VALUE"
                ;;
        esac
    done
#|LN|150|
done

log_message "All updates completed successfully"
            </file>
          </dir>
          <dir path="gest/t/v0/r0">
            <file path="ingest/v0/r0/ingest.sh" line_interval="25">
#!/bin/bash
### DEV: DEPRECATED, REMOVE ONCE WRAPPER/MAIN IS TESTED/VERIFIED
### DEV: Reference for functionality to port to new modularized system

# Database and user variables
DB_USER="postgres"
DB_TEMPLATE="template_postgis"
NUM_PROCESSES=16
BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

# Source variable
SOURCE="June2024"

# Construct origin value based on source
ORIGIN_VALUE="iNat-${SOURCE}"

# Version variable
VERSION_VALUE="v0"

# Construct database name
DB_NAME="ibrida-${VERSION_VALUE}"

# Function to execute SQL commands
execute_sql() {
#|LN|25|
  local sql="$1"
  docker exec ibrida psql -U "$DB_USER" -d "$DB_NAME" -c "$sql"
}

# Function to print progress
print_progress() {
  local message="$1"
  echo "======================================"
  echo "$message"
  echo "======================================"
}

# Create database, drop if exists
print_progress "Creating database"
docker exec ibrida psql -U "$DB_USER" -c "DROP DATABASE IF EXISTS \"$DB_NAME\";"
docker exec ibrida psql -U "$DB_USER" -c "CREATE DATABASE \"$DB_NAME\" WITH TEMPLATE $DB_TEMPLATE OWNER $DB_USER;"

# Connect to the database and create tables
print_progress "Creating tables"
execute_sql "
BEGIN;

CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
#|LN|50|
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
#|LN|75|
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

COMMIT;
"

# Import data
print_progress "Importing data"
execute_sql "
BEGIN;

COPY observations FROM '/metadata/${SOURCE}/observations.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY photos FROM '/metadata/${SOURCE}/photos.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY taxa FROM '/metadata/${SOURCE}/taxa.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY observers FROM '/metadata/${SOURCE}/observers.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;

COMMIT;
"
#|LN|100|

# Create indexes
print_progress "Creating indexes"
execute_sql "
BEGIN;

CREATE INDEX index_photos_photo_uuid ON photos USING btree (photo_uuid);
CREATE INDEX index_photos_observation_uuid ON photos USING btree (observation_uuid);
CREATE INDEX index_photos_position ON photos USING btree (position);
CREATE INDEX index_photos_photo_id ON photos USING btree (photo_id);
CREATE INDEX index_taxa_taxon_id ON taxa USING btree (taxon_id);
CREATE INDEX index_observers_observers_id ON observers USING btree (observer_id);
CREATE INDEX index_observations_observer_id ON observations USING btree (observer_id);
CREATE INDEX index_observations_quality ON observations USING btree (quality_grade);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);
CREATE INDEX index_taxa_active ON taxa USING btree (active);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);

COMMIT;
"

# Add geom column (parallelized calculation using geom.sh)
print_progress "Adding geom column"
execute_sql "ALTER TABLE observations ADD COLUMN geom public.geometry;"

#|LN|125|
# Run parallel geom calculations
print_progress "Running parallel geom calculations"
"${BASE_DIR}/geom.sh" "$DB_NAME" "observations" "$NUM_PROCESSES" "$BASE_DIR"

# Create geom index
print_progress "Creating geom index"
execute_sql "
BEGIN;

CREATE INDEX observations_geom ON observations USING GIST (geom);

COMMIT;
"

# Vacuum analyze
print_progress "Vacuum analyze"
execute_sql "VACUUM ANALYZE;"

# Add origin and version columns in parallel
print_progress "Adding origin and version columns"
execute_sql "
BEGIN;

ALTER TABLE taxa ADD COLUMN origin VARCHAR(255);
ALTER TABLE observers ADD COLUMN origin VARCHAR(255);
#|LN|150|
ALTER TABLE observations ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN version VARCHAR(255);
ALTER TABLE observations ADD COLUMN version VARCHAR(255);
ALTER TABLE observers ADD COLUMN version VARCHAR(255);
ALTER TABLE taxa ADD COLUMN version VARCHAR(255);

COMMIT;
"

# Run parallel updates for origin and version columns
print_progress "Running parallel updates for origin and version columns"
"${BASE_DIR}/vers_origin.sh" "$DB_NAME" "$NUM_PROCESSES" "$ORIGIN_VALUE" "$VERSION_VALUE"

# Create indexes for origin and version columns
print_progress "Creating indexes for origin and version columns"
execute_sql "
BEGIN;

CREATE INDEX index_taxa_origins ON taxa USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_taxa_name ON taxa USING GIN (to_tsvector('simple', name));
CREATE INDEX index_observers_origins ON observers USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_observations_origins ON observations USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_origins ON photos USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_version ON photos USING GIN (to_tsvector('simple', version));
#|LN|175|
CREATE INDEX index_observations_version ON observations USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observers_version ON observers USING GIN (to_tsvector('simple', version));
CREATE INDEX index_taxa_version ON taxa USING GIN (to_tsvector('simple', version));

COMMIT;
"

print_progress "Database setup complete"
            </file>
            <file path="ingest/v0/r0/wrapper.sh">
#!/bin/bash

# Database and user variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

# Source variable
export SOURCE="June2024"
export METADATA_PATH="/metadata/${SOURCE}"

# Version and origin values
export ORIGIN_VALUE="iNat-${SOURCE}"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r0"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r0/structure.sql"

# Execute main script
"${BASE_DIR}/common/main.sh"    
            </file>
          </dir>
          <dir path="gest/t/v0/r1">
            <file path="ingest/v0/r1/structure.sql">
-- Structure for v0r1 (December 2024 release)
-- Note: anomaly_score column added in r1, not present in r0

CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date,
    anomaly_score numeric(15,6)  -- New column in r1
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

-- Note: The following columns are added by our ingestion process:
-- All tables:
--   origin VARCHAR(255)
--   version VARCHAR(255)
--   release VARCHAR(255)
-- Observations table:
--   geom public.geometry
            </file>
            <file path="ingest/v0/r1/wrapper.sh">
#!/bin/bash

### REVIEW: Previous run didn't populate version/origin columns. We applied a fix to vers_origin.sh (argument mismatch) but watch logs carefully next run.

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing ingest process with configuration:"

# Database and user variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"
log_message "Database User: ${DB_USER}"
log_message "Template DB: ${DB_TEMPLATE}"
log_message "Parallel Processes: ${NUM_PROCESSES}"

# Source variable
export SOURCE="Dec2024"
export METADATA_PATH="/metadata/${SOURCE}"
log_message "Source: ${SOURCE}"
log_message "Metadata Path: ${METADATA_PATH}"

# Version and origin values
export ORIGIN_VALUE="iNat-${SOURCE}"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r1/structure.sql"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"
log_message "Origin: ${ORIGIN_VALUE}"
log_message "Structure SQL: ${STRUCTURE_SQL}"

# Execute main script
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
            </file>
          </dir>
          <dir path="gest/t/v0/r2">
            <file path="ingest/v0/r2/structure.sql">
-- Structure for v0r1 (December 2024 release)
-- Note: anomaly_score column added in r1, not present in r0

CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date,
    anomaly_score numeric(15,6)  -- New column in r1
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

-- Note: The following columns are added by our ingestion process:
-- All tables:
--   origin VARCHAR(255)
--   version VARCHAR(255)
--   release VARCHAR(255)
-- Observations table:
--   geom public.geometry
            </file>
            <file path="ingest/v0/r2/wrapper.sh">
#!/bin/bash

### REVIEW: Previous run didn't populate version/origin columns. We applied a fix to vers_origin.sh (argument mismatch) but watch logs carefully next run.

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing ingest process with configuration:"

# Database and user variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"
log_message "Database User: ${DB_USER}"
log_message "Template DB: ${DB_TEMPLATE}"
log_message "Parallel Processes: ${NUM_PROCESSES}"

# Source variable
export SOURCE="Feb2025"
export METADATA_PATH="/metadata/${SOURCE}"
log_message "Source: ${SOURCE}"
log_message "Metadata Path: ${METADATA_PATH}"

# Process elevation?
export ENABLE_ELEVATION=true
export DEM_DIR="/datasets/dem/merit"
export EPSG="4326"
export TILE_SIZE="100x100"

# Version and origin values
export ORIGIN_VALUE="iNat-${SOURCE}"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r2"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r2/structure.sql"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"
log_message "Origin: ${ORIGIN_VALUE}"
log_message "Structure SQL: ${STRUCTURE_SQL}"

# Execute main script
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
            </file>
          </dir>
          <dir path="gest/t/v0/utils">
            <file path="ingest/v0/utils/add_release.sh">
#!/bin/bash

# Database variables
DB_USER="postgres"
DB_NAME="ibrida-v0"  # The existing database name
DB_CONTAINER="ibridaDB"
RELEASE_VALUE="r0"

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

# Add release column to all tables
print_progress "Adding release column to tables"
execute_sql "
BEGIN;
ALTER TABLE taxa ADD COLUMN release VARCHAR(255);
ALTER TABLE observers ADD COLUMN release VARCHAR(255);
ALTER TABLE observations ADD COLUMN release VARCHAR(255);
ALTER TABLE photos ADD COLUMN release VARCHAR(255);
COMMIT;
"

# Set release values
print_progress "Setting release values"
execute_sql "
BEGIN;
UPDATE taxa SET release = '${RELEASE_VALUE}';
UPDATE observers SET release = '${RELEASE_VALUE}';
UPDATE observations SET release = '${RELEASE_VALUE}';
UPDATE photos SET release = '${RELEASE_VALUE}';
COMMIT;
"

# Create indexes for release column
print_progress "Creating indexes for release column"
execute_sql "
BEGIN;
CREATE INDEX index_taxa_release ON taxa USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observers_release ON observers USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observations_release ON observations USING GIN (to_tsvector('simple', release));
CREATE INDEX index_photos_release ON photos USING GIN (to_tsvector('simple', release));
COMMIT;
"

print_progress "Release column added and populated successfully"
            </file>
            <dir path="gest/t/v0/utils/elevation">
              <file path="ingest/v0/utils/elevation/create_elevation_table.sh">
#!/bin/bash
#
# create_elevation_table.sh
#
# Creates or ensures the elevation_raster table exists in the target database,
# using create_elevation_table.sql. Index is also ensured via IF NOT EXISTS.
#
# Usage:
#   create_elevation_table.sh <DB_NAME> <DB_USER> <DB_CONTAINER>
#
# Environment Variables:
#   - BASE_DIR (optional if create_elevation_table.sql is elsewhere)
# 
# This script relies on helper functions from functions.sh for consistency.

set -euo pipefail

# ------------------------------------------------------------------------------
# 1. Parse arguments
# ------------------------------------------------------------------------------
if [ "$#" -lt 3 ]; then
  echo "Usage: $0 <DB_NAME> <DB_USER> <DB_CONTAINER>"
  exit 1
fi

DB_NAME="$1"
DB_USER="$2"
DB_CONTAINER="$3"

# If BASE_DIR not set, default to current script's grandparent
BASE_DIR="${BASE_DIR:-"$(cd "$(dirname "$0")/../../.." && pwd)"}"

# ------------------------------------------------------------------------------
# 2. Source shared functions
# ------------------------------------------------------------------------------
# We assume common/functions.sh is up two levels from 'elevation' subdir
source "${BASE_DIR}/common/functions.sh"

# ------------------------------------------------------------------------------
# 3. Run
# ------------------------------------------------------------------------------
print_progress "Creating elevation_raster table in database '${DB_NAME}'"

SQL_FILE="${BASE_DIR}/utils/elevation/create_elevation_table.sql"
if [ ! -f "${SQL_FILE}" ]; then
  echo "Error: Missing SQL file at ${SQL_FILE}"
  exit 1
fi

# We do not have a dedicated 'execute_sql_file' helper, so we cat + pipe:
cat "${SQL_FILE}" | docker exec -i "${DB_CONTAINER}" psql -U "${DB_USER}" -d "${DB_NAME}"

print_progress "elevation_raster table is created or already exists."
send_notification "[OK] Created/verified elevation_raster table in ${DB_NAME}"

              </file>
              <file path="ingest/v0/utils/elevation/create_elevation_table.sql">
-- create_elevation_table.sql
--
-- Creates the "elevation_raster" table to store MERIT DEM raster data,
-- along with a GIST index for efficient spatial lookups.

CREATE TABLE IF NOT EXISTS elevation_raster (
    rid SERIAL PRIMARY KEY,
    rast raster,
    filename TEXT
);

CREATE INDEX IF NOT EXISTS elevation_raster_st_convexhull_idx
    ON elevation_raster
    USING gist (ST_ConvexHull(rast));

              </file>
              <file path="ingest/v0/utils/elevation/load_dem.sh">
#!/bin/bash
#
# load_dem.sh
#
# Loads MERIT DEM tiles from .tar archives into the elevation_raster table via raster2pgsql.
#
# Usage:
#   load_dem.sh <DEM_DIR> <DB_NAME> <DB_USER> <DB_CONTAINER> <EPSG> <TILE_SIZE>
#
# Example:
#   load_dem.sh /datasets/dem/merit ibrida-v0-r1 postgres ibridaDB 4326 100x100
#
# Notes:
#   - Expects .tar files each containing .tif(s).
#   - Uses raster2pgsql to tile (-t <TILE_SIZE>) and index (-I).
#   - Calls print_progress and send_notification from functions.sh.

set -euo pipefail

# ------------------------------------------------------------------------------
# 1. Parse arguments
# ------------------------------------------------------------------------------
if [ "$#" -lt 6 ]; then
  echo "Usage: $0 <DEM_DIR> <DB_NAME> <DB_USER> <DB_CONTAINER> <EPSG> <TILE_SIZE>"
  exit 1
fi

# Host-side paths
HOST_DEM_DIR="$1"
DB_NAME="$2"
DB_USER="$3"
DB_CONTAINER="$4"
EPSG="$5"
TILE_SIZE="$6"

# Container-side paths (translate from host paths)
CONTAINER_DEM_DIR="/dem/merit"  # /datasets/dem/merit -> /dem/merit
CONTAINER_TEMP_DIR="/dem/merit/temp"

# If BASE_DIR not set, default to current script's grandparent
BASE_DIR="${BASE_DIR:-"$(cd "$(dirname "$0")/../../.." && pwd)"}"

# ------------------------------------------------------------------------------
# 2. Source shared functions
# ------------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ------------------------------------------------------------------------------
# 3. Prepare temporary directory (use host path for mkdir)
# ------------------------------------------------------------------------------
TEMP_DIR="${HOST_DEM_DIR}/temp"
ensure_directory "${TEMP_DIR}"

print_progress "Loading DEM data from ${HOST_DEM_DIR} into ${DB_NAME}"

# ------------------------------------------------------------------------------
# 4. Loop over .tar files, extract, load via raster2pgsql
# ------------------------------------------------------------------------------
for tarfile in "${HOST_DEM_DIR}"/*.tar; do
  if [ ! -f "${tarfile}" ]; then
    # If no .tar files exist, skip
    continue
  fi

  # Extract (using host paths)
  print_progress "Extracting ${tarfile}..."
  tar -xf "${tarfile}" -C "${TEMP_DIR}"

  # Find any .tif file(s) (using host paths)
  found_tifs=($(find "${TEMP_DIR}" -type f -name '*.tif'))
  if [ "${#found_tifs[@]}" -eq 0 ]; then
    echo "Warning: No .tif found in ${tarfile}; skipping."
    rm -rf "${TEMP_DIR:?}"/*
    continue
  fi

  # Load each TIF
  for tiffile in "${found_tifs[@]}"; do
    print_progress "Loading ${tiffile} into PostGIS (EPSG=${EPSG}, tile=${TILE_SIZE})"
    
    # Convert host path to container path for the TIF file
    CONTAINER_TIFFILE="${CONTAINER_TEMP_DIR}/$(basename "$(dirname "${tiffile}")")/$(basename "${tiffile}")"
    
    # Run raster2pgsql inside the container with container paths
    docker exec "${DB_CONTAINER}" raster2pgsql -a -s "${EPSG}" -t "${TILE_SIZE}" -I "${CONTAINER_TIFFILE}" elevation_raster \
      | docker exec -i "${DB_CONTAINER}" psql -U "${DB_USER}" -d "${DB_NAME}"

    send_notification "[OK] Loaded DEM tile: $(basename "${tiffile}") into ${DB_NAME}"
  done

  # Cleanup extracted files (using host path)
  rm -rf "${TEMP_DIR:?}"/*
done

print_progress "DEM loading complete."
send_notification "[OK] Completed loading DEM data into ${DB_NAME}"

              </file>
              <file path="ingest/v0/utils/elevation/main.sh">
#!/bin/bash
#
# main.sh
#
# High-level orchestration for setting up elevation data:
#   1) create_elevation_table.sh
#   2) load_dem.sh
#   3) update_elevation.sh
#
# Usage:
#   main.sh <DB_NAME> <DB_USER> <DB_CONTAINER> <DEM_DIR> <NUM_PROCESSES> [EPSG=4326] [TILE_SIZE=100x100]
#
# Example:
#   main.sh ibrida-v0-r1 postgres ibridaDB /datasets/dem/merit 16 4326 100x100
#
# Notes:
#   - Called by wrapper.sh typically, but can be run standalone.
#   - We rely on the geometry column (observations.geom) to already exist!

set -euo pipefail

# ------------------------------------------------------------------------------
# 1. Parse arguments
# ------------------------------------------------------------------------------
if [ "$#" -lt 5 ]; then
  echo "Usage: $0 <DB_NAME> <DB_USER> <DB_CONTAINER> <DEM_DIR> <NUM_PROCESSES> [EPSG] [TILE_SIZE]"
  exit 1
fi

DB_NAME="$1"
DB_USER="$2"
DB_CONTAINER="$3"
DEM_DIR="$4"
NUM_PROCESSES="$5"
EPSG="${6:-4326}"
TILE_SIZE="${7:-100x100}"

# If BASE_DIR not set, default to current script's grandparent
BASE_DIR="${BASE_DIR:-"$(cd "$(dirname "$0")/../../.." && pwd)"}"

# ------------------------------------------------------------------------------
# 2. Source shared functions
# ------------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

print_progress "=== Elevation: main.sh start ==="
send_notification "[INFO] Starting elevation main flow for DB=${DB_NAME}"

# ------------------------------------------------------------------------------
# 3. Step 1: Create elevation_raster table
# ------------------------------------------------------------------------------
"${BASE_DIR}/utils/elevation/create_elevation_table.sh" \
  "${DB_NAME}" \
  "${DB_USER}" \
  "${DB_CONTAINER}"

# ------------------------------------------------------------------------------
# 4. Step 2: Load DEM data
# ------------------------------------------------------------------------------
"${BASE_DIR}/utils/elevation/load_dem.sh" \
  "${DEM_DIR}" \
  "${DB_NAME}" \
  "${DB_USER}" \
  "${DB_CONTAINER}" \
  "${EPSG}" \
  "${TILE_SIZE}"

# ------------------------------------------------------------------------------
# 5. Step 3: Update observations with elevation
# ------------------------------------------------------------------------------
"${BASE_DIR}/utils/elevation/update_elevation.sh" \
  "${DB_NAME}" \
  "${DB_USER}" \
  "${DB_CONTAINER}" \
  "${NUM_PROCESSES}"

print_progress "=== Elevation: main.sh complete ==="
send_notification "[OK] Elevation pipeline complete for DB=${DB_NAME}"

              </file>
              <file path="ingest/v0/utils/elevation/update_elevation.sh">
#!/bin/bash
#
# update_elevation.sh
#
# Populates observations.elevation_meters from the elevation_raster table
# using ST_Value(raster, geometry). Runs in parallel for efficiency.
#
# Usage:
#   update_elevation.sh <DB_NAME> <DB_USER> <DB_CONTAINER> <NUM_PROCESSES>
#
# Environment Variables (expected):
#   - We rely on functions.sh for print_progress, execute_sql, etc.
#
# Notes:
#   - If ST_Value(...) is out of coverage (ocean or no-data area),
#     elevation_meters will remain NULL.
#   - We call VACUUM ANALYZE at the end to optimize performance.

set -euo pipefail

# ------------------------------------------------------------------------------
# 1. Parse arguments
# ------------------------------------------------------------------------------
if [ "$#" -lt 4 ]; then
  echo "Usage: $0 <DB_NAME> <DB_USER> <DB_CONTAINER> <NUM_PROCESSES>"
  exit 1
fi

DB_NAME="$1"
DB_USER="$2"
DB_CONTAINER="$3"
NUM_PROCESSES="$4"

# If BASE_DIR not set, default to current script's grandparent
BASE_DIR="${BASE_DIR:-"$(cd "$(dirname "$0")/../../.." && pwd)"}"

# ------------------------------------------------------------------------------
# 2. Source shared functions
# ------------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ------------------------------------------------------------------------------
# 3. Ensure the column 'elevation_meters' exists on observations
# ------------------------------------------------------------------------------
print_progress "Ensuring observations.elevation_meters column exists"
execute_sql "ALTER TABLE observations ADD COLUMN IF NOT EXISTS elevation_meters numeric(10,2);" 

# ------------------------------------------------------------------------------
# 4. Determine row count, compute batch size
# ------------------------------------------------------------------------------
print_progress "Determining row count in observations"
TOTAL_ROWS=$(docker exec "${DB_CONTAINER}" psql -U "${DB_USER}" -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM observations;" | tr -d ' ')
if [ -z "${TOTAL_ROWS}" ] || [ "${TOTAL_ROWS}" -eq 0 ]; then
  echo "No observations found. Skipping elevation update."
  exit 0
fi

BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES + 1))
send_notification "[INFO] Elevation update starting: ${TOTAL_ROWS} rows in total, batch size=${BATCH_SIZE}"

# ------------------------------------------------------------------------------
# 5. Parallel update function
# ------------------------------------------------------------------------------
update_chunk() {
  local offset="$1"
  local limit="$2"
  local chunk_id="$3"

  print_progress "Elevation update chunk #${chunk_id} (offset=${offset}, limit=${limit})"

  # Build an inline SQL
  local sql="
    UPDATE observations
    SET elevation_meters = ST_Value(er.rast, observations.geom)
    FROM elevation_raster er
    WHERE ST_Intersects(er.rast, observations.geom)
      AND observations.ctid IN (
        SELECT ctid FROM observations
        ORDER BY ctid
        OFFSET ${offset}
        LIMIT ${limit}
      );
  "

  # Run the update
  execute_sql "${sql}"

  print_progress "Chunk #${chunk_id} complete"
  send_notification "[OK] Elevation update chunk #${chunk_id} done"
}

# ------------------------------------------------------------------------------
# 6. Launch parallel updates
# ------------------------------------------------------------------------------
pids=()
for ((i=0; i<NUM_PROCESSES; i++)); do
  OFFSET=$((i * BATCH_SIZE))
  update_chunk "${OFFSET}" "${BATCH_SIZE}" "${i}" &
  pids+=($!)
done

for pid in "${pids[@]}"; do
  wait "$pid"
done

print_progress "All elevation updates completed for ${TOTAL_ROWS} rows."
send_notification "[OK] All elevation updates completed"

# ------------------------------------------------------------------------------
# 7. Final VACUUM ANALYZE
# ------------------------------------------------------------------------------
print_progress "Running VACUUM ANALYZE on observations..."
execute_sql "VACUUM ANALYZE observations;"
send_notification "[OK] VACUUM ANALYZE on observations complete"

              </file>
              <file path="ingest/v0/utils/elevation/wrapper.sh">
#!/bin/bash
#
# wrapper.sh
#
# A wrapper that sets environment variables / parameters and calls the
# elevation main.sh. This is useful for a one-off scenario on an existing DB,
# or for hooking into your ingestion flow after geometry is set.
#
# Usage:
#   wrapper.sh
#   (no arguments; you can edit the variables in-script)
#
# Example:
#   chmod +x wrapper.sh
#   ./wrapper.sh
#
# Required Tools:
#   - Docker
#   - raster2pgsql
#

set -euo pipefail

# ------------------------------------------------------------------------------
# 1. Configuration
# ------------------------------------------------------------------------------
export DB_NAME="ibrida-v0-r1"
export DB_USER="postgres"
export DB_CONTAINER="ibridaDB"
export DEM_DIR="/datasets/dem/merit"
export NUM_PROCESSES="16"
export EPSG="4326"
export TILE_SIZE="100x100"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

# ------------------------------------------------------------------------------
# 2. Logging / Sourcing
# ------------------------------------------------------------------------------
LOG_FILE="$(dirname "$(readlink -f "$0")")/wrapper_$(date +%Y%m%d_%H%M%S).log"
echo "Starting elevation wrapper at $(date)" | tee -a "${LOG_FILE}"

source "${BASE_DIR}/common/functions.sh"

print_progress "Invoking elevation main script" | tee -a "${LOG_FILE}"
send_notification "[INFO] Elevation wrapper invoked"

# ------------------------------------------------------------------------------
# 3. Call main.sh
# ------------------------------------------------------------------------------
"${BASE_DIR}/utils/elevation/main.sh" \
  "${DB_NAME}" \
  "${DB_USER}" \
  "${DB_CONTAINER}" \
  "${DEM_DIR}" \
  "${NUM_PROCESSES}" \
  "${EPSG}" \
  "${TILE_SIZE}" \
  2>&1 | tee -a "${LOG_FILE}"

print_progress "Elevation wrapper complete" | tee -a "${LOG_FILE}"
send_notification "[OK] Elevation wrapper flow finished successfully"

              </file>
            </dir>
          </dir>
        </dir>
      </dir>
    </dir>
    <dir path="m">
      <dir path="m/m">
        <file path="dem/download_merit.sh">
#!/bin/bash

# Configuration
USERNAME="hydrography"
PASSWORD="rivernetwork"
BASE_URL="http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_Hydro/distribute/v1.0"
DEST_DIR="/datasets/dem/merit"
LOG_FILE="${DEST_DIR}/download.log"

# Create destination directory
mkdir -p "${DEST_DIR}"

# Initialize log file
echo "Starting MERIT DEM download at $(date)" > "${LOG_FILE}"

# Function to download a single file with retry logic
download_file() {
    local filename="$1"
    local attempts=3
    local wait_time=30

    for ((i=1; i<=attempts; i++)); do
        echo "Downloading ${filename} (attempt ${i}/${attempts})..." | tee -a "${LOG_FILE}"
        
        if wget --quiet --user="${USERNAME}" \
                --password="${PASSWORD}" \
                --no-check-certificate \
                -P "${DEST_DIR}" \
                "${BASE_URL}/${filename}"; then
            echo "Successfully downloaded ${filename}" | tee -a "${LOG_FILE}"
            return 0
        else
            echo "Failed to download ${filename} on attempt ${i}" | tee -a "${LOG_FILE}"
            if [ $i -lt $attempts ]; then
                echo "Waiting ${wait_time} seconds before retry..." | tee -a "${LOG_FILE}"
                sleep ${wait_time}
            fi
        fi
    done
    
    echo "Failed to download ${filename} after ${attempts} attempts" | tee -a "${LOG_FILE}"
    return 1
}

# List of all valid files to download (excluding "no data" regions)
declare -a files=(
    # N60-N90
    "elv_n60w180.tar" "elv_n60w150.tar" "elv_n60w120.tar" "elv_n60w090.tar" "elv_n60w060.tar" "elv_n60w030.tar"
    "elv_n60e000.tar" "elv_n60e030.tar" "elv_n60e060.tar" "elv_n60e090.tar" "elv_n60e120.tar" "elv_n60e150.tar"
    # N30-N60
    "elv_n30w180.tar" "elv_n30w150.tar" "elv_n30w120.tar" "elv_n30w090.tar" "elv_n30w060.tar" "elv_n30w030.tar"
    "elv_n30e000.tar" "elv_n30e030.tar" "elv_n30e060.tar" "elv_n30e090.tar" "elv_n30e120.tar" "elv_n30e150.tar"
    # N00-N30
    "elv_n00w180.tar" "elv_n00w120.tar" "elv_n00w090.tar" "elv_n00w060.tar" "elv_n00w030.tar"
    "elv_n00e000.tar" "elv_n00e030.tar" "elv_n00e060.tar" "elv_n00e090.tar" "elv_n00e120.tar" "elv_n00e150.tar"
    # S30-N00
    "elv_s30w180.tar" "elv_s30w150.tar" "elv_s30w120.tar" "elv_s30w090.tar" "elv_s30w060.tar" "elv_s30w030.tar"
    "elv_s30e000.tar" "elv_s30e030.tar" "elv_s30e060.tar" "elv_s30e090.tar" "elv_s30e120.tar" "elv_s30e150.tar"
    # S60-S30
    "elv_s60w180.tar" "elv_s60w090.tar" "elv_s60w060.tar" "elv_s60w030.tar"
    "elv_s60e000.tar" "elv_s60e030.tar" "elv_s60e060.tar" "elv_s60e090.tar" "elv_s60e120.tar" "elv_s60e150.tar"
)

# Download all files
failed_files=()
for file in "${files[@]}"; do
    if ! download_file "${file}"; then
        failed_files+=("${file}")
    fi
    # Small delay between downloads
    sleep 2
done

# Report results
echo -e "\nDownload Summary:" | tee -a "${LOG_FILE}"
echo "Total files attempted: ${#files[@]}" | tee -a "${LOG_FILE}"
echo "Failed downloads: ${#failed_files[@]}" | tee -a "${LOG_FILE}"

if [ ${#failed_files[@]} -gt 0 ]; then
    echo "Failed files:" | tee -a "${LOG_FILE}"
    printf '%s\n' "${failed_files[@]}" | tee -a "${LOG_FILE}"
    exit 1
fi

echo "All downloads completed successfully at $(date)" | tee -a "${LOG_FILE}"
        </file>
        <file path="dem/download_merit_parallel.sh" line_interval="25">
#!/bin/bash
#
# download_merit_parallel.sh
#
# Downloads MERIT DEM files in parallel with controlled concurrency. 
# Retains retry logic for robustness.

# --- Configuration ---

# Credentials
USERNAME="hydrography"
PASSWORD="rivernetwork"

# Base URL for the dataset
BASE_URL="http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_Hydro/distribute/v1.0"

# Destination directory for downloaded files
DEST_DIR="/datasets/dem/merit"

# Main log file
LOG_FILE="${DEST_DIR}/download.log"

# Default number of parallel processes (override by passing an argument)
NUM_PARALLEL=4
#|LN|25|
if [ -n "$1" ]; then
  NUM_PARALLEL="$1"
fi

# Maximum number of retry attempts for each file
MAX_ATTEMPTS=3

# Seconds to wait between retries
RETRY_WAIT=180

# Seconds to wait between spawn of parallel tasks (to avoid hammering the server)
# You can set this to 0 if you prefer no delay at all.
SPAWN_DELAY=45


# --- Setup ---

# Create destination directory
mkdir -p "${DEST_DIR}"

# Initialize log file
echo "Starting MERIT DEM download at $(date)" > "${LOG_FILE}"
echo "Using concurrency level: ${NUM_PARALLEL}" | tee -a "${LOG_FILE}"

# Temporary file to track failures across parallel processes
#|LN|50|
FAIL_FILE=$(mktemp)
touch "$FAIL_FILE"


# --- Helper Functions ---

# Thread-safe logging function for general messages.
# In extreme concurrency, lines may still interleave. For robust locking,
# consider using 'flock'. We keep it simple here.
log_message() {
  local msg="$1"
  echo "$msg" | tee -a "$LOG_FILE"
}

# Download a single file with retry logic
# Usage: download_file "filename.tar"
download_file() {
    local filename="$1"
    local attempt

    for ((attempt=1; attempt<=MAX_ATTEMPTS; attempt++)); do
        log_message "Downloading ${filename} (attempt ${attempt}/${MAX_ATTEMPTS})..."
        
        if wget --quiet --user="${USERNAME}" \
                --password="${PASSWORD}" \
#|LN|75|
                --no-check-certificate \
                -P "${DEST_DIR}" \
                "${BASE_URL}/${filename}"; then
            log_message "Successfully downloaded ${filename}"
            return 0
        else
            log_message "Failed to download ${filename} on attempt ${attempt}"
            if [ $attempt -lt $MAX_ATTEMPTS ]; then
                log_message "Waiting ${RETRY_WAIT} seconds before retry..."
                sleep "${RETRY_WAIT}"
            fi
        fi
    done
    
    # If we reached here, all attempts failed
    echo "${filename}" >> "${FAIL_FILE}"
    log_message "Failed to download ${filename} after ${MAX_ATTEMPTS} attempts"
    return 1
}


# --- File List ---

# List of all valid files to download (excluding "no data" regions)
files=(
#|LN|100|
    # # N60-N90
    "elv_n60w180.tar" "elv_n60w150.tar" "elv_n60w120.tar" "elv_n60w090.tar" "elv_n60w060.tar" "elv_n60w030.tar"
    "elv_n60e000.tar" "elv_n60e030.tar" "elv_n60e060.tar" "elv_n60e090.tar" "elv_n60e120.tar" "elv_n60e150.tar"
    # # N30-N60
    "elv_n30w180.tar" "elv_n30w150.tar" "elv_n30w120.tar" "elv_n30w090.tar" "elv_n30w060.tar" "elv_n30w030.tar"
    "elv_n30e000.tar" "elv_n30e030.tar" "elv_n30e060.tar" "elv_n30e090.tar" "elv_n30e120.tar" "elv_n30e150.tar"
    # N00-N30
    "elv_n00w180.tar" "elv_n00w120.tar" "elv_n00w090.tar" "elv_n00w060.tar" "elv_n00w030.tar"
    "elv_n00e000.tar" "elv_n00e030.tar" "elv_n00e060.tar" "elv_n00e090.tar" "elv_n00e120.tar" "elv_n00e150.tar"
    # S30-N00
    "elv_s30w180.tar" "elv_s30w150.tar" "elv_s30w120.tar" "elv_s30w090.tar" "elv_s30w060.tar" "elv_s30w030.tar"
    "elv_s30e000.tar" "elv_s30e030.tar" "elv_s30e060.tar" "elv_s30e090.tar" "elv_s30e120.tar" "elv_s30e150.tar"
    # S60-S30
    "elv_s60w180.tar" "elv_s60w090.tar" "elv_s60w060.tar" "elv_s60w030.tar"
    "elv_s60e000.tar" "elv_s60e030.tar" "elv_s60e060.tar" "elv_s60e090.tar" "elv_s60e120.tar" "elv_s60e150.tar"
)

total_files=${#files[@]}
log_message "Total files to download: ${total_files}"

# --- Parallel Download Loop ---

pids=()
count=0

#|LN|125|
for file in "${files[@]}"; do
    # Start the download in a background subshell
    (
      download_file "${file}"
    ) &
    pids+=($!)
    
    # Throttle concurrency
    ((count++))
    if [ "$((count % NUM_PARALLEL))" -eq 0 ]; then
        # Wait for at least one job to finish before spawning new ones
        wait -n
    fi

    # Optional small delay to prevent saturating the remote server
    sleep "${SPAWN_DELAY}"
done

# Wait for all remaining jobs to finish
wait

# --- Summarize ---

# Gather any failed files from the temp failure file
mapfile -t failed_files < "$FAIL_FILE"
#|LN|150|
rm -f "$FAIL_FILE"

num_failed=${#failed_files[@]}
log_message ""
log_message "Download Summary:"
log_message "Total files attempted: ${total_files}"
log_message "Failed downloads: ${num_failed}"

if [ ${num_failed} -gt 0 ]; then
    log_message "Failed files:"
    for f in "${failed_files[@]}"; do
        log_message "  - $f"
    done
    log_message "Check the log file for more details: ${LOG_FILE}"
    exit 1
fi

log_message "All downloads completed successfully at $(date)"
exit 0
        </file>
      </dir>
    </dir>
    <dir path="port">
      <dir path="port/t">
        <dir path="port/t/v0">
          <dir path="port/t/v0/common">
            <file path="export/v0/common/clade_defns.sh" line_interval="25">
#!/bin/bash
# ------------------------------------------------------------------------------
# clade_defns.sh
# ------------------------------------------------------------------------------
# This file defines the integer-based filtering expressions for macroclades,
# clades, and metaclades, referencing columns in "expanded_taxa".
#
# Usage:
#   source clade_defns.sh
#   Then pick a macroclade (MACROCLADE="..."), or a clade (CLADE="..."),
#   or a metaclade (METACLADE="...") in your environment, and the
#   cladistic.sh script will build a condition from one of the arrays below.
#
# Example:
#   MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'
#   CLADES["insecta"]='("L50_taxonID" = 47158)'
#   METACLADES["primary_terrestrial_arthropoda"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'
#
# Be sure to substitute the correct taxonIDs for your local database!
# ------------------------------------------------------------------------------
#
# Sections in this file:
#   1) Macroclade Definitions
#   2) Clade Definitions
#|LN|25|
#   3) Metaclade Definitions
#   4) get_clade_condition() helper
#
# NOTE: We do NOT remove any existing definitions or comments.

# ---[ 1) Macroclade Definitions ]---------------------------------------------
# Typically for kingdom-level (L70) or phylum-level (L60) anchors.

declare -A MACROCLADES

# 1) Arthropoda => phylum at L60 = 47120
MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'

# 2) Chordata => phylum at L60 = 2
MACROCLADES["chordata"]='("L60_taxonID" = 2)'

# 3) Plantae => kingdom at L70 = 47126
MACROCLADES["plantae"]='("L70_taxonID" = 47126)'

# 4) Fungi => kingdom at L70 = 47170
MACROCLADES["fungi"]='("L70_taxonID" = 47170)'

# (Optional) If you consider Actinopterygii, Mammalia, Reptilia, etc.
# to be "macroclades," you may define them here instead of in CLADES.
# For instance:
#|LN|50|
#   MACROCLADES["mammalia"]='("L50_taxonID" = 40151)'


# ---[ 2) Clade Definitions ]--------------------------------------------------
# Typically for class-level (L50), order-level (L40), or narrower taxonomic groups.
# single-root, so functionally equivalent to METACLADES.

declare -A CLADES

# -- Plant Clades (Subphylum and Class levels) --
# -- Plant Subphylum (L57) --
CLADES["angiospermae"]='("L57_taxonID" = 47125)' # flowering plants

# -- Plant Classes (L50) --
CLADES["liliopsida"]='("L50_taxonID" = 47163)'    # monocots
CLADES["magnoliopsida"]='("L50_taxonID" = 47124)' # dicots

# -- Class-level (L50) Examples --
CLADES["actinopterygii"]='("L50_taxonID" = 47178)'
CLADES["amphibia"]='("L50_taxonID" = 20978)'
CLADES["arachnida"]='("L50_taxonID" = 47119)'
CLADES["aves"]='("L50_taxonID" = 3)'
CLADES["insecta"]='("L50_taxonID" = 47158)'
CLADES["mammalia"]='("L50_taxonID" = 40151)'
CLADES["reptilia"]='("L50_taxonID" = 26036)'
#|LN|75|

# -- Order-level (L40) Examples --
CLADES["testudines"]='("L40_taxonID" = 39532)'
CLADES["crocodylia"]='("L40_taxonID" = 26039)'
CLADES["coleoptera"]='("L40_taxonID" = 47208)'
CLADES["lepidoptera"]='("L40_taxonID" = 47157)'
CLADES["hymenoptera"]='("L40_taxonID" = 47201)'
CLADES["hemiptera"]='("L40_taxonID" = 47744)'
CLADES["orthoptera"]='("L40_taxonID" = 47651)'
CLADES["odonata"]='("L40_taxonID" = 47792)'
CLADES["diptera"]='("L40_taxonID" = 47822)'

# -- Additional Named Groups (Suborders, Clade Subsets, etc.) --
# Pterygota => The DB shows two taxonIDs (184884, 418641) plus
# another entry with L40_taxonID=48796. We combine them with OR:
CLADES["pterygota"]='("taxonID" = 184884 OR "taxonID" = 418641 OR "L40_taxonID" = 48796)'

# Phasmatodea => Not found in your query results. If/when you know its ID,
# you can fill it in here:
# CLADES["phasmatodea"]='("L40_taxonID" = ???)'

# Subclades within Hymenoptera (all share L40_taxonID=47201).
# Typically, referencing the top-level order is "hymenoptera"
# while these might be more specific anchor taxa:
CLADES["aculeata"]='("taxonID" = 326777)'
#|LN|100|
CLADES["apoidea"]='("taxonID" = 47222)'
CLADES["formicidae"]='("taxonID" = 47336)'
CLADES["vespoidea"]='("taxonID" = 48740)'
CLADES["vespidae"]='("taxonID" = 52747)'


# ---[ 3) Metaclade Definitions ]----------------------------------------------
# Multi-root or cross-macroclade definitions. Compose bigger groups using OR.

declare -A METACLADES

# Example 1: primary_terrestrial_arthropoda (pta) => Insecta OR Arachnida.
METACLADES["pta"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'

# Example 2: flying_vertebrates => Birds (aves) OR Bats (chiroptera)
# METACLADES["flying_vertebrates"]='("L50_taxonID" = 3 OR "L40_taxonID" = 7721)'

# Example 3: nonavian_reptiles => reptilia minus birds.
# METACLADES["nonavian_reptiles"]='("L50_taxonID" = 26036 AND "L50_taxonID" != 3)'


# ---[ 4) get_clade_condition() Helper ]-----------------------------------------
# Picks the correct expression given environment variables (METACLADE, CLADE,
# MACROCLADE). This is used by cladistic.sh to filter rows.

#|LN|125|
function get_clade_condition() {
  local condition

  # 1) If METACLADE is set (and found in METACLADES), return that
  if [[ -n "${METACLADE}" && -n "${METACLADES[${METACLADE}]}" ]]; then
    condition="${METACLADES[${METACLADE}]}"
    echo "${condition}"
    return
  fi

  # 2) Else if CLADE is set
  if [[ -n "${CLADE}" && -n "${CLADES[${CLADE}]}" ]]; then
    condition="${CLADES[${CLADE}]}"
    echo "${condition}"
    return
  fi

  # 3) Else if MACROCLADE is set
  if [[ -n "${MACROCLADE}" && -n "${MACROCLADES[${MACROCLADE}]}" ]]; then
    condition="${MACROCLADES[${MACROCLADE}]}"
    echo "${condition}"
    return
  fi

  # 4) Fallback: no recognized key => no filter
#|LN|150|
  echo "TRUE"
}

export -f get_clade_condition
            </file>
            <file path="export/v0/common/clade_helpers.sh" line_interval="25">
#!/bin/bash
# ------------------------------------------------------------------------------
# clade_helpers.sh
# ------------------------------------------------------------------------------
# This file contains helper functions for multi-root/metaclade logic,
# rank boundary calculations, and advanced taxon-ancestry checks.
#
# Proposed usage:
#   1) "parse_clade_expression()" to parse user-provided condition strings
#      (e.g. "L50_taxonID=123 OR L40_taxonID=9999") into structured data.
#   2) "check_root_independence()" to verify that each root is truly disjoint
#      (none is an ancestor of another).
#   3) "get_major_rank_floor()" to compute the next-lower major-rank boundary
#      if user does not want to include minor ranks. Typically used if the root
#      is e.g. 57 => 50. If user includes minor ranks, we skip the rounding.
#
# NOTE: We do not forcibly integrate with existing "get_clade_condition()"
# in clade_defns.sh. Instead, you can call parse_clade_expression() if you
# want to do deeper multi-root logic.
#
# Implementation details:
#   - We store a reference map from "L<number>_taxonID" to the numeric rank
#     (e.g. "L50_taxonID" => 50). If the user requests minor ranks, we do not
#     round them down to the multiple of 10.
#|LN|25|
#   - We rely on "expanded_taxa" for ancestry checks. The "check_root_independence()"
#     function is conceptual: it gathers each root's entire ancestry (e.g. ~30
#     columns from L5..L70) and ensures no overlap among root sets.
#
# ------------------------------------------------------------------------------
#
# Exports:
#   - parse_clade_expression()
#   - check_root_independence()
#   - get_major_rank_floor()
#

# -------------------------------------------------------------
# A) Internal reference: Maps "L50_taxonID" => 50, "L40_taxonID" => 40, etc.
# -------------------------------------------------------------
declare -A RANKLEVEL_MAP=(
  ["L5_taxonID"]="5"
  ["L10_taxonID"]="10"
  ["L11_taxonID"]="11"
  ["L12_taxonID"]="12"
  ["L13_taxonID"]="13"
  ["L15_taxonID"]="15"
  ["L20_taxonID"]="20"
  ["L24_taxonID"]="24"
  ["L25_taxonID"]="25"
#|LN|50|
  ["L26_taxonID"]="26"
  ["L27_taxonID"]="27"
  ["L30_taxonID"]="30"
  ["L32_taxonID"]="32"
  ["L33_taxonID"]="33"
  ["L33_5_taxonID"]="33.5"
  ["L34_taxonID"]="34"
  ["L34_5_taxonID"]="34.5"
  ["L35_taxonID"]="35"
  ["L37_taxonID"]="37"
  ["L40_taxonID"]="40"
  ["L43_taxonID"]="43"
  ["L44_taxonID"]="44"
  ["L45_taxonID"]="45"
  ["L47_taxonID"]="47"
  ["L50_taxonID"]="50"
  ["L53_taxonID"]="53"
  ["L57_taxonID"]="57"
  ["L60_taxonID"]="60"
  ["L67_taxonID"]="67"
  ["L70_taxonID"]="70"
  # stateofmatter => 100, if we had that in expanded_taxa
)

# --------------------------------------------------------------------------
#|LN|75|
# parse_clade_expression()
# --------------------------------------------------------------------------
# Parses a SQL-like expression containing L{XX}_taxonID conditions into an array 
# of "rank=taxonID" pairs.
#
# Expected usage:
#   - We typically pass the result of get_clade_condition(), which looks like:
#     ("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)
#   - The caller captures the results in an array:
#     roots=( $(parse_clade_expression "$clade_condition") )
#
# Processing steps:
#   1) Removes parentheses and double quotes
#   2) Splits on " OR " to handle multiple conditions
#   3) For each condition:
#      - Splits on '=' to get the LHS and RHS
#      - Extracts the rank number from L{XX}_taxonID pattern
#      - Pairs the rank with the taxonID
#
# Return format:
#   Space-separated strings in the form "rank=taxonID", e.g.:
#   "50=47158" "50=47119"
#
# Examples:
#   Input:  "L50_taxonID" = 47158
#|LN|100|
#   Output: 50=47158
#
#   Input:  ("L50_taxonID" = 47158 OR "L40_taxonID" = 9999)
#   Output: 50=47158 40=9999
#
# Notes:
#   - Case-insensitive: l50_taxonid and L50_taxonID are equivalent
#   - Spaces around '=' are optional
#   - Ignores any conditions not matching L{XX}_taxonID pattern
#   - Requires numeric taxonID values
# --------------------------------------------------------------------------
function parse_clade_expression() {
  local expr="$1"

  # 1) Remove parentheses and double quotes
  local cleaned_expr
  cleaned_expr="$(echo "$expr" | tr -d '()"')"
  echo "DEBUG [2]: After removing parentheses/quotes: '$cleaned_expr'" >&2

  # 2) Split on " OR " properly using sed
  local or_parts
  or_parts="$(echo "$cleaned_expr" | sed 's/ OR /\n/g')"
  
  local results=()
  
#|LN|125|
  while IFS= read -r part; do
    # Trim spaces and split on =
    local lhs rhs
    IFS='=' read -r lhs rhs <<< "$(echo "$part" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')"
    
    # Remove any remaining spaces
    lhs="$(echo "$lhs" | sed 's/[[:space:]]//g')"
    rhs="$(echo "$rhs" | sed 's/[[:space:]]//g')"
    
    # Extract the numeric part from LXX_taxonID
    if [[ $lhs =~ L([0-9]+)_taxonID ]]; then
      local rank="${BASH_REMATCH[1]}"
      results+=( "${rank}=${rhs}" )
    fi
  done <<< "$or_parts"

  echo "${results[@]}"
}

function check_root_independence() {
  # --------------------------------------------------------------------------
  # check_root_independence()
  #
  # PURPOSE:
  #   Ensures that each root in a multi-root scenario is truly independent,
#|LN|150|
  #   i.e., no root is an ancestor or descendant of another when viewed at
  #   or below the highest rank boundary. For example, if you have two roots
  #   at rank=50 (Insecta, Arachnida), they do share a phylum at rank=60
  #   (Arthropoda), but that is above their rank boundary, so it should NOT
  #   trigger a conflict.
  #
  # IMPLEMENTATION STEPS:
  #   1) Parse the root array (each item = "rank=taxonID", e.g. "50=47158").
  #   2) Find the globalMaxRank = max(r_i for each root).
  #   3) For each root, fetch its single row from expanded_taxa (which includes
  #      columns L5..L70). Then cross-join or left-join each potential ancestor
  #      ID to get that ancestor's rankLevel from expanded_taxa.
  #      Keep only those whose rankLevel <= globalMaxRank.
  #   4) Build a set of taxonIDs for that root (space-separated).
  #   5) Compare each pair of root sets for intersection. If they share a taxonID
  #      that is rankLevel <= globalMaxRank, we treat it as an overlap => return 1.
  #
  #   If no overlap is found among the rank <= globalMaxRank ancestors, return 0.
  #
  # USAGE:
  #   check_root_independence <db_name> <rootArray...>
  #   e.g. check_root_independence "myDB" "50=47158" "50=47119"
  #
  # RETURNS:
  #   0 if no overlap found, 1 if overlap is detected or root is not found.
#|LN|175|
  # --------------------------------------------------------------------------

  local dbName="$1"
  shift
  local roots=("$@")  # e.g. ("50=47158" "50=47119")

  # If there's 0 or 1 root, there's nothing to compare => trivially independent
  if [ "${#roots[@]}" -le 1 ]; then
    return 0
  fi

  # 1) Determine the global max rank among all root definitions
  local globalMaxRank=0
  for r in "${roots[@]}"; do
    local rr="${r%%=*}"
    if (( rr > globalMaxRank )); then
      globalMaxRank="$rr"
    fi
  done

  declare -A rootSets  # will map index => "list of ancestor taxonIDs"

  for i in "${!roots[@]}"; do
    local pair="${roots[$i]}"
    local rank="${pair%%=*}"
#|LN|200|
    local tid="${pair##*=}"

    # We'll do an expanded cross-lateral approach to gather the root's entire
    # L5..L70 columns, then retrieve each ancestor's rankLevel, ignoring any
    # with rankLevel > globalMaxRank.
    #
    # Because we only do ONE row for the root (plus ~30 columns), a single
    # CROSS JOIN to expanded_taxa for each ancestor ID is feasible.

    local sql="
COPY (
  WITH one_root AS (
    SELECT
      e.\"taxonID\" AS sp_id,
      e.\"L5_taxonID\", e.\"L10_taxonID\", e.\"L11_taxonID\", e.\"L12_taxonID\",
      e.\"L13_taxonID\", e.\"L15_taxonID\", e.\"L20_taxonID\", e.\"L24_taxonID\",
      e.\"L25_taxonID\", e.\"L26_taxonID\", e.\"L27_taxonID\", e.\"L30_taxonID\",
      e.\"L32_taxonID\", e.\"L33_taxonID\", e.\"L33_5_taxonID\", e.\"L34_taxonID\",
      e.\"L34_5_taxonID\", e.\"L35_taxonID\", e.\"L37_taxonID\", e.\"L40_taxonID\",
      e.\"L43_taxonID\", e.\"L44_taxonID\", e.\"L45_taxonID\", e.\"L47_taxonID\",
      e.\"L50_taxonID\", e.\"L53_taxonID\", e.\"L57_taxonID\", e.\"L60_taxonID\",
      e.\"L67_taxonID\", e.\"L70_taxonID\"
    FROM expanded_taxa e
    WHERE e.\"taxonID\" = ${tid}
  ),
#|LN|225|
  potential_ancestors AS (
    SELECT sp_id as taxon_id FROM one_root
    UNION ALL
    SELECT anc.\"taxonID\"
    FROM one_root o
    CROSS JOIN LATERAL (VALUES
      (o.\"L5_taxonID\"),(o.\"L10_taxonID\"),(o.\"L11_taxonID\"),(o.\"L12_taxonID\"),
      (o.\"L13_taxonID\"),(o.\"L15_taxonID\"),(o.\"L20_taxonID\"),(o.\"L24_taxonID\"),
      (o.\"L25_taxonID\"),(o.\"L26_taxonID\"),(o.\"L27_taxonID\"),(o.\"L30_taxonID\"),
      (o.\"L32_taxonID\"),(o.\"L33_taxonID\"),(o.\"L33_5_taxonID\"),(o.\"L34_taxonID\"),
      (o.\"L34_5_taxonID\"),(o.\"L35_taxonID\"),(o.\"L37_taxonID\"),(o.\"L40_taxonID\"),
      (o.\"L43_taxonID\"),(o.\"L44_taxonID\"),(o.\"L45_taxonID\"),(o.\"L47_taxonID\"),
      (o.\"L50_taxonID\"),(o.\"L53_taxonID\"),(o.\"L57_taxonID\"),(o.\"L60_taxonID\"),
      (o.\"L67_taxonID\"),(o.\"L70_taxonID\")
    ) x(ancestor_id)
    JOIN expanded_taxa anc ON anc.\"taxonID\" = x.ancestor_id
    WHERE anc.\"rankLevel\" <= ${globalMaxRank}
  )
  SELECT array_agg(potential_ancestors.taxon_id) AS allowed_ancestors
  FROM potential_ancestors
) TO STDOUT WITH CSV HEADER;
"
    local query_result
    query_result="$(execute_sql "$sql")"

#|LN|250|
    # If the query returns only a header line, it might indicate no row found
    # for that root. We can check for 'allowed_ancestors' in the last line.
    local data_line
    data_line="$(echo "$query_result" | tail -n1)"
    if [[ "$data_line" == *"allowed_ancestors"* ]]; then
      echo "ERROR: check_root_independence: No row found or no ancestors for taxonID=${tid}" >&2
      return 1
    fi

    # data_line might look like: {47158,47157,47120,...}
    # We'll remove braces and parse
    local trimmed="$(echo "$data_line" | tr -d '{}')"
    # e.g. 47158,47157,47120
    # We'll split on commas
    IFS=',' read -ra ancestors <<< "$trimmed"

    # Now store them in space-separated form
    rootSets["$i"]="${ancestors[*]}"
  done

  # 3) Compare each pair of sets for intersection
  for ((i=0; i<${#roots[@]}; i++)); do
    for ((j=i+1; j<${#roots[@]}; j++)); do
      local set1=" ${rootSets[$i]} "
      for t2 in ${rootSets[$j]}; do
#|LN|275|
        # If the token t2 appears in set1 => overlap
        # (We assume space-bounded match to avoid partial string hits)
        if [[ "$set1" =~ " $t2 " ]]; then
          echo "ERROR: Overlap detected between root #$i (${roots[$i]}) \
and root #$j (${roots[$j]}) on taxonID=${t2}" >&2
          return 1
        fi
      done
    done
  done

  return 0
}

# -------------------------------------------------------------
# D) get_major_rank_floor()
# -------------------------------------------------------------
# This function returns the next-lower major rank multiple of 10 if we want
# to exclude minor ranks. For instance:
#   if input=57 => output=50
#   if input=50 => output=40
#   if input=70 => output=60
#
# If the user wants minor ranks, we might skip or do partial rounding logic.
# For now, we do a straightforward approach:
#|LN|300|
#
function get_major_rank_floor() {
  local input_rank="$1"
  # We'll do a naive loop:
  # possible major ranks = [70,60,50,40,30,20,10,5]
  # or we can do math: floor((input_rank/10))*10 => but that fails for e.g. 57 => 50 is fine
  # Actually that might be enough, but let's handle if it's exactly a multiple of 10 => we subtract 10 again
  # e.g. 50 => 40, because we want "strictly less than the root rank".
  # If input=57 => floor(57/10)*10=50 => good
  # If input=50 => floor(50/10)*10=50 => but we want 40 => so let's do -10 if exactly multiple

  local base=$(( input_rank/10*10 ))
  if (( $(echo "$input_rank == $base" | bc) == 1 )); then
    # means input is multiple of 10
    base=$(( base-10 ))
  fi
  echo "$base"
}

export -f parse_clade_expression
export -f check_root_independence
export -f get_major_rank_floor
            </file>
            <file path="export/v0/common/cladistic.sh" line_interval="25">
#!/bin/bash
# ------------------------------------------------------------------------------
# cladistic.sh (Revised for Option A: always quoted taxon-level columns)
# ------------------------------------------------------------------------------
# Creates a final observation subset for a user-specified clade/metaclade,
# referencing the "expanded_taxa" table. The input table for this script is
# typically provided in ANCESTORS_OBS_TABLE (set by regional_base.sh).
#
# Steps:
#   1) Validate environment & drop <EXPORT_GROUP>_observations if it exists
#   2) Construct a filtering WHERE clause for research/non-research quality
#   3) Create the <EXPORT_GROUP>_observations table by joining to expanded_taxa
#   4) Optionally wipe partial ranks (L20, L30, L40) if MIN_OCCURRENCES_PER_RANK is set
#   5) Export to CSV via partition-based random sampling
#
# Environment Variables (required):
#   ANCESTORS_OBS_TABLE
#   EXPORT_GROUP
#   DB_CONTAINER, DB_USER, DB_NAME, BASE_DIR, etc.
#
# Optional:
#   RG_FILTER_MODE, MIN_OCCURRENCES_PER_RANK, MAX_RN, PRIMARY_ONLY,
#   INCLUDE_ELEVATION_EXPORT
#
#|LN|25|
# Revision highlights:
#   - All references to columns like L5_taxonID, L10_taxonID, etc. are double-quoted
#     so that Postgres stores them in mixed-case and we can select them without
#     running into case-folding issues.
# ------------------------------------------------------------------------------

set -e

# 1) Source common functions & check ANCESTORS_OBS_TABLE
source "${BASE_DIR}/common/functions.sh"

if [ -z "${ANCESTORS_OBS_TABLE:-}" ]; then
  echo "ERROR: cladistic.sh requires ANCESTORS_OBS_TABLE to be set."
  exit 1
fi

print_progress "cladistic.sh: Using ancestor-based table = ${ANCESTORS_OBS_TABLE}"

TABLE_NAME="${EXPORT_GROUP}_observations"
execute_sql "DROP TABLE IF EXISTS \"${TABLE_NAME}\" CASCADE;"

# ------------------------------------------------------------------------------
# 2) Construct RG filter condition & possibly rewrite "L10_taxonID"
# ------------------------------------------------------------------------------
rg_where_condition="TRUE"
#|LN|50|
rg_l10_col="e.\"L10_taxonID\""  # might become NULL::integer if we do a wipe

case "${RG_FILTER_MODE:-ALL}" in
  "ONLY_RESEARCH")
    rg_where_condition="o.quality_grade='research'"
    ;;
  "ALL")
    rg_where_condition="TRUE"
    ;;
  "ALL_EXCLUDE_SPECIES_NON_RESEARCH")
    rg_where_condition="NOT (o.quality_grade!='research' AND e.\"L10_taxonID\" IS NOT NULL)"
    ;;
  "ONLY_NONRESEARCH")
    rg_where_condition="o.quality_grade!='research'"
    ;;
  "ONLY_NONRESEARCH_EXCLUDE_SPECIES")
    rg_where_condition="(o.quality_grade!='research' AND e.\"L10_taxonID\" IS NULL)"
    ;;
  "ONLY_NONRESEARCH_WIPE_SPECIES_LABEL")
    rg_where_condition="o.quality_grade!='research'"
    rg_l10_col="NULL::integer"
    ;;
  *)
    rg_where_condition="TRUE"
    ;;
#|LN|75|
esac

# ------------------------------------------------------------------------------
# 3) Create <EXPORT_GROUP>_observations by joining ANCESTORS_OBS_TABLE + expanded_taxa
# ------------------------------------------------------------------------------
OBS_COLUMNS="$(get_obs_columns)"  # e.g. observation_uuid, observer_id, latitude, longitude, etc.

# We explicitly alias each expanded_taxa column in quotes, so that Postgres
# stores them in mixed-case (e.g. "L5_taxonID") and we can select them reliably.
EXPANDED_TAXA_COLS="
    e.\"taxonID\"        AS \"expanded_taxonID\",
    e.\"rankLevel\"      AS \"expanded_rankLevel\",
    e.\"name\"           AS \"expanded_name\",
    e.\"L5_taxonID\"     AS \"L5_taxonID\",
    ${rg_l10_col}        AS \"L10_taxonID\",
    e.\"L11_taxonID\"    AS \"L11_taxonID\",
    e.\"L12_taxonID\"    AS \"L12_taxonID\",
    e.\"L13_taxonID\"    AS \"L13_taxonID\",
    e.\"L15_taxonID\"    AS \"L15_taxonID\",
    e.\"L20_taxonID\"    AS \"L20_taxonID\",
    e.\"L24_taxonID\"    AS \"L24_taxonID\",
    e.\"L25_taxonID\"    AS \"L25_taxonID\",
    e.\"L26_taxonID\"    AS \"L26_taxonID\",
    e.\"L27_taxonID\"    AS \"L27_taxonID\",
    e.\"L30_taxonID\"    AS \"L30_taxonID\",
#|LN|100|
    e.\"L32_taxonID\"    AS \"L32_taxonID\",
    e.\"L33_taxonID\"    AS \"L33_taxonID\",
    e.\"L33_5_taxonID\"  AS \"L33_5_taxonID\",
    e.\"L34_taxonID\"    AS \"L34_taxonID\",
    e.\"L34_5_taxonID\"  AS \"L34_5_taxonID\",
    e.\"L35_taxonID\"    AS \"L35_taxonID\",
    e.\"L37_taxonID\"    AS \"L37_taxonID\",
    e.\"L40_taxonID\"    AS \"L40_taxonID\",
    e.\"L43_taxonID\"    AS \"L43_taxonID\",
    e.\"L44_taxonID\"    AS \"L44_taxonID\",
    e.\"L45_taxonID\"    AS \"L45_taxonID\",
    e.\"L47_taxonID\"    AS \"L47_taxonID\",
    e.\"L50_taxonID\"    AS \"L50_taxonID\",
    e.\"L53_taxonID\"    AS \"L53_taxonID\",
    e.\"L57_taxonID\"    AS \"L57_taxonID\",
    e.\"L60_taxonID\"    AS \"L60_taxonID\",
    e.\"L67_taxonID\"    AS \"L67_taxonID\",
    e.\"L70_taxonID\"    AS \"L70_taxonID\"
"

execute_sql "
CREATE TABLE \"${TABLE_NAME}\" AS
SELECT
    ${OBS_COLUMNS},        -- these are unquoted columns like observation_uuid, etc.
    o.in_region,           -- already all-lowercase
#|LN|125|
    ${EXPANDED_TAXA_COLS}
FROM \"${ANCESTORS_OBS_TABLE}\" o
JOIN expanded_taxa e ON e.\"taxonID\" = o.taxon_id
WHERE e.\"taxonActive\" = TRUE
  AND (${rg_where_condition});
"

# ------------------------------------------------------------------------------
# 4) Optional partial-rank wipe for L20, L30, L40
# ------------------------------------------------------------------------------
if [ -z "${MIN_OCCURRENCES_PER_RANK:-}" ] || [ "${MIN_OCCURRENCES_PER_RANK}" = "-1" ]; then
  print_progress "Skipping partial-rank wipe (MIN_OCCURRENCES_PER_RANK not set or -1)."
else
  print_progress "Applying partial-rank wipe with threshold = ${MIN_OCCURRENCES_PER_RANK}"
  for rc in L20_taxonID L30_taxonID L40_taxonID; do
    print_progress "Wiping low-occurrence ${rc} if usage < ${MIN_OCCURRENCES_PER_RANK}"
    execute_sql "
    WITH usage_ct AS (
      SELECT \"${rc}\" as tid, COUNT(*) as c
      FROM \"${TABLE_NAME}\"
      WHERE \"${rc}\" IS NOT NULL
      GROUP BY 1
    )
    UPDATE \"${TABLE_NAME}\"
    SET \"${rc}\" = NULL
#|LN|150|
    FROM usage_ct
    WHERE usage_ct.tid = \"${TABLE_NAME}\".\"${rc}\"
      AND usage_ct.c < ${MIN_OCCURRENCES_PER_RANK};
    "
  done
fi

# ------------------------------------------------------------------------------
# 5) Export Final CSV with partition-based sampling for research-grade species
# ------------------------------------------------------------------------------
print_progress "cladistic.sh: Exporting final CSV with partition-based sampling"

pos_condition="TRUE"
if [ "${PRIMARY_ONLY:-false}" = "true" ]; then
  pos_condition="p.position=0"
fi

# Build the final column list for the CSV, all double-quoted so that the
# headers appear exactly as L5_taxonID, etc., in the CSV.
#
# We'll re-use get_obs_columns() output but wrap them in quotes for final usage.
# Then we manually append in_region, expanded columns, etc., all in quotes.

quote_columns() {
  # Helper that takes a comma-delimited string of column names and
#|LN|175|
  # returns them as a quoted, comma-delimited list (e.g. "colA","colB",...).
  local input="$1"
  local quoted=""
  IFS=',' read -ra cols <<< "$input"
  for col in "${cols[@]}"; do
    col="$(echo "$col" | xargs)"  # trim spaces
    [ -n "$quoted" ] && quoted="$quoted, \"$col\"" || quoted="\"$col\""
  done
  echo "$quoted"
}

RAW_OBS_COLS="$(get_obs_columns)"  # e.g. observation_uuid, observer_id, ...
QUOTED_OBS_COLS="$(quote_columns "$RAW_OBS_COLS")"

# Now define the expanded columns in quotes. Note that these match the aliases
# we used above (AS "L5_taxonID", etc.). Also in_region is a lowercase column,
# but we quote it for consistency in the final CSV.
CSV_OBS_COLS="$QUOTED_OBS_COLS,
\"in_region\",
\"expanded_taxonID\",
\"expanded_rankLevel\",
\"expanded_name\",
\"L5_taxonID\",
\"L10_taxonID\",
\"L11_taxonID\",
#|LN|200|
\"L12_taxonID\",
\"L13_taxonID\",
\"L15_taxonID\",
\"L20_taxonID\",
\"L24_taxonID\",
\"L25_taxonID\",
\"L26_taxonID\",
\"L27_taxonID\",
\"L30_taxonID\",
\"L32_taxonID\",
\"L33_taxonID\",
\"L33_5_taxonID\",
\"L34_taxonID\",
\"L34_5_taxonID\",
\"L35_taxonID\",
\"L37_taxonID\",
\"L40_taxonID\",
\"L43_taxonID\",
\"L44_taxonID\",
\"L45_taxonID\",
\"L47_taxonID\",
\"L50_taxonID\",
\"L53_taxonID\",
\"L57_taxonID\",
\"L60_taxonID\",
#|LN|225|
\"L67_taxonID\",
\"L70_taxonID\""

# Photo columns, also quoted
CSV_PHOTO_COLS="\"photo_uuid\", \"photo_id\", \"extension\", \"license\", \"width\", \"height\", \"position\""

# Debug logging: use quote_literal() to avoid array-literal syntax errors
debug_sql_obs="
SELECT 'DEBUG: Final CSV obs columns => ' ||
       quote_literal('${CSV_OBS_COLS}');
"
debug_sql_photo="
SELECT 'DEBUG: Final CSV photo columns => ' ||
       quote_literal('${CSV_PHOTO_COLS}');
"

execute_sql "$debug_sql_obs"
execute_sql "$debug_sql_photo"

# If no MAX_RN, default to 3000
if [ -z "${MAX_RN:-}" ]; then
  echo "Warning: MAX_RN not set, defaulting to 3000"
  MAX_RN=3000
fi

#|LN|250|
EXPORT_FILE="${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv"

# Final COPY query uses these columns in quotes
execute_sql "
COPY (
  WITH
  capped_research_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"  -- This is a quoted column in the new table
        ORDER BY
          CASE WHEN o.in_region THEN 0 ELSE 1 END,
          random()
      ) AS rn
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE ${pos_condition}
#|LN|275|
      AND o.quality_grade='research'
      AND o.\"L10_taxonID\" IS NOT NULL
  ),
  everything_else AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      NULL::bigint AS rn
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE ${pos_condition}
      AND NOT (o.quality_grade='research' AND o.\"L10_taxonID\" IS NOT NULL)
  )
  SELECT
    ${CSV_OBS_COLS},
    ${CSV_PHOTO_COLS},
    rn
  FROM capped_research_species
  WHERE rn <= ${MAX_RN}
#|LN|300|

  UNION ALL

  SELECT
    ${CSV_OBS_COLS},
    ${CSV_PHOTO_COLS},
    rn
  FROM everything_else
) TO '${EXPORT_FILE}'
  WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"

print_progress "cladistic.sh: CSV export complete"
print_progress "Exported final CSV to ${EXPORT_FILE}"
            </file>
            <file path="export/v0/common/functions.sh">
#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

get_obs_columns() {
    # Start with standard columns
    local cols="observation_uuid, observer_id, latitude, longitude"
    
    # Add elevation_meters if export is enabled.
    if [ "${INCLUDE_ELEVATION_EXPORT:-true}" = "true" ] && [ "${RELEASE_VALUE:-r1}" != "r0" ]; then
        cols="${cols}, elevation_meters"
    fi
    
    # Then add the remaining columns
    cols="${cols}, positional_accuracy, taxon_id, quality_grade, observed_on"
    
    # If anomaly_score exists for any release value other than r0, add it.
    if [[ "${RELEASE_VALUE}" != "r0" ]]; then
        cols="${cols}, anomaly_score"
    fi
    
    echo "$cols"
}


# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    if [ ! -d "${dir}" ]; then
        mkdir -p "${dir}"
        chmod -R 777 "${dir}"
    fi
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f get_obs_columns
export -f ensure_directory
export -f send_notification
            </file>
            <file path="export/v0/common/main.sh" line_interval="25">
#!/bin/bash
#
# main.sh
#
# Orchestrates the export pipeline by:
#   1) Validating environment variables
#   2) Always calling regional_base.sh (which handles creating/reusing
#      the region/clade-specific ancestor tables as needed).
#   3) Calling cladistic.sh to produce the final <EXPORT_GROUP>_observations table
#   4) Writing a unified export summary (environment variables + final stats)
#   5) Optionally copying the wrapper script for reproducibility
#
# NOTE:
#  - We no longer do skip/existence checks here. Instead, regional_base.sh
#    performs partial skip logic for its tables (_all_sp, _all_sp_and_ancestors_*, etc.).
#  - We have removed references to ANCESTOR_ROOT_RANKLEVEL, since our new multi-root
#    approach does not require it.
#
# This script expects the following environment variables to be set by the wrapper:
#   DB_USER           -> PostgreSQL user (e.g. "postgres")
#   VERSION_VALUE     -> Database version identifier (e.g. "v0")
#   RELEASE_VALUE     -> Release identifier (e.g. "r1")
#   ORIGIN_VALUE      -> (Optional) For logging context
#   DB_NAME           -> Name of the database (e.g. "ibrida-v0-r1")
#|LN|25|
#   REGION_TAG        -> Region bounding box key (e.g. "NAfull")
#   MIN_OBS           -> Minimum observations required for a species to be included
#   MAX_RN            -> Max random number of research-grade rows per species in final CSV
#   DB_CONTAINER      -> Docker container name for exec (e.g. "ibridaDB")
#   HOST_EXPORT_BASE_PATH -> Host system directory for exports
#   CONTAINER_EXPORT_BASE_PATH -> Container path that maps to HOST_EXPORT_BASE_PATH
#   EXPORT_SUBDIR     -> Subdirectory for the export (e.g. "v0/r1/primary_only_50min_4000max")
#   EXPORT_GROUP      -> Name of the final group (used in final table naming)
#
# Additionally, you may define:
#   WRAPPER_PATH      -> Path to the wrapper script for reproducibility (copied into the output dir if present).
#   INCLUDE_OUT_OF_REGION_OBS -> Whether to keep out-of-region observations for a region-based species.
#   RG_FILTER_MODE    -> One of: ONLY_RESEARCH, ALL, ALL_EXCLUDE_SPECIES_NON_RESEARCH, etc.
#   PRIMARY_ONLY      -> If true, only the primary (position=0) photo is included.
#   SKIP_REGIONAL_BASE-> If true, we skip regeneration of base tables if they exist.
#   INCLUDE_ELEVATION_EXPORT -> If "true", we include the 'elevation_meters' column (provided the DB has it, e.g. not "r0").
#
# All these environment variables are typically set in the release-specific wrapper (e.g. r1/wrapper_amphibia_all_exc_nonrg_sp.sh).
#

set -e

# ------------------------------------------------------------------------------
# 0) Source common functions
# ------------------------------------------------------------------------------
#|LN|50|
# We'll assume the caller sets BASE_DIR to the root of export/v0
# so that we can find common/functions.sh easily.
source "${BASE_DIR}/common/functions.sh"

# ------------------------------------------------------------------------------
# 1) Validate Required Environment Variables
# ------------------------------------------------------------------------------
required_vars=(
    "DB_USER" "VERSION_VALUE" "RELEASE_VALUE" "ORIGIN_VALUE"
    "DB_NAME" "REGION_TAG" "MIN_OBS" "MAX_RN"
    "DB_CONTAINER" "HOST_EXPORT_BASE_PATH" "CONTAINER_EXPORT_BASE_PATH"
    "EXPORT_SUBDIR" "EXPORT_GROUP"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var:-}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Some environment variables are optional but relevant:
# - SKIP_REGIONAL_BASE, INCLUDE_OUT_OF_REGION_OBS, RG_FILTER_MODE, MIN_OCCURRENCES_PER_RANK,
#   INCLUDE_MINOR_RANKS_IN_ANCESTORS, PRIMARY_ONLY, etc.
# We'll let them default if not set.
#|LN|75|

# ------------------------------------------------------------------------------
# 2) Create Export Directory Structure
# ------------------------------------------------------------------------------
print_progress "Creating export directory structure"
EXPORT_DIR="${CONTAINER_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
HOST_EXPORT_DIR="${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
ensure_directory "${HOST_EXPORT_DIR}"

# ------------------------------------------------------------------------------
# 3) Create PostgreSQL Extension & Role if needed (once per container, safe to run again)
# ------------------------------------------------------------------------------
execute_sql "
DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS dblink;
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'exportuser') THEN
        CREATE ROLE exportuser;
    END IF;
END \$\$;"

# ------------------------------------------------------------------------------
# Optional Logging for Elevation Setting
# ------------------------------------------------------------------------------
if [ "${INCLUDE_ELEVATION_EXPORT:-true}" = "true" ]; then
#|LN|100|
    print_progress "INCLUDE_ELEVATION_EXPORT=true => Elevation data (elevation_meters) will be included if present"
else
    print_progress "INCLUDE_ELEVATION_EXPORT=false => Elevation data will NOT be included"
fi

# ------------------------------------------------------------------------------
# Timing: We'll measure how long each major phase takes
# ------------------------------------------------------------------------------
overall_start=$(date +%s)

# ------------------------------------------------------------------------------
# 4) Always Invoke regional_base.sh
# ------------------------------------------------------------------------------
# The script 'regional_base.sh' is responsible for building or reusing
# region/clade-specific base tables. If SKIP_REGIONAL_BASE=true and the table
# exists, it is reused. Otherwise, it is created fresh.
regional_start=$(date +%s)
print_progress "Invoking ancestor-aware regional_base.sh"
source "${BASE_DIR}/common/regional_base.sh"
print_progress "regional_base.sh completed"
regional_end=$(date +%s)
regional_secs=$(( regional_end - regional_start ))

# ------------------------------------------------------------------------------
# 5) Apply Cladistic Filtering => Produces <EXPORT_GROUP>_observations
#|LN|125|
# ------------------------------------------------------------------------------
cladistic_start=$(date +%s)
print_progress "Applying cladistic filters via cladistic.sh"
source "${BASE_DIR}/common/cladistic.sh"
print_progress "Cladistic filtering complete"
cladistic_end=$(date +%s)
cladistic_secs=$(( cladistic_end - cladistic_start ))

# ------------------------------------------------------------------------------
# 6) Single Unified Export Summary
# ------------------------------------------------------------------------------
# We'll store environment variables, row counts, timing, etc. in a single file.
stats_start=$(date +%s)
print_progress "Creating unified export summary"

STATS=$(execute_sql "
WITH export_stats AS (
    SELECT
        COUNT(DISTINCT observation_uuid) AS num_observations,
        COUNT(DISTINCT taxon_id) AS num_taxa,
        COUNT(DISTINCT observer_id) AS num_observers
    FROM \"${EXPORT_GROUP}_observations\"
)
SELECT format(
    'Observations: %s\nUnique Taxa: %s\nUnique Observers: %s',
#|LN|150|
    num_observations, num_taxa, num_observers
)
FROM export_stats;")

SUMMARY_FILE="${HOST_EXPORT_DIR}/${EXPORT_GROUP}_export_summary.txt"
{
  echo "Export Summary"
  echo "Version: ${VERSION_VALUE}"
  echo "Release: ${RELEASE_VALUE}"
  echo "Origin: ${ORIGIN_VALUE}"
  echo "Region: ${REGION_TAG}"
  echo "Minimum Observations (species): ${MIN_OBS}"
  echo "Maximum Random Number (MAX_RN): ${MAX_RN}"
  echo "Export Group: ${EXPORT_GROUP}"
  echo "Date: $(date)"
  echo "SKIP_REGIONAL_BASE: ${SKIP_REGIONAL_BASE}"
  echo "SKIP_ALL_SP_TABLE: ${SKIP_ALL_SP_TABLE:-$SKIP_REGIONAL_BASE}"
  echo "SKIP_ANCESTORS_TABLE: ${SKIP_ANCESTORS_TABLE:-$SKIP_REGIONAL_BASE}"
  echo "INCLUDE_OUT_OF_REGION_OBS: ${INCLUDE_OUT_OF_REGION_OBS}"
  echo "INCLUDE_MINOR_RANKS_IN_ANCESTORS: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
  echo "RG_FILTER_MODE: ${RG_FILTER_MODE}"
  echo "MIN_OCCURRENCES_PER_RANK: ${MIN_OCCURRENCES_PER_RANK}"
  echo "INCLUDE_ELEVATION_EXPORT: ${INCLUDE_ELEVATION_EXPORT}"
  echo ""
  echo "Final Table Stats:"
#|LN|175|
  echo "${STATS}"
  echo ""
  echo "Timing:"
  echo " - Regional Base: ${regional_secs} seconds"
} > "${SUMMARY_FILE}"

stats_end=$(date +%s)
stats_secs=$(( stats_end - stats_start ))
print_progress "Stats/summary step took ${stats_secs} seconds"

# ------------------------------------------------------------------------------
# 7) Optionally Copy the Wrapper Script for Reproducibility
# ------------------------------------------------------------------------------
if [ -n "${WRAPPER_PATH:-}" ] && [ -f "${WRAPPER_PATH}" ]; then
    cp "${WRAPPER_PATH}" "${HOST_EXPORT_DIR}/"
fi

# ------------------------------------------------------------------------------
# 8) Wrap Up
# ------------------------------------------------------------------------------
overall_end=$(date +%s)
overall_secs=$(( overall_end - overall_start ))
print_progress "Export process complete (total time: ${overall_secs} seconds)"

{
#|LN|200|
  echo " - Cladistic: ${cladistic_secs} seconds"
  echo " - Summary/Stats Step: ${stats_secs} seconds"
  echo " - Overall: ${overall_secs} seconds"
} >> "${SUMMARY_FILE}"

send_notification "Export for ${EXPORT_GROUP} complete. Summary at ${SUMMARY_FILE}"
            </file>
            <file path="export/v0/common/region_defns.sh">
#!/bin/bash
# ------------------------------------------------------------------------------
# region_defns.sh
# ------------------------------------------------------------------------------
# This file defines the bounding box coordinates for each supported region.
#
# Usage:
#   source region_defns.sh
#   Then set REGION_TAG in your environment, and use get_region_coordinates()
#   to populate XMIN, XMAX, YMIN, YMAX environment variables.
# ------------------------------------------------------------------------------

declare -A REGION_COORDINATES

# North America
REGION_COORDINATES["NAfull"]="(-169.453125 12.211180 -23.554688 84.897147)"

# Europe
REGION_COORDINATES["EURwest"]="(-12.128906 40.245992 12.480469 60.586967)"
REGION_COORDINATES["EURnorth"]="(-25.927734 54.673831 45.966797 71.357067)"
REGION_COORDINATES["EUReast"]="(10.722656 41.771312 39.550781 59.977005)"
REGION_COORDINATES["EURfull"]="(-30.761719 33.284620 43.593750 72.262310)"

# Mediterranean
REGION_COORDINATES["MED"]="(-16.259766 29.916852 36.474609 46.316584)"

# Australia
REGION_COORDINATES["AUSfull"]="(111.269531 -47.989922 181.230469 -9.622414)"

# Asia
REGION_COORDINATES["ASIAse"]="(82.441406 -11.523088 153.457031 28.613459)"
REGION_COORDINATES["ASIAeast"]="(462.304688 23.241346 550.195313 78.630006)"
REGION_COORDINATES["ASIAcentral"]="(408.515625 36.031332 467.753906 76.142958)"
REGION_COORDINATES["ASIAsouth"]="(420.468750 1.581830 455.097656 39.232253)"
REGION_COORDINATES["ASIAsw"]="(386.718750 12.897489 423.281250 48.922499)"
REGION_COORDINATES["ASIA_nw"]="(393.046875 46.800059 473.203125 81.621352)"

# South America
REGION_COORDINATES["SAfull"]="(271.230469 -57.040730 330.644531 15.114553)"

# Africa
REGION_COORDINATES["AFRfull"]="(339.082031 -37.718590 421.699219 39.232253)"

# ------------------------------------------------------------------------------
# get_region_coordinates()
# ------------------------------------------------------------------------------
# Sets XMIN, YMIN, XMAX, YMAX variables from the region definition for REGION_TAG.
# If REGION_TAG is not recognized, prints an error and returns 1.
#
# Usage:
#   export REGION_TAG="XYZ"
#   source region_defns.sh
#   get_region_coordinates  # => sets XMIN, YMIN, XMAX, YMAX
# ------------------------------------------------------------------------------
function get_region_coordinates() {
    local coords="${REGION_COORDINATES[$REGION_TAG]}"
    if [ -z "$coords" ]; then
        echo "ERROR: Unknown REGION_TAG: $REGION_TAG" >&2
        return 1
    fi
    
    # Parse the coordinate quadruple from parentheses
    read XMIN YMIN XMAX YMAX <<< "${coords//[()]/}"

    # Export them for use by the caller
    export XMIN YMIN XMAX YMAX
}

export -f get_region_coordinates

            </file>
            <file path="export/v0/common/regional_base.sh" line_interval="25">
#!/bin/bash
# ------------------------------------------------------------------------------
# regional_base.sh
# ------------------------------------------------------------------------------
# Generates region-specific species tables and associated ancestor sets,
# factoring in the user's clade/metaclade and the major/minor rank mode.
#
# Steps:
#   1) Parse environment variables and region coordinates.
#   2) Build or reuse the <REGION_TAG>_min<MIN_OBS>_all_sp table (region + MIN_OBS only).
#   3) Parse clade condition (single or multi-root). If multi-root, check overlap.
#   4) Build or reuse <REGION_TAG>_min<MIN_OBS>_all_sp_and_ancestors_<cladeID>_<mode>
#   5) Build or reuse <REGION_TAG>_min<MIN_OBS>_sp_and_ancestors_obs_<cladeID>_<mode>
#   6) Output final info/summary
#
# Requires:
#   - environment variables: DB_NAME, DB_CONTAINER, DB_USER, ...
#   - script variables: REGION_TAG, MIN_OBS, SKIP_REGIONAL_BASE,
#     INCLUDE_OUT_OF_REGION_OBS, INCLUDE_MINOR_RANKS_IN_ANCESTORS,
#     etc.
#
# ------------------------------------------------------------------------------

source "${BASE_DIR}/common/functions.sh"
#|LN|25|
source "${BASE_DIR}/common/clade_defns.sh"
source "${BASE_DIR}/common/clade_helpers.sh"
source "${BASE_DIR}/common/region_defns.sh"

# ---------------------------------------------------------------------------
# 0) Validate Environment + Setup
# ---------------------------------------------------------------------------
: "${REGION_TAG:?Error: REGION_TAG is not set}"
: "${MIN_OBS:?Error: MIN_OBS is not set}"
: "${SKIP_REGIONAL_BASE:?Error: SKIP_REGIONAL_BASE is not set}"
: "${INCLUDE_OUT_OF_REGION_OBS:?Error: INCLUDE_OUT_OF_REGION_OBS is not set}"
: "${INCLUDE_MINOR_RANKS_IN_ANCESTORS:?Error: INCLUDE_MINOR_RANKS_IN_ANCESTORS is not set}"

# ---------------------------------------------------------------------------
# New Granular Control Flags
# ---------------------------------------------------------------------------
# If granular flags aren't explicitly set, default to the value of SKIP_REGIONAL_BASE
# This maintains backward compatibility
export SKIP_ALL_SP_TABLE="${SKIP_ALL_SP_TABLE:-$SKIP_REGIONAL_BASE}"
export SKIP_ANCESTORS_TABLE="${SKIP_ANCESTORS_TABLE:-$SKIP_REGIONAL_BASE}"

print_progress "=== regional_base.sh: Starting Ancestor-Aware Regional Base Generation ==="
print_progress "Using granular control flags: SKIP_ALL_SP_TABLE=${SKIP_ALL_SP_TABLE}, SKIP_ANCESTORS_TABLE=${SKIP_ANCESTORS_TABLE}"

# Retrieve bounding box for the region
#|LN|50|
get_region_coordinates || {
  echo "Failed to retrieve bounding box for REGION_TAG=${REGION_TAG}" >&2
  exit 1
}

print_progress "Using bounding box => XMIN=${XMIN}, YMIN=${YMIN}, XMAX=${XMAX}, YMAX=${YMAX}"

# ---------------------------------------------------------------------------
# 1) Build or Reuse <REGION_TAG>_min<MIN_OBS>_all_sp
# ---------------------------------------------------------------------------
ALL_SP_TABLE="${REGION_TAG}_min${MIN_OBS}_all_sp"

check_and_build_all_sp() {
  # Check existence
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ALL_SP_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    # If table exists, check row count
    local row_count
#|LN|75|
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ALL_SP_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ALL_SP_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_ALL_SP_TABLE}" = "true" ]; then
        print_progress "SKIP_ALL_SP_TABLE=true => reusing existing _all_sp table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating (or recreating) table \"${ALL_SP_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ALL_SP_TABLE}\" CASCADE;"

  # Build the table with bounding box + rank_level=10 + MIN_OBS filter
  execute_sql "
  CREATE TABLE \"${ALL_SP_TABLE}\" AS
  SELECT s.taxon_id
  FROM observations s
#|LN|100|
  JOIN taxa t ON t.taxon_id = s.taxon_id
  WHERE t.rank_level = 10
    AND s.quality_grade = 'research'
    AND s.geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
  GROUP BY s.taxon_id
  HAVING COUNT(s.observation_uuid) >= ${MIN_OBS};
  "
}

check_and_build_all_sp

# ---------------------------------------------------------------------------
# 2) Parse Clade Condition & Check Overlap if Multi-root
# ---------------------------------------------------------------------------
CLADE_CONDITION="$(get_clade_condition)"
print_progress "Clade Condition: ${CLADE_CONDITION}"

root_list=( $(parse_clade_expression "${CLADE_CONDITION}") )
root_count="${#root_list[@]}"
print_progress "Found ${root_count} root(s) from the clade condition"

# Decide on a short ID for the clade/metaclade
# (if you want to embed actual environment variables: e.g. $CLADE or $METACLADE
#  or parse the user-supplied string from the condition. We'll do a naive approach.)
if [ -n "${METACLADE}" ]; then
#|LN|125|
  CLADE_ID="${METACLADE}"
elif [ -n "${CLADE}" ]; then
  CLADE_ID="${CLADE}"
elif [ -n "${MACROCLADE}" ]; then
  CLADE_ID="${MACROCLADE}"
else
  # fallback if user didn't set anything
  CLADE_ID="universal"
fi

# Clean up the clade_id so it doesn't contain spaces or special chars
CLADE_ID="${CLADE_ID// /_}"

# If multi-root => check overlap
if [ "${root_count}" -gt 1 ]; then
  print_progress "Multiple roots => checking independence"
  check_root_independence "${DB_NAME}" "${root_list[@]}"
  if [ $? -ne 0 ]; then
    echo "ERROR: Overlap detected among metaclade roots. Aborting."
    exit 1
  fi
  print_progress "All roots are mutually independent"
fi

# Decide majorOrMinor string
#|LN|150|
if [ "${INCLUDE_MINOR_RANKS_IN_ANCESTORS}" = "true" ]; then
  RANK_MODE="inclMinor"
else
  RANK_MODE="majorOnly"
fi

# Build final table names
ANCESTORS_TABLE="${REGION_TAG}_min${MIN_OBS}_all_sp_and_ancestors_${CLADE_ID}_${RANK_MODE}"
ANCESTORS_OBS_TABLE="${REGION_TAG}_min${MIN_OBS}_sp_and_ancestors_obs_${CLADE_ID}_${RANK_MODE}"

# ---------------------------------------------------------------------------
# 3) Build or Reuse <REGION_TAG>_min<MIN_OBS>_all_sp_and_ancestors_<cladeID>_<mode>
# ---------------------------------------------------------------------------
check_and_build_ancestors() {
  # 1) Check if the table already exists and skip if user wants to skip ancestors
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ANCESTORS_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    local row_count
#|LN|175|
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ANCESTORS_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ANCESTORS_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_ANCESTORS_TABLE}" = "true" ]; then
        print_progress "SKIP_ANCESTORS_TABLE=true => reusing existing ancestors table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating table \"${ANCESTORS_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ANCESTORS_TABLE}\" CASCADE;"
  execute_sql "
  CREATE TABLE \"${ANCESTORS_TABLE}\" (
    taxon_id integer PRIMARY KEY
  );
  "

#|LN|200|
  local insert_ancestors_for_root
  insert_ancestors_for_root() {
    local root_pair="$1"  # e.g. "50=47158"
    local rank_part="${root_pair%%=*}"
    local root_taxid="${root_pair##*=}"

    local col_name="L${rank_part}_taxonID"

    # Decide boundary (majorOnly vs. inclMinor)
    local boundary_rank="$rank_part"
    if [ "${INCLUDE_MINOR_RANKS_IN_ANCESTORS}" = "false" ]; then
      boundary_rank="$(get_major_rank_floor "${rank_part}")"
    fi

    execute_sql "
    ----------------------------------------------------------------
    -- 1) Gather species from <ALL_SP_TABLE> that belong to this root
    ----------------------------------------------------------------
    DROP TABLE IF EXISTS temp_${root_taxid}_sp_list CASCADE;
    CREATE TEMP TABLE temp_${root_taxid}_sp_list AS
    SELECT s.taxon_id
    FROM \"${ALL_SP_TABLE}\" s
    JOIN expanded_taxa e ON e.\"taxonID\" = s.taxon_id
    WHERE e.\"${col_name}\" = ${root_taxid};

#|LN|225|
    ----------------------------------------------------------------
    -- 2) Unroll each species's ancestor IDs (L5..L70) and filter by rank
    ----------------------------------------------------------------
    DROP TABLE IF EXISTS temp_${root_taxid}_all_ancestors CASCADE;

    WITH unravel AS (
      -- 'unravel' yields each row's potential ancestor columns
      SELECT
        e.\"taxonID\"        AS sp_id,
        e.\"L5_taxonID\"     AS L5_id,
        e.\"L10_taxonID\"    AS L10_id,
        e.\"L11_taxonID\"    AS L11_id,
        e.\"L12_taxonID\"    AS L12_id,
        e.\"L13_taxonID\"    AS L13_id,
        e.\"L15_taxonID\"    AS L15_id,
        e.\"L20_taxonID\"    AS L20_id,
        e.\"L24_taxonID\"    AS L24_id,
        e.\"L25_taxonID\"    AS L25_id,
        e.\"L26_taxonID\"    AS L26_id,
        e.\"L27_taxonID\"    AS L27_id,
        e.\"L30_taxonID\"    AS L30_id,
        e.\"L32_taxonID\"    AS L32_id,
        e.\"L33_taxonID\"    AS L33_id,
        e.\"L33_5_taxonID\"  AS L33_5_id,
        e.\"L34_taxonID\"    AS L34_id,
#|LN|250|
        e.\"L34_5_taxonID\"  AS L34_5_id,
        e.\"L35_taxonID\"    AS L35_id,
        e.\"L37_taxonID\"    AS L37_id,
        e.\"L40_taxonID\"    AS L40_id,
        e.\"L43_taxonID\"    AS L43_id,
        e.\"L44_taxonID\"    AS L44_id,
        e.\"L45_taxonID\"    AS L45_id,
        e.\"L47_taxonID\"    AS L47_id,
        e.\"L50_taxonID\"    AS L50_id,
        e.\"L53_taxonID\"    AS L53_id,
        e.\"L57_taxonID\"    AS L57_id,
        e.\"L60_taxonID\"    AS L60_id,
        e.\"L67_taxonID\"    AS L67_id,
        e.\"L70_taxonID\"    AS L70_id
      FROM expanded_taxa e
      JOIN temp_${root_taxid}_sp_list sp
         ON e.\"taxonID\" = sp.taxon_id
    ),
    all_ancestors AS (
      -- We'll produce rows for the species' own ID (sp_id)
      -- plus each potential ancestor ID, then filter by rankLevel < boundary_rank.
      SELECT sp_id AS taxon_id
      FROM unravel

      UNION ALL
#|LN|275|

      SELECT x.\"taxonID\" AS taxon_id
      FROM unravel u
      CROSS JOIN LATERAL (VALUES
        (u.L5_id),(u.L10_id),(u.L11_id),(u.L12_id),(u.L13_id),(u.L15_id),
        (u.L20_id),(u.L24_id),(u.L25_id),(u.L26_id),(u.L27_id),(u.L30_id),
        (u.L32_id),(u.L33_id),(u.L33_5_id),(u.L34_id),(u.L34_5_id),(u.L35_id),
        (u.L37_id),(u.L40_id),(u.L43_id),(u.L44_id),(u.L45_id),(u.L47_id),
        (u.L50_id),(u.L53_id),(u.L57_id),(u.L60_id),(u.L67_id),(u.L70_id)
      ) anc(ancestor_id)
      JOIN expanded_taxa x ON x.\"taxonID\" = anc.ancestor_id
      WHERE x.\"rankLevel\" < ${boundary_rank}
    )
    SELECT DISTINCT taxon_id
    INTO TEMP temp_${root_taxid}_all_ancestors
    FROM all_ancestors
    WHERE taxon_id IS NOT NULL;

    ----------------------------------------------------------------
    -- 3) Insert into the final ancestors table
    ----------------------------------------------------------------
    INSERT INTO \"${ANCESTORS_TABLE}\"(taxon_id)
    SELECT DISTINCT taxon_id
    FROM temp_${root_taxid}_all_ancestors;
    "
#|LN|300|
  }

  # Decide single vs. multi-root
  if [ "${root_count}" -eq 0 ]; then
    print_progress "No recognized root => no ancestors inserted. (Might be 'TRUE' clade?)"
  elif [ "${root_count}" -eq 1 ]; then
    print_progress "Single root => straightforward insertion"
    insert_ancestors_for_root "${root_list[0]}"
  else
    print_progress "Multi-root => union each root's ancestor set"
    for root_entry in "${root_list[@]}"; do
      insert_ancestors_for_root "${root_entry}"
    done
  fi
}

check_and_build_ancestors

# ---------------------------------------------------------------------------
# 4) Build or Reuse <REGION_TAG>_min<MIN_OBS>_sp_and_ancestors_obs_<cladeID>_<mode>
# ---------------------------------------------------------------------------
check_and_build_ancestors_obs() {
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
#|LN|325|
    WHERE schemaname='public'
      AND tablename='${ANCESTORS_OBS_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    local row_count
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ANCESTORS_OBS_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ANCESTORS_OBS_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_ANCESTORS_TABLE}" = "true" ]; then
        print_progress "SKIP_ANCESTORS_TABLE=true => reusing existing ancestors_obs table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating table \"${ANCESTORS_OBS_TABLE}\""
#|LN|350|
  execute_sql "DROP TABLE IF EXISTS \"${ANCESTORS_OBS_TABLE}\" CASCADE;"

  local OBS_COLUMNS
  OBS_COLUMNS="$(get_obs_columns)"

  # ---------------------------------------------------------------------------
  # ADDED FEATURE: Always store an `in_region` boolean for each observation
  # ---------------------------------------------------------------------------
  # If INCLUDE_OUT_OF_REGION_OBS=true, we do NOT filter by bounding box in the
  # WHERE clause, but we compute a boolean:
  #    COALESCE(ST_Within(geom, ST_MakeEnvelope(...)), false) AS in_region
  #
  # If INCLUDE_OUT_OF_REGION_OBS=false, we do filter by bounding box
  #    AND geom && ST_MakeEnvelope(...)
  # and simply store in_region=TRUE for all rows.

  local BBOX="ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)"

  if [ "${INCLUDE_OUT_OF_REGION_OBS}" = "true" ]; then
    execute_sql "
    CREATE TABLE \"${ANCESTORS_OBS_TABLE}\" AS
    SELECT
      ${OBS_COLUMNS},
      COALESCE(ST_Within(geom, ${BBOX}), false) AS in_region
    FROM observations
#|LN|375|
    WHERE taxon_id IN (
      SELECT taxon_id
      FROM \"${ANCESTORS_TABLE}\"
    );
    "
  else
    execute_sql "
    CREATE TABLE \"${ANCESTORS_OBS_TABLE}\" AS
    SELECT
      ${OBS_COLUMNS},
      true AS in_region
    FROM observations
    WHERE taxon_id IN (
      SELECT taxon_id
      FROM \"${ANCESTORS_TABLE}\"
    )
    AND geom && ${BBOX};
    "
  fi
}

check_and_build_ancestors_obs

export ANCESTORS_OBS_TABLE="${ANCESTORS_OBS_TABLE}" # for cladistic.sh

#|LN|400|
print_progress "=== regional_base.sh: Completed building base tables for ${REGION_TAG}, minObs=${MIN_OBS}, clade=${CLADE_ID}, mode=${RANK_MODE} ==="
            </file>
          </dir>
          <dir path="port/t/v0/r0">
            <file path="export/v0/r0/wrapper.sh">
#!/bin/bash

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r0"
export ORIGIN_VALUE="iNat-June2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"

# Base paths
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"

### Primary Terrestrial Arthropoda Export Parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true
export EXPORT_GROUP="primary_terrestrial_arthropoda"
export PROCESS_OTHER=false
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"

### Amphibia Export Parameters (commented out)
# export REGION_TAG="NAfull"
# export MIN_OBS=400
# export MAX_RN=1000
# export PRIMARY_ONLY=true
# export EXPORT_GROUP="amphibia"
# export PROCESS_OTHER=false
# export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"

# Execute main script
"${BASE_DIR}/common/main.sh"
            </file>
          </dir>
          <dir path="port/t/v0/r1">
            <file path="export/v0/r1/wrapper.sh">
#!/bin/bash
#
# wrapper.sh
#
# A typical user-facing script that sets environment variables and then calls main.sh.
# We define WRAPPER_PATH="$0" so that main.sh can copy this file for reproducibility.

SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"

echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Provide path to wrapper for reproducibility
export WRAPPER_PATH="$0"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=200
export PRIMARY_ONLY=true

# We could set CLADE or METACLADE here; let's pick something as an example:
export CLADE="amphibia"
export EXPORT_GROUP="${CLADE}"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false

# NEW ENV VARS
export INCLUDE_OUT_OF_REGION_OBS=true
export RG_FILTER_MODE="ALL"

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"

"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"
            </file>
            <file path="export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_all_exc_nonrg_sp_inc_out_of_region"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true
# Include elevation_meters in the final dataset?
export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false # NOTE this differs from the other wrappers, export group name isn't accurate

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_amphibia_all_miniTest.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=200
export MAX_RN=800
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_all_miniTest"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=false    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true
# Include elevation_meters in the final dataset?
export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=200
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_angiospermae_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=60
export MAX_RN=1750
export PRIMARY_ONLY=true

export CLADE="angiospermae"
export EXPORT_GROUP="angiospermae_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=60
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_aves_all_exc_nonrg_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="aves"
export EXPORT_GROUP="aves_all_exc_nonrg_sp_inc_oor_full_ancestor_search_no_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# NOTE elevation not ready yet, exclude for now
export INCLUDE_ELEVATION_EXPORT=false
# TODO when ready, set to true, adjust EXPORT_GROUP to include elevation

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_aves_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="aves"
export EXPORT_GROUP="aves_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_aves_reuse_all_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="aves"
export EXPORT_GROUP="aves_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip All Species Table Creation: ${SKIP_ALL_SP_TABLE}"
log_message "Skip Ancestors Table Creation: ${SKIP_ANCESTORS_TABLE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!" 
            </file>
            <file path="export/v0/r1/wrapper_mammalia_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="mammalia"
export EXPORT_GROUP="mammalia_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_pta.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

# We’ll use a metaclade here as an example
export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=false

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ONLY_RESEARCH"

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_pta_all_exc_nonrg_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2750
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp_inc_oor_full_ancestor_search_no_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# NOTE elevation not ready yet, exclude for now
export INCLUDE_ELEVATION_EXPORT=false
# TODO when ready, set to true, adjust EXPORT_GROUP to include elevation
# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_pta_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2750
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true
# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_pta_non_rg.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

# We’ll use a metaclade here as an example
export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_non_rg"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=false

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ONLY_NONRESEARCH_WIPE_SPECIES_LABEL"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_reptilia.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=4000
export PRIMARY_ONLY=true
export CLADE="reptilia"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"
            </file>
            <file path="export/v0/r1/wrapper_reptilia_all_exc_nonrg_sp.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="reptilia"
export EXPORT_GROUP="reptilia_all_exc_nonrg_sp_inc_oor_full_ancestor_search_no_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# NOTE elevation not ready yet, exclude for now
export INCLUDE_ELEVATION_EXPORT=false
# TODO when ready, set to true, adjust EXPORT_GROUP to include elevation

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
            <file path="export/v0/r1/wrapper_reptilia_all_exc_nonrg_sp_inc_oor_fas_elev.sh">
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="reptilia"
export EXPORT_GROUP="reptilia_all_exc_nonrg_sp_inc_oor_fas_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # Master switch, we'll set the granular flags below

# New granular control flags
export SKIP_ALL_SP_TABLE=true    # REUSE the base species table
export SKIP_ANCESTORS_TABLE=false # CREATE new clade-specific ancestor tables

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

            </file>
          </dir>
          <dir path="port/t/v0/scrap">
            <file path="export/v0/scrap/export_per_species_snippet.sh">
#!/bin/bash
#
# export_per_species_snippet.sh
#
# This script performs a per-species random sampling export from an
# already-existing table. It does NOT drop or re-create the table.
# It's intended for quick usage to avoid re-running all upstream steps.
#
# Usage: set environment variables before calling, e.g.:
#   export DB_CONTAINER="ibridaDB"
#   export DB_USER="postgres"
#   export DB_NAME="ibrida-v0-r1"
#   export EXPORT_GROUP="primary_terrestrial_arthropoda"
#   export EXPORT_DIR="/exports/v0/r1/primary_only_50min_4000max"
#   export MAX_RN=4000
#   export PRIMARY_ONLY=true
#   Then run:
#   ./export_per_species_snippet.sh
#

# CLARIFY: We assume that the user already has a table named ${EXPORT_GROUP}_observations
#          that includes all columns needed, and a 'photos' table too.
# ASSUMPTION: The container user has write access to $EXPORT_DIR.

SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
source "${SCRIPT_DIR}/functions.sh"

TABLE_NAME="${EXPORT_GROUP}_observations"

print_progress "Starting quick per-species export from existing table: ${TABLE_NAME}"

# The actual COPY logic is adapted from cladistic.sh, focusing on partition-based random sampling:
if [ "${PRIMARY_ONLY}" = true ]; then
    # Photos with position=0, quality_grade='research'
    execute_sql "
COPY (
  WITH per_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY random()
      ) AS species_rand_idx
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE p.position = 0
      AND o.quality_grade = 'research'
  )
  SELECT *
  FROM per_species
  WHERE
    (\"L10_taxonID\" IS NULL)
    OR (\"L10_taxonID\" IS NOT NULL AND species_rand_idx <= ${MAX_RN})
) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"
else
    # All photos for the final set, restricted to quality_grade='research'
    execute_sql "
COPY (
  WITH per_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY random()
      ) AS species_rand_idx
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE o.quality_grade = 'research'
  )
  SELECT *
  FROM per_species
  WHERE
    (\"L10_taxonID\" IS NULL)
    OR (\"L10_taxonID\" IS NOT NULL AND species_rand_idx <= ${MAX_RN})
) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"
fi

print_progress "Quick per-species export complete."
            </file>
            <file path="export/v0/scrap/schemas.md">
# expanded_taxa

One row per taxon. Expands the 'ancestry' column from the 'taxa' table into a set of columns.
    Can be much more performany than recursive string parsing for ancestry.

```sql
ibrida-v0-r1=# \d expanded_taxa
                        Table "public.expanded_taxa"
      Column      |          Type          | Collation | Nullable | Default
------------------+------------------------+-----------+----------+---------
 taxonID          | integer                |           | not null |
 rankLevel        | double precision       |           |          |
 rank             | character varying(255) |           |          |
 name             | character varying(255) |           |          |
 taxonActive      | boolean                |           |          |
 L5_taxonID       | integer                |           |          |
 L5_name          | character varying(255) |           |          |
 L5_commonName    | character varying(255) |           |          |
 L10_taxonID      | integer                |           |          |
 L10_name         | character varying(255) |           |          |
 L10_commonName   | character varying(255) |           |          |
 L11_taxonID      | integer                |           |          |
 L11_name         | character varying(255) |           |          |
 L11_commonName   | character varying(255) |           |          |
 L12_taxonID      | integer                |           |          |
 L12_name         | character varying(255) |           |          |
 L12_commonName   | character varying(255) |           |          |
 L13_taxonID      | integer                |           |          |
 L13_name         | character varying(255) |           |          |
 L13_commonName   | character varying(255) |           |          |
 L15_taxonID      | integer                |           |          |
 L15_name         | character varying(255) |           |          |
 L15_commonName   | character varying(255) |           |          |
 L20_taxonID      | integer                |           |          |
 L20_name         | character varying(255) |           |          |
 L20_commonName   | character varying(255) |           |          |
 L24_taxonID      | integer                |           |          |
 L24_name         | character varying(255) |           |          |
 L24_commonName   | character varying(255) |           |          |
 L25_taxonID      | integer                |           |          |
 L25_name         | character varying(255) |           |          |
 L25_commonName   | character varying(255) |           |          |
 L26_taxonID      | integer                |           |          |
 L26_name         | character varying(255) |           |          |
 L26_commonName   | character varying(255) |           |          |
 L27_taxonID      | integer                |           |          |
 L27_name         | character varying(255) |           |          |
 L27_commonName   | character varying(255) |           |          |
 L30_taxonID      | integer                |           |          |
 L30_name         | character varying(255) |           |          |
 L30_commonName   | character varying(255) |           |          |
 L32_taxonID      | integer                |           |          |
 L32_name         | character varying(255) |           |          |
 L32_commonName   | character varying(255) |           |          |
 L33_taxonID      | integer                |           |          |
 L33_name         | character varying(255) |           |          |
 L33_commonName   | character varying(255) |           |          |
 L33_5_taxonID    | integer                |           |          |
 L33_5_name       | character varying(255) |           |          |
 L33_5_commonName | character varying(255) |           |          |
 L34_taxonID      | integer                |           |          |
 L34_name         | character varying(255) |           |          |
 L34_commonName   | character varying(255) |           |          |
 L34_5_taxonID    | integer                |           |          |
 L34_5_name       | character varying(255) |           |          |
 L34_5_commonName | character varying(255) |           |          |
 L35_taxonID      | integer                |           |          |
 L35_name         | character varying(255) |           |          |
 L35_commonName   | character varying(255) |           |          |
 L37_taxonID      | integer                |           |          |
 L37_name         | character varying(255) |           |          |
 L37_commonName   | character varying(255) |           |          |
 L40_taxonID      | integer                |           |          |
 L40_name         | character varying(255) |           |          |
 L40_commonName   | character varying(255) |           |          |
 L43_taxonID      | integer                |           |          |
 L43_name         | character varying(255) |           |          |
 L43_commonName   | character varying(255) |           |          |
 L44_taxonID      | integer                |           |          |
 L44_name         | character varying(255) |           |          |
 L44_commonName   | character varying(255) |           |          |
 L45_taxonID      | integer                |           |          |
 L45_name         | character varying(255) |           |          |
 L45_commonName   | character varying(255) |           |          |
 L47_taxonID      | integer                |           |          |

...skipping 1 line
 L47_commonName   | character varying(255) |           |          |
 L50_taxonID      | integer                |           |          |
 L50_name         | character varying(255) |           |          |
 L50_commonName   | character varying(255) |           |          |
 L53_taxonID      | integer                |           |          |
 L53_name         | character varying(255) |           |          |
 L53_commonName   | character varying(255) |           |          |
 L57_taxonID      | integer                |           |          |
 L57_name         | character varying(255) |           |          |
 L57_commonName   | character varying(255) |           |          |
 L60_taxonID      | integer                |           |          |
 L60_name         | character varying(255) |           |          |
 L60_commonName   | character varying(255) |           |          |
 L67_taxonID      | integer                |           |          |
 L67_name         | character varying(255) |           |          |
 L67_commonName   | character varying(255) |           |          |
 L70_taxonID      | integer                |           |          |
 L70_name         | character varying(255) |           |          |
 L70_commonName   | character varying(255) |           |          |
Indexes:
    "expanded_taxa_pkey" PRIMARY KEY, btree ("taxonID")
    "idx_expanded_taxa_l10_taxonid" btree ("L10_taxonID")
    "idx_expanded_taxa_l20_taxonid" btree ("L20_taxonID")
    "idx_expanded_taxa_l30_taxonid" btree ("L30_taxonID")
    "idx_expanded_taxa_l40_taxonid" btree ("L40_taxonID")
    "idx_expanded_taxa_l50_taxonid" btree ("L50_taxonID")
    "idx_expanded_taxa_l60_taxonid" btree ("L60_taxonID")
    "idx_expanded_taxa_l70_taxonid" btree ("L70_taxonID")
    "idx_expanded_taxa_name" btree (name)
    "idx_expanded_taxa_ranklevel" btree ("rankLevel")
    "idx_expanded_taxa_taxonid" btree ("taxonID")
```
---

# taxa

One row per taxon.

```sql
ibrida-v0-r1=# \d taxa
                         Table "public.taxa"
   Column   |          Type          | Collation | Nullable | Default
------------+------------------------+-----------+----------+---------
 taxon_id   | integer                |           | not null |
 ancestry   | character varying(255) |           |          |
 rank_level | double precision       |           |          |
 rank       | character varying(255) |           |          |
 name       | character varying(255) |           |          |
 active     | boolean                |           |          |
 origin     | character varying(255) |           |          |
 version    | character varying(255) |           |          |
 release    | character varying(255) |           |          |
Indexes:
    "index_taxa_active" btree (active)
    "index_taxa_name" gin (to_tsvector('simple'::regconfig, name::text))
    "index_taxa_origins" gin (to_tsvector('simple'::regconfig, origin::text))
    "index_taxa_release" gin (to_tsvector('simple'::regconfig, release::text))
    "index_taxa_taxon_id" btree (taxon_id)
    "index_taxa_version" gin (to_tsvector('simple'::regconfig, version::text))
```

---

# observations

```sql
ibrida-v0-r1=# \d observations
                          Table "public.observations"
       Column        |          Type          | Collation | Nullable | Default
---------------------+------------------------+-----------+----------+---------
 observation_uuid    | uuid                   |           | not null |
 observer_id         | integer                |           |          |
 latitude            | numeric(15,10)         |           |          |
 longitude           | numeric(15,10)         |           |          |
 positional_accuracy | integer                |           |          |
 taxon_id            | integer                |           |          |
 quality_grade       | character varying(255) |           |          |
 observed_on         | date                   |           |          |
 anomaly_score       | numeric(15,6)          |           |          |
 geom                | geometry               |           |          |
 origin              | character varying(255) |           |          |
 version             | character varying(255) |           |          |
 release             | character varying(255) |           |          |
Indexes:
    "idx_observations_anomaly" btree (anomaly_score)
    "index_observations_observer_id" btree (observer_id)
    "index_observations_origins" gin (to_tsvector('simple'::regconfig, origin::text))
    "index_observations_quality" btree (quality_grade)
    "index_observations_release" gin (to_tsvector('simple'::regconfig, release::text))
    "index_observations_taxon_id" btree (taxon_id)
    "index_observations_version" gin (to_tsvector('simple'::regconfig, version::text))
    "observations_geom" gist (geom)
```

---

# photos

```sql
ibrida-v0-r1=# \d photos
                           Table "public.photos"
      Column      |          Type          | Collation | Nullable | Default
------------------+------------------------+-----------+----------+---------
 photo_uuid       | uuid                   |           | not null |
 photo_id         | integer                |           | not null |
 observation_uuid | uuid                   |           | not null |
 observer_id      | integer                |           |          |
 extension        | character varying(5)   |           |          |
 license          | character varying(255) |           |          |
 width            | smallint               |           |          |
 height           | smallint               |           |          |
 position         | smallint               |           |          |
 origin           | character varying(255) |           |          |
 version          | character varying(255) |           |          |
 release          | character varying(255) |           |          |
Indexes:
    "index_photos_observation_uuid" btree (observation_uuid)
    "index_photos_origins" gin (to_tsvector('simple'::regconfig, origin::text))
    "index_photos_photo_id" btree (photo_id)
    "index_photos_photo_uuid" btree (photo_uuid)
    "index_photos_position" btree ("position")
    "index_photos_release" gin (to_tsvector('simple'::regconfig, release::text))
    "index_photos_version" gin (to_tsvector('simple'::regconfig, version::text))
```

---

# observers

```sql
ibrida-v0-r1=# \d observers
                       Table "public.observers"
   Column    |          Type          | Collation | Nullable | Default
-------------+------------------------+-----------+----------+---------
 observer_id | integer                |           | not null |
 login       | character varying(255) |           |          |
 name        | character varying(255) |           |          |
 origin      | character varying(255) |           |          |
 version     | character varying(255) |           |          |
 release     | character varying(255) |           |          |
Indexes:
    "index_observers_observers_id" btree (observer_id)
    "index_observers_origins" gin (to_tsvector('simple'::regconfig, origin::text))
    "index_observers_release" gin (to_tsvector('simple'::regconfig, release::text))
    "index_observers_version" gin (to_tsvector('simple'::regconfig, version::text))
```
            </file>
            <file path="export/v0/scrap/scrap.md">
# from before we rebuilt ancestor search

## NOTES/LIMITATIONS
- The upper boundary used for ancestor search, which is determine by the CLADE/METACLADE/MACROCLADE, is used for the regional-base tables ()"${REGION_TAG}_min${MIN_OBS}_all_sp_and_ancestors\"), not on one of the clade-export specific tables in cladistic.sh. So this is at odds with the previous design (before we added ancestor-aware logic), where previously the _all_sp table only varied by the REGION_TAG/MIN_OBS. This is probably OK, and might even be necessary for the purposes of determining the exact set of ancestral taxonIDs that need to be included in the base tables when looking to export more than just research-grade observations (i.e. observations with an uncontested species-level label) but it is a bit of a departure from the previous design. So we need to confirm what the set of taxa in the _all_sp_and_ancestors table depends upon (I think it is only the REGION_TAG/MIN_OBS/boundary ranks), and we can potentially mitigate by adjusting the generated base tables names to include the highest root rank (or highest root ranks, in the case of metaclades with multiple root ranks) used in the ancestor search; this will properly version the regional base tables and prevent reuse of base tables when the ancestor scopes differ.
  - So this means that the boundaries of the ancestor search for generating the regional _all_sp_and_ancestors is defined with respect to the configured clade/metaclade for a job, and so the regional base table might need to be recreated for successive job using clades/metaclades with different root ranks.
    - Really, the ancestor-aware logic should be implemented on the cladistic.sh tables.
    - The regional base table names do not fully capture the 'versining', so e.g. a NAfull_min50_all_sp_and_ancestors table generated from a PTA (metaclade) job would not be reusable for a successive job that used a MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)' macroclade, since the PTA root ranks are lower than the L60 rank-- so that regional base table would be missing L50 ancestors. 
      - This would actually be OK in theory but it might break some downstream assumptions, so it would be better to recreate the regional base table for each successive job if that job uses a different root rank.
      - TODO: Confirm that it is only the root rank, not the root taxonID, that is used to define the ancestor search for the regional base tables.
        - If the regional base table _all_sp_and_ancestors only varies by the REGION_TAG/MIN_OBS/boundary ranks, then we could mitigate by adjusting the generated base tables names to include the highest root rank used in the ancestor search.
        - Otherwise, we would need to include the CLADE/METACLADE/MACROCLADE in the regional base table name.
  - regional base table is an increasingly inappropraite name for this table. It was fine when the tables always just included the species in the region that passed the MIN_OBS threshold/the corresponding observations, but the contents of the table are now dependent on the CLADE/METACLADE/MACROCLADE.
    - This issue was averted for INCLUDE_OUT_OF_REGION_OBS, because the regional base observations table always include all observations for the species in the region that passed the MIN_OBS threshold (and now for all their ancestors in the scope of the ancestor search, too).
      - And then if INCLUDE_OUT_OF_REGION_OBS=false, then we re-applied the bounding box for the final table.
    - There might be a similar mitigation approach we could take for ancestor search here. A much more inclusive set of observations for, i.e. _all_sp_and_ancestors would include all species in the region that passed the MIN_OBS threshold and all the ancestors of those species up to but not including L100 (state of matter), i.e. unrestricted ancestor search. _sp_and_ancestors_obs would include all observations where taxon_id=[<a taxonID from _all_sp_and_ancestors].
      - By default, only search for the major-rank ancestors, i.e. L20, L30, L40, L50, L57, L60, L70. So INCLUDE_MINOR_RANKS_IN_ANCESTORS=false. If INCLUDE_MINOR_RANKS_IN_ANCESTORS=true, then include minor ranks in the unbounded ancestor search, and adjust the table names (_all_sp_and_ancestors_incl_minor_ranks, _sp_and_ancestors_obs_incl_minor_ranks). Searching minor ranks can occur significant performance penalty as only major ranks are indexed, and we will not need to use this in the intermediate future.
      - Possibly do a second ancestor search with only the set of ancestors that are up to the boundary rank for that export? This would be used for the filtering for the final table for that export job.
      - But then, for the final table, we'd need to apply a filter to exclude the observations that fall outside of the scope of the job/clade-specific ancestor search. It gets kind of complicated to define this filter-- do we consider the base taxonID of the observation? Not necessarily, since sometimes we wipe labels of some ranks if the no. observations for that taxonID are too low in the final step (depending on MIN_OCCURRENCES_PER_RANK, which is an alternate floor for wiping labels for higher ranks if the no. occurences for those taxonIDs are too low, while still keeping the observations if they have passing labels for some ranks that are not wiped, e.g. an observation has a non-research grade species label, so we wipe L10_taxonID to null, and it has a L20_taxonID label that is very rare, so occurs less than MIN_OCCURRENCES_PER_RANK in the final table, but it has a usable L30_taxonID label and we want to downstream model to be able to learn that a sample like 'that' should be learned as being in that family (L30).
    - My current standing is that we should do the ancestor search with the constrained upper boundary, and accept that we will lose some reusability between regional base tables between exports that use different clades/metaclades (but maybe can reuse if the ranks of the clades stay constant between jobs, need to confirm).
      - This simplifies the logic in cladistic.sh and preserves the functionality of the existing filters in cladistic.sh.

            </file>
          </dir>
        </dir>
      </dir>
    </dir>
    <dir path="xa">
      <dir path="xa/a">
        <file path="taxa/ColDP_raw_samples.txt">
<Distribution.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:areaID	col:area	col:gazetteer	col:status	col:referenceID	col:remarks
BLL2J		CN		iso	native		
BLL2J		TW		iso	native		
3LWT2		CR		iso	native		
444RB			Uruguay (Rivera)	text	native		
69PYD			North America	text	native		
8VVZD		MXC		tdwg	native		
8VVZD		MXG		tdwg	native		
8VVZD		MXE		tdwg	native		
8VVZD		MXT		tdwg	native		
</Distribution.tsv>
</head -n 10>

<Media.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:url	col:type	col:format	col:title	col:created	col:creator	col:license	col:link	col:remarks
</Media.tsv>
</head -n 10>

<metadata.yaml>
<head -n 10>
---
key: 299029
doi: 10.48580/dg9ld
title: Catalogue of Life
alias: COL24
description: |-
  The Catalogue of Life is an assembly of expert-based global species checklists with the aim to build a comprehensive catalogue of all known species of organisms on Earth. Continuous progress is made towards completion, but for now, it probably includes just over 80% of the world's known species. The Catalogue of Life estimates 2.3M extant species on the planet recognised by taxonomists at present time. This means that for many groups it continues to be deficient, and users may notice that many species are still missing from the Catalogue.

  ### What's new in 2024 Annual Checklist?

</metadata.yaml>
</head -n 10>

<NameRelation.tsv>
<head -n 10>
col:nameID	col:relatedNameID	col:sourceID	col:type	col:referenceID	col:page	col:remarks
CX5Y	6ZBQ2		basionym			
4DQ2L	6ZDLT		basionym			
4DQ2M	6ZDLY		basionym			
85QXH	BKZY		basionym			
4J2RF	6XKWB		basionym			
67CMV	CDYT5		basionym			
67CN7	4KGYM		basionym			
5VCMV	3L8K9		basionym			
67CMQ	6DV6D		basionym			
</NameRelation.tsv>
</head -n 10>

<NameUsage.tsv>
<head -n 10>
col:ID	col:alternativeID	col:nameAlternativeID	col:sourceID	col:parentID	col:basionymID	col:status	col:scientificName	col:authorship	col:rank	col:notho	col:originalSpelling	col:uninomial	col:genericName	col:infragenericEpithet	col:specificEpithet	col:infraspecificEpithet	col:cultivarEpithet	col:combinationAuthorship	col:combinationAuthorshipID	col:combinationExAuthorship	col:combinationExAuthorshipID	col:combinationAuthorshipYear	col:basionymAuthorship	col:basionymAuthorshipID	col:basionymExAuthorship	col:basionymExAuthorshipID	col:basionymAuthorshipYear	col:namePhrase	col:nameReferenceID	col:publishedInYear	col:publishedInPage	col:publishedInPageLink	col:gender	col:genderAgreement	col:etymology	col:code	col:nameStatus	col:accordingToID	col:accordingToPage	col:accordingToPageLink	col:referenceID	col:scrutinizer	col:scrutinizerID	col:scrutinizerDate	col:extinct	col:temporalRangeStart	col:temporalRangeEnd	col:environment	col:species	col:section	col:subgenus	col:genus	col:subtribe	col:tribe	col:subfamily	col:family	col:superfamily	col:suborder	col:order	col:subclass	col:class	col:subphylum	col:phylum	col:kingdom	col:ordinal	col:branchLength	col:link	col:nameRemarks	col:remarks
673FW			2232	3CP83	3CP83	synonym	Anisophyllum hyssopifolium	(L.) Haw.	species				Anisophyllum		hyssopifolium			Haw.					L.						0a8270ab-617f-402d-b3bf-c3f0fe6b0a0a							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:1042669-2		
9L2J9			2232	3F5B8	3F5B8	synonym	Asperula aparine var. aparine		variety				Asperula		aparine	aparine													308d3079-d3d3-4523-af56-80e4507eade8							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:77168438-1		
8RX53			1130	9WRMR		synonym	Saccoia escoffierae var. triangulata	Sacco, 1895	variety				Saccoia		escoffierae	triangulata		Sacco				1895							75d294da-5cc2-4241-b178-c5686ae93d97		42, pl. 1, fig. 116					zoological					28419c41-a4be-4567-9131-512c920576bd																										https://www.molluscabase.org/aphia.php?p=taxdetails&id=1545847		
555CQ			1141	C9FM		synonym	Telanthera flavescens	(Kunth) Moq.	species				Telanthera		flavescens			Moq.					Kunth						c59b73dc-e818-4ec0-8131-4f317c9139c6							botanical																																	
7TDYP			1130	3PP2B	9BD4Q	synonym	Triphora sarissa	Dall, 1889	species				Triphora		sarissa			Dall				1889														zoological																															https://www.molluscabase.org/aphia.php?p=taxdetails&id=1481479		
C33N7			1175	858KS		accepted	Amalocythere fulgida	Guan, 1978	species				Amalocythere		fulgida			Guan				1978							8db3f7fe-e3df-49ec-978c-717af25cea87							zoological	established				8db3f7fe-e3df-49ec-978c-717af25cea87	Brandão, Simone Nunes		2016-06-30	true			marine																			https://www.marinespecies.org/ostracoda/aphia.php?p=taxdetails&id=773107		
85BKX			2144	84JVC		accepted	Coronalpheus	Wicksten, 1999	genus			Coronalpheus						Wicksten				1999														zoological	acceptable				bc596268-7fff-4a3c-868b-3c392a7be32b			2021-04-26	false																						https://www.itis.gov/servlet/SingleRpt/SingleRpt?search_topic=TSN&search_value=1147818		
B3XCS			1141	5ZXM8		ambiguous synonym	Convolvulus rupestris	Buch	species				Convolvulus		rupestris			Buch											5f264aa0-ccf7-4da8-ad97-0ef3f4913f7e		193					botanical																																	
3HNHY			2232	8VXZW		accepted	Guzmania variegata	L.B.Sm.	species				Guzmania		variegata			L.B.Sm.											6683ea9d-d407-472a-8daa-943b551b3eaf							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:115257-2		Ecuador to Peru
</NameUsage.tsv>
</head -n 10>

<reference.json>
<head -n 10>
[
{"id":"1382","author":[{"family":"Weiss, Norbert, ed."}],"issued":{"date-parts":[[2001]]},"title":"Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM","container-title":"International Journal of Systematic and Evolutionary Microbiology, vol. 51, no. 2"},
{"id":"1383","author":[{"family":"Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al."}],"issued":{"date-parts":[[1992]]},"title":"Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b","container-title":"Archives in Microbiology, vol. 157"},
{"id":"1384","author":[{"family":"Partensky, F., W. R. Hess, and D. Vaulot"}],"issued":{"date-parts":[[1999]]},"title":"Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance","container-title":"Microbiology and Molecular Biology Reviews, vol. 63, no. 1"},
{"id":"171","author":[{"family":"Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.)"}],"issued":{"date-parts":[[1986]]},"container-title":"Bergey's Manual of Systematic Bacteriology. vol. 2"},
{"id":"1331","author":[{"family":"Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds."}],"issued":{"date-parts":[[1980]]},"title":"Approved Lists of Bacterial Names","container-title":"International Journal of Systematic Bacteriology, vol. 30, no.1"},
{"id":"4731","author":[{"family":"Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds."}],"issued":{"date-parts":[[1989]]},"container-title":"Approved Lists of Bacterial Names"},
{"author":[{"family":"S"}],"issued":{"date-parts":[[2000,1,1]]},"title":"SN2000/Levine, 1982"},
{"author":[{"family":"S"}],"issued":{"date-parts":[[2000,1,1]]},"title":"SN2000/Perkins, 2000"},
{"title":"As per Gigaductus"},
</reference.json>
</head -n 10>

<Reference.tsv>
<head -n 10>
col:ID	col:alternativeID	col:sourceID	col:citation	col:type	col:author	col:editor	col:title	col:titleShort	col:containerAuthor	col:containerTitle	col:containerTitleShort	col:issued	col:accessed	col:collectionTitle	col:collectionEditor	col:volume	col:issue	col:edition	col:page	col:publisher	col:publisherPlace	col:version	col:isbn	col:issn	col:doi	col:link	col:remarks
12ddc4b3-d551-4c38-8d20-4a2246b9e8d6		2144	Weiss, Norbert, ed. (2001). Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM. International Journal of Systematic and Evolutionary Microbiology, Vol. 51, No. 2.		Weiss, Norbert, ed.		Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM			International Journal of Systematic and Evolutionary Microbiology, vol. 51, no. 2		2001															
5eb14d90-e68f-4fda-be6c-453c6965707b		2144	Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al. (1992). Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b. Archives in Microbiology, Vol. 157.		Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al.		Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b			Archives in Microbiology, vol. 157		1992															
6b833931-90b9-4dca-b98c-d3e300075c7e		2144	Partensky, F., W. R. Hess, and D. Vaulot. (1999). Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance. Microbiology and Molecular Biology Reviews, Vol. 63, No. 1.		Partensky, F., W. R. Hess, and D. Vaulot		Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance			Microbiology and Molecular Biology Reviews, vol. 63, no. 1		1999															
c28f7f52-c223-4410-90ec-9142da3e34ab		2144	Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.). (1986). Bergey’s Manual of Systematic Bacteriology. Vol. 2.		Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.)					Bergey's Manual of Systematic Bacteriology. vol. 2		1986															
fa90c9ed-a347-4ae8-9feb-d2dec6bd8f97		2144	Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds. (1980). Approved Lists of Bacterial Names. International Journal of Systematic Bacteriology, Vol. 30, no.1.		Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds.		Approved Lists of Bacterial Names			International Journal of Systematic Bacteriology, vol. 30, no.1		1980															
be12e305-5fb7-4184-b33d-b6601e9c4140		2144	Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds. (1989). Approved Lists of Bacterial Names.		Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds.					Approved Lists of Bacterial Names		1989															
c75a73a2-0c85-4d63-a46a-c7f46d5f7913		2007	Seenivasan, R.; Sausen, N.; Medlin, L. K.; Melkonian, M. (2013). Picomonas judraskeda gen. et sp. nov.: the first identified member of the Picozoa phylum nov., a widespread group of picoeukaryotes, formerly known as 'picobiliphytes'. PLoS ONE. 8(3): e59565.																								
2ea6bf13-11f2-474f-b9ab-4e30333d3ade		2007	Article title: Phylogeny of novel naked filose and reticulose Cercozoa: Granofilosea cl. n. and Proteomyxidea revised.																								
535b1b3f-d59c-4da8-bc3b-abf664243b17		2007	Yabuki, A.; Chao, E. E.; Ishida, K.-I.; Cavalier-Smith, T. (2012). Microheliella maris (Microhelida ord. n.), an ultrastructurally highly distinctive new axopodial protist species and genus, and the unity of phylum Heliozoa. Protist. 163(3): 356-388.																								
</Reference.tsv>
</head -n 10>

<SpeciesEstimate.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:estimate	col:type	col:referenceID	col:remarks
BRKNQ		5385	species living	74	
622DH		317	species living	30	
4XK		85	species living	74	
FJ5		37	species living	30	
625XT		2	species living	55	
HJR		93	species living	30	
9LT		46	species living	30	
CZK		79	species living	55	
8NKJ8		71	species living	29	
</SpeciesEstimate.tsv>
</head -n 10>

<SpeciesInteraction.tsv>
<head -n 10>
col:taxonID	col:relatedTaxonID	col:sourceID	col:relatedTaxonScientificName	col:type	col:referenceID	col:remarks
</SpeciesInteraction.tsv>
</head -n 10>

<TaxonConceptRelation.tsv>
<head -n 10>
col:taxonID	col:relatedTaxonID	col:sourceID	col:type	col:referenceID	col:remarks
</TaxonConceptRelation.tsv>
</head -n 10>

<TaxonProperty.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:property	col:value	col:ordinal	col:referenceID	col:page	col:remarks
</TaxonProperty.tsv>
</head -n 10>

<TypeMaterial.tsv>
<head -n 10>
col:ID	col:nameID	col:sourceID	col:citation	col:status	col:referenceID	col:page	col:country	col:locality	col:latitude	col:longitude	col:altitude	col:sex	col:host	col:associatedSequences	col:date	col:collector	col:institutionCode	col:catalogNumber	col:link	col:remarks
</TypeMaterial.tsv>
</head -n 10>

<VernacularName.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:name	col:transliteration	col:language	col:preferred	col:country	col:area	col:sex	col:referenceID	col:remarks
B6LM6		bacteria	bacteria	eng						
B6LM6		bacterias	bacterias	spa						
R5LB		Sandcarpet	Sandcarpet	eng						
76SYT		Alcajes	Alcajes							
76SYT		Rajamatraca	Rajamatraca							
333V8		Patana	Patana	cbq						
6C59B		Clavellina	Clavellina							
3XSNW		Pitayita	Pitayita							
3XSJY		Cochilinque	Cochilinque							
</VernacularName.tsv>
</head -n 10>

<source/1005.yaml>
<head -n 50>
---
key: 1005
doi: 10.48580/dfrdl-37p
title: Catalogue of Craneflies of the World
alias: CCW
description: "The Catalogue of the Craneflies of the World (CCW) covers all genera,\
  \ subgenera, species, subspecies, and synonyms of the World Tipuloidea (Insecta\
  \ – Diptera – Nematocera – families Pediciidae, Limoniidae, Cylindrotomidae, Tipulidae).\
  \ It also includes up to date information on the distribution of the species and\
  \ subspecies, specified by countries and, for the larger countries, states, provinces\
  \ and islands. The website’s list of references has over 7.400 titles, about 4.300\
  \ of which available as easy downloadable pdf."
issued: 2021-05-07
version: May 2021
contact:
  given: P
  family: Oosterbroek
creator:
 -
  given: P.
  family: Oosterbroek
contributor:
 -
  city: Leiden
  country: NL
  address: "Leiden, Netherlands"
  organisation: Naturalis Biodiversity Center
keyword: []
containerKey: 299029
containerTitle: Catalogue of Life
containerCreator:
 -
  orcid: 0000-0001-6197-9951
  given: Olaf
  family: Bánki
  city: Amsterdam
  country: NL
  note: COL Managing Director
  address: "Amsterdam, Netherlands"
  organisation: Catalogue of Life
 -
  orcid: 0000-0003-2137-2690
  given: Yury
  family: Roskov
  city: Champaign
  state: IL
  country: US
  note: COL Executive Editor
  address: "Champaign, IL, United States of America"
  organisation: Illinois Natural History Survey				
</source/1005.yaml>
</head -n 50>

        </file>
        <file path="taxa/__init__.py">

        </file>
        <file path="taxa/analysis_utils.py">
from ibridaDB.schema import Observations, Taxa, TaxaTemp

def count_new_taxa(session, taxon_id):
    return session.query(TaxaTemp).filter(TaxaTemp.taxon_id == taxon_id, ~session.query(Taxa).filter(Taxa.taxon_id == taxon_id).exists()).count()

def count_deprecated_taxa(session, taxon_id):
    return session.query(Taxa).filter(Taxa.taxon_id == taxon_id, ~session.query(TaxaTemp).filter(TaxaTemp.taxon_id == taxon_id).exists()).count()

def count_active_status_changes(session, taxon_id):
    return session.query(Taxa, TaxaTemp).filter(
        Taxa.taxon_id == taxon_id,
        TaxaTemp.taxon_id == taxon_id,
        Taxa.active != TaxaTemp.active
    ).count()

def count_name_changes(session, taxon_id):
    return session.query(Taxa, TaxaTemp).filter(
        Taxa.taxon_id == taxon_id,
        TaxaTemp.taxon_id == taxon_id,
        Taxa.name != TaxaTemp.name
    ).count()

def count_other_attribute_changes(session, taxon_id):
    return session.query(Taxa, TaxaTemp).filter(
        Taxa.taxon_id == taxon_id,
        TaxaTemp.taxon_id == taxon_id,
        (Taxa.ancestry != TaxaTemp.ancestry) |
        (Taxa.rank_level != TaxaTemp.rank_level) |
        (Taxa.rank != TaxaTemp.rank)
    ).count()

def count_observations_for_taxa(session, taxon_id, category):
    if category == 'new':
        return session.query(Observations).join(TaxaTemp, Observations.taxon_id == TaxaTemp.taxon_id).filter(
            TaxaTemp.taxon_id == taxon_id,
            ~session.query(Taxa).filter(Taxa.taxon_id == taxon_id).exists()
        ).count()
    elif category == 'deprecated':
        return session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).filter(
            Taxa.taxon_id == taxon_id,
            ~session.query(TaxaTemp).filter(TaxaTemp.taxon_id == taxon_id).exists()
        ).count()
    elif category == 'active_status_changes':
        return session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
            Taxa.taxon_id == taxon_id,
            Taxa.active != TaxaTemp.active
        ).count()

def count_observations_for_common_taxa(session, taxon_id, category):
    if category == 'new':
        return session.query(Observations).join(TaxaTemp, Observations.taxon_id == TaxaTemp.taxon_id).filter(
            TaxaTemp.taxon_id == taxon_id,
            ~session.query(Taxa).filter(Taxa.taxon_id == taxon_id).exists(),
            session.query(Observations).filter(Observations.taxon_id == TaxaTemp.taxon_id).count() > 180
        ).count()
    elif category == 'deprecated':
        return session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).filter(
            Taxa.taxon_id == taxon_id,
            ~session.query(TaxaTemp).filter(TaxaTemp.taxon_id == taxon_id).exists(),
            session.query(Observations).filter(Observations.taxon_id == Taxa.taxon_id).count() > 180
        ).count()
    elif category == 'active_status_changes':
        return session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
            Taxa.taxon_id == taxon_id,
            Taxa.active != TaxaTemp.active,
            session.query(Observations).filter(Observations.taxon_id == Taxa.taxon_id).count() > 180
        ).count()
        </file>
        <file path="taxa/analyze_diff.py" line_interval="25">
import os
import csv
import argparse
from sqlalchemy import create_engine, Column, Integer, String, Float, Date, Boolean, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from ibridaDB.schema import Observations, Photos, Taxa, TaxaTemp, Observers
from ibridaDB.taxa.analysis_utils import (
    count_new_taxa,
    count_deprecated_taxa,
    count_active_status_changes,
    count_name_changes,
    count_other_attribute_changes,
    count_observations_for_taxa,
    count_observations_for_common_taxa
)
import logging


# Setup basic configuration for logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def create_db_engine(db_user, db_password, db_host, db_port, db_name):
#|LN|25|
    connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    return create_engine(connection_string)


def create_temp_taxa_table(session):
    TaxaTemp.__table__.create(session.bind, checkfirst=True)


def drop_temp_taxa_table(session):
    TaxaTemp.__table__.drop(session.bind, checkfirst=True)


def load_temp_taxa_data(origin, session, csv_file_path):
    with open(csv_file_path, 'r') as f:
        next(f)  # Skip the header row
        reader = csv.reader(f, delimiter='\t', quotechar='\b')
        for row in reader:
            if len(row) < 6:
                logging.warning(f"Skipping incomplete row: {row}")
                continue
            try:
                new_taxa = TaxaTemp(
                    taxon_id=int(row[0]),
                    ancestry=row[1],
                    rank_level=float(row[2]) if row[2] else None,
#|LN|50|
                    rank=row[3],
                    name=row[4],
                    active=row[5].lower() == 'true'
                )
                session.add(new_taxa)
            except ValueError as e:
                logging.error(f"Error processing row {row}: {e}")
                session.rollback()
                continue
    session.commit()


def analyze_specific_taxa_changes(origin, session, taxa_groups):
    output_dir = f"diffs/{origin}"
    os.makedirs(output_dir, exist_ok=True)

    for rank_level, taxa_ids in taxa_groups.items():
        print(f"Analyzing changes for {rank_level} taxa...")
        with open(f"{output_dir}/{rank_level}_analysis.txt", "w") as f:
            f.write(f"Analysis for {rank_level} taxa:\n\n")

            for taxon_id in taxa_ids:
                print(f"Analyzing changes for taxon ID: {taxon_id}")
                f.write(f"Taxon ID: {taxon_id}\n")

#|LN|75|
                new_taxa_count = count_new_taxa(session, taxon_id)
                f.write(f"New taxa count: {new_taxa_count}\n")

                deprecated_taxa_count = count_deprecated_taxa(session, taxon_id)
                f.write(f"Deprecated taxa count: {deprecated_taxa_count}\n")

                active_status_changes_count = count_active_status_changes(session, taxon_id)
                f.write(f"Active status changes count: {active_status_changes_count}\n")

                name_changes_count = count_name_changes(session, taxon_id)
                f.write(f"Name changes count: {name_changes_count}\n")

                other_attribute_changes_count = count_other_attribute_changes(session, taxon_id)
                f.write(f"Other attribute changes count: {other_attribute_changes_count}\n")

                observations_new_taxa_count = count_observations_for_taxa(session, taxon_id, 'new')
                f.write(f"Observations with new taxa count: {observations_new_taxa_count}\n")

                observations_deprecated_taxa_count = count_observations_for_taxa(session, taxon_id, 'deprecated')
                f.write(f"Observations with deprecated taxa count: {observations_deprecated_taxa_count}\n")

                observations_active_status_changes_count = count_observations_for_taxa(session, taxon_id, 'active_status_changes')
                f.write(f"Observations with active status changes count: {observations_active_status_changes_count}\n")

                observations_common_new_taxa_count = count_observations_for_common_taxa(session, taxon_id, 'new')
#|LN|100|
                f.write(f"Observations with common new taxa count: {observations_common_new_taxa_count}\n")

                observations_common_deprecated_taxa_count = count_observations_for_common_taxa(session, taxon_id, 'deprecated')
                f.write(f"Observations with common deprecated taxa count: {observations_common_deprecated_taxa_count}\n")

                observations_common_active_status_changes_count = count_observations_for_common_taxa(session, taxon_id, 'active_status_changes')
                f.write(f"Observations with common active status changes count: {observations_common_active_status_changes_count}\n")

                f.write("\n")


def analyze_taxa_changes(origin, session, output_dir):
    print("Analyzing overall taxa changes...")
    
    print("Counting taxon IDs in the new taxa data...")
    new_taxon_count = session.query(TaxaTemp).count()
    with open(f"{output_dir}/new_taxon_count.csv", "w") as f:
        f.write(f"new_taxon_count\n{new_taxon_count}\n")

    print("Finding deprecated taxon IDs...")
    deprecated_taxon_ids = session.query(Taxa.taxon_id).filter(~Taxa.taxon_id.in_(session.query(TaxaTemp.taxon_id))).all()
    with open(f"{output_dir}/deprecated_taxon_ids.csv", "w") as f:
        f.write("taxon_id\n")
        for taxon_id in deprecated_taxon_ids:
            f.write(f"{taxon_id[0]}\n")
#|LN|125|

    print("Finding new taxon IDs...")
    new_taxon_ids = session.query(TaxaTemp.taxon_id, TaxaTemp.ancestry, TaxaTemp.rank_level, TaxaTemp.active).filter(~TaxaTemp.taxon_id.in_(session.query(Taxa.taxon_id))).all()
    with open(f"{output_dir}/new_taxon_ids.csv", "w") as f:
        f.write("taxon_id,ancestry,rank_level,active\n")
        for taxon_id, ancestry, rank_level, active in new_taxon_ids:
            f.write(f"{taxon_id},{ancestry},{rank_level},{active}\n")

    print("Finding taxon IDs with changed attributes...")
    changed_attributes = session.query(
        Taxa.taxon_id,
        Taxa.ancestry, TaxaTemp.ancestry,
        Taxa.rank_level, TaxaTemp.rank_level,
        Taxa.rank, TaxaTemp.rank,
        Taxa.name, TaxaTemp.name,
        Taxa.active, TaxaTemp.active
    ).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
        (Taxa.ancestry != TaxaTemp.ancestry) |
        (Taxa.rank_level != TaxaTemp.rank_level) |
        (Taxa.rank != TaxaTemp.rank) |
        (Taxa.name != TaxaTemp.name) |
        (Taxa.active != TaxaTemp.active)
    ).all()

    with open(f"{output_dir}/changed_attributes.csv", "w") as f:
#|LN|150|
        f.write("taxon_id,existing_ancestry,new_ancestry,existing_rank_level,new_rank_level,existing_rank,new_rank,existing_name,new_name,existing_active,new_active\n")
        for row in changed_attributes:
            f.write(",".join(str(value) for value in row) + "\n")

    print("Listing taxon IDs with changed 'active' values...")
    active_status_changes = session.query(
        Taxa.taxon_id,
        Taxa.rank_level,
        Taxa.active,
        TaxaTemp.active
    ).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
        Taxa.active != TaxaTemp.active
    ).all()

    with open(f"{output_dir}/active_status_changes.csv", "w") as f:
        f.write("taxon_id,rank_level,existing_active,new_active\n")
        for taxon_id, rank_level, existing_active, new_active in active_status_changes:
            f.write(f"{taxon_id},{rank_level},{existing_active},{new_active}\n")

    print("Listing taxon IDs with changed 'name' values...")
    name_changes = session.query(
        Taxa.taxon_id,
        Taxa.rank_level,
        Taxa.name,
        TaxaTemp.name
#|LN|175|
    ).join(TaxaTemp, Taxa.taxon_id == TaxaTemp.taxon_id).filter(
        Taxa.name != TaxaTemp.name
    ).all()

    with open(f"{output_dir}/name_changes.csv", "w") as f:
        f.write("taxon_id,rank_level,existing_name,new_name\n")
        for taxon_id, rank_level, existing_name, new_name in name_changes:
            f.write(f"{taxon_id},{rank_level},{existing_name},{new_name}\n")

    print("Counting observations with inactive taxon IDs...")
    inactive_observations_count = session.query(Observations).join(Taxa, Observations.taxon_id == Taxa.taxon_id).filter(Taxa.active == False).count()
    with open(f"{output_dir}/inactive_observations_count.csv", "w") as f:
        f.write(f"inactive_observations_count\n{inactive_observations_count}\n")


def main():
    parser = argparse.ArgumentParser(description="Analyze taxa changes between existing database and new CSV")
    parser.add_argument("--origin", required=True, help="Date code of the new taxa CSV (e.g., May2024)")
    parser.add_argument("--db-user", default="postgres", help="Database user")
    parser.add_argument("--db-password", default="password", help="Database password")
    parser.add_argument("--db-host", default="localhost", help="Database host")
    parser.add_argument("--db-port", default="5432", help="Database port")
    parser.add_argument("--db-name", default="postgres", help="Database name")
    parser.add_argument("--csv-file-path", help="Path to the new taxa CSV file")
    parser.add_argument("--use-existing-temp-table", action="store_true", help="Use existing TaxaTemp table instead of loading a new one")
#|LN|200|
    parser.add_argument("--clear-temp", action="store_true", help="Drop the existing TaxaTemp table if it exists")

    args = parser.parse_args()

    engine = create_db_engine(args.db_user, args.db_password, args.db_host, args.db_port, args.db_name)
    Session = sessionmaker(bind=engine)
    session = Session()

    if args.clear_temp:
        if args.use_existing_temp_table:
            logging.error("Cannot use both --clear-temp and --use-existing-temp-table flags together.")
            return
        drop_temp_taxa_table(session)
        create_temp_taxa_table(session)
        if args.csv_file_path:
            load_temp_taxa_data(args.origin, session, args.csv_file_path)
        else:
            csv_file_path = f'/ibrida/metadata/{args.origin}/taxa.csv'
            load_temp_taxa_data(args.origin, session, csv_file_path)
    else:
        create_temp_taxa_table(session)
        if not args.use_existing_temp_table:
            if args.csv_file_path:
                load_temp_taxa_data(args.origin, session, args.csv_file_path)
            else:
#|LN|225|
                csv_file_path = f'/ibrida/metadata/{args.origin}/taxa.csv'
                load_temp_taxa_data(args.origin, session, csv_file_path)

    output_dir = f"diffs/{args.origin}"
    os.makedirs(output_dir, exist_ok=True)

    analyze_taxa_changes(args.origin, session, output_dir)

    taxa_groups = {
        "L60": [47120],
        "L50": [47163, 47124, 40151, 3, 26036, 20978, 47119, 47158],
        "L40": [47744, 47157, 47792, 47651, 47208, 47822, 47201]
    }
    analyze_specific_taxa_changes(args.origin, session, taxa_groups)

    session.close()


if __name__ == "__main__":
    main()
        </file>
        <file path="taxa/reference.md">
# Taxon ranks
## code_to_name
*maps taxon rank polli-style code to rank names*
code_to_name = {
    'L5': 'subspecies',
    'L10': 'species',
    'L11': 'complex',
    'L12': 'subsection', 
    'L13': 'section',
    'L15': 'subgenus',
    'L20': 'genus',
    'L24': 'subtribe',
    'L25': 'tribe',
    'L26': 'supertribe',
    'L27': 'subfamily',
    'L30': 'family',
    'L32': 'epifamily',
    'L33': 'superfamily',
    'L33_5': 'zoosubsection',
    'L34': 'zoosection',
    'L34_5': 'parvorder',
    'L35': 'infraorder',
    'L37': 'suborder',
    'L40': 'order',
    'L43': 'superorder',
    'L44': 'subterclass',
    'L45': 'infraclass',
    'L47': 'subclass',
    'L50': 'class',
    'L53': 'superclass',
    'L57': 'subphylum',
    'L60': 'phylum',
    'L67': 'subkingdom',
    'L70': 'kingdom'
}
### ambiguous ranks
We assume that the possibly ambiguous ranks are of the above ranks downstream. However, note that the following levels could be ambiguous:
*Possible ranks:*
- L5: form, infrahybrid, subspecies
- L10: hybrid, species
- L20: genus, genushybrid
        </file>
        <dir path="xa/a/diffs">
          <dir path="xa/a/diffs/May2024">
            <file path="taxa/diffs/May2024/L40_analysis.txt">
Analysis for L40 taxa:

Taxon ID: 47744
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47157
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47792
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47651
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47208
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47822
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47201
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0


            </file>
            <file path="taxa/diffs/May2024/L50_analysis.txt">
Analysis for L50 taxa:

Taxon ID: 47163
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47124
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 40151
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 3
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 26036
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 20978
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47119
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0

Taxon ID: 47158
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0


            </file>
            <file path="taxa/diffs/May2024/L60_analysis.txt">
Analysis for L60 taxa:

Taxon ID: 47120
New taxa count: 0
Deprecated taxa count: 0
Active status changes count: 0
Name changes count: 0
Other attribute changes count: 0
Observations with new taxa count: 0
Observations with deprecated taxa count: 0
Observations with active status changes count: 0
Observations with common new taxa count: 0
Observations with common deprecated taxa count: 0
Observations with common active status changes count: 0


            </file>
          </dir>
        </dir>
        <dir path="xa/a/expand">
          <file path="taxa/expand/expand_taxa.sh" line_interval="25">
#!/usr/bin/env bash
# -----------------------------------------------------------------------------
# expand_taxa.sh
# -----------------------------------------------------------------------------
# Creates a new "expanded_taxa" table by expanding ancestry from the existing
# "taxa" table into structured columns ("L{level}_taxonID", "L{level}_name", etc.).
#
# This version uses *string concatenation* with quote_ident(...) and quote_nullable(...),
# bypassing placeholders entirely. This is the "sure" approach: no risk of
# placeholders vanishing, since we embed the actual values directly into the
# final SQL string.
#
# Steps:
#   1) Drop 'expanded_taxa' if exists; create base columns with quotes.
#   2) Add columns for each rank level ("L5_taxonID", "L5_name", etc.).
#   3) Create expand_taxa_procedure() as a function:
#      - We skip rank levels not in RANK_LEVELS (no 100).
#      - If debugging is enabled (DEBUG_EXPAND_TAXA=true), we RAISE NOTICE
#        about the row's data and the final SQL statement.
#      - We *string-concatenate* the column references, so no placeholders are used.
#   4) SELECT expand_taxa_procedure() to populate the table.
#   5) Create indexes on "L10_taxonID"... "L70_taxonID", plus "taxonID", "rankLevel", "name".
#   6) VACUUM (ANALYZE), notifications, done.
#
#|LN|25|
# Usage:
#   DEBUG_EXPAND_TAXA=true ./expand_taxa.sh
# -----------------------------------------------------------------------------

# ===[ 1) Setup & Logging ]====================================================
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Log messages (with timestamps) to both console and file
log_message() {
    local timestamp
    timestamp="$(date +'%Y-%m-%dT%H:%M:%S%z')"
    echo "[$timestamp] $1" | tee -a "${LOG_FILE}"
}

# Redirect stdout/stderr to console+log
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

# Source your common functions (for execute_sql, send_notification, etc.)
source "/home/caleb/repo/ibridaDB/dbTools/export/v0/common/functions.sh"

# Environment / defaults
DB_CONTAINER="${DB_CONTAINER:-ibridaDB}"
#|LN|50|
DB_NAME="${DB_NAME:-ibrida-v0-r1}"
DB_USER="${DB_USER:-postgres}"

# If DEBUG_EXPAND_TAXA=true, we pass a GUC variable into Postgres to enable debug
DEBUG_EXPAND="${DEBUG_EXPAND_TAXA:-false}"  # "true" or "false"

# rank levels to expand (no 100). We'll handle 5, 10, 11, ... 70
RANK_LEVELS=(5 10 11 12 13 15 20 24 25 26 27 30 32 33 33.5 34 34.5 35 37 40 43 44 45 47 50 53 57 60 67 70)

# We'll create indexes only on L10..L70
INDEX_LEVELS=(10 20 30 40 50 60 70)

log_message "Beginning expand_taxa.sh for DB: ${DB_NAME} (DEBUG_EXPAND_TAXA=${DEBUG_EXPAND})"

# ===[ 2) Create expanded_taxa schema ]========================================
log_message "Step 1: Dropping old expanded_taxa and creating base columns with quotes."

execute_sql "
DROP TABLE IF EXISTS \"expanded_taxa\" CASCADE;
CREATE TABLE \"expanded_taxa\" (
    \"taxonID\"       INTEGER PRIMARY KEY,
    \"rankLevel\"     DOUBLE PRECISION,
    \"rank\"          VARCHAR(255),
    \"name\"          VARCHAR(255),
    \"taxonActive\"   BOOLEAN
#|LN|75|
    -- We'll add \"LXX_taxonID\", \"LXX_name\", \"LXX_commonName\" columns next
);
"

# ===[ 3) Add columns for each rank level ]====================================
log_message "Step 2: Adding L{level}_taxonID, L{level}_name, L{level}_commonName columns."

ADD_COLS=""
for L in "${RANK_LEVELS[@]}"; do
  SAFE_L=$(echo "${L}" | sed 's/\./_/g')
  ADD_COLS+=" ADD COLUMN \"L${SAFE_L}_taxonID\" INTEGER,
             ADD COLUMN \"L${SAFE_L}_name\" VARCHAR(255),
             ADD COLUMN \"L${SAFE_L}_commonName\" VARCHAR(255),"
done

# Remove trailing comma
ADD_COLS="${ADD_COLS%,}"

execute_sql "
ALTER TABLE \"expanded_taxa\"
${ADD_COLS};
"

# ===[ 4) Create expand_taxa_procedure() function ]============================
log_message "Step 3: Creating expand_taxa_procedure() with string-concatenation for dynamic columns."
#|LN|100|

# We'll incorporate a GUC "myapp.debug_expand" to signal debug mode in PL/pgSQL
if [ "${DEBUG_EXPAND}" = "true" ]; then
  execute_sql "SET myapp.debug_expand = 'on';"
else
  execute_sql "SET myapp.debug_expand = 'off';"
fi

execute_sql "
DROP FUNCTION IF EXISTS expand_taxa_procedure() CASCADE;

CREATE OR REPLACE FUNCTION expand_taxa_procedure()
RETURNS void
LANGUAGE plpgsql
AS \$\$
DECLARE
    t_rec RECORD;
    ancestor_ids TEXT[];
    this_ancestor TEXT;
    anc_data RECORD;
    effective_level TEXT;
    row_sql TEXT;
    debugging boolean := false;
BEGIN
    -- We'll read our GUC to see if debug is on
#|LN|125|
    BEGIN
        IF current_setting('myapp.debug_expand') = 'on' THEN
            debugging := true;
        END IF;
    EXCEPTION
        WHEN others THEN
            debugging := false;  -- if the GUC is not set, do nothing
    END;

    -- Only retrieve active taxa rows, ignoring inactive ones
    FOR t_rec IN
        SELECT taxon_id, ancestry, rank_level, rank, name, active
        FROM taxa
        WHERE active = true
    LOOP
        -- Insert base row
        INSERT INTO \"expanded_taxa\"(\"taxonID\", \"rankLevel\", \"rank\", \"name\", \"taxonActive\")
        VALUES (t_rec.taxon_id, t_rec.rank_level, t_rec.rank, t_rec.name, t_rec.active);

        IF t_rec.ancestry IS NOT NULL AND t_rec.ancestry <> '' THEN
            ancestor_ids := string_to_array(t_rec.ancestry, '/');
        ELSE
            ancestor_ids := ARRAY[]::TEXT[];
        END IF;

#|LN|150|
        -- Include self
        ancestor_ids := ancestor_ids || t_rec.taxon_id::TEXT;

        FOREACH this_ancestor IN ARRAY ancestor_ids
        LOOP
            BEGIN
                IF this_ancestor IS NULL THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping NULL ancestor for row taxon_id=%', t_rec.taxon_id;
                    END IF;
                    CONTINUE;
                END IF;

                SELECT rank_level, rank, name
                  INTO anc_data
                  FROM taxa
                 WHERE taxon_id = this_ancestor::INTEGER
                 LIMIT 1;

                IF NOT FOUND OR anc_data.name IS NULL THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping ancestor=% for row taxon_id=%: not found or name is NULL', this_ancestor, t_rec.taxon_id;
                    END IF;
                    CONTINUE;
                END IF;
#|LN|175|

                IF anc_data.rank_level NOT IN (
                    5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30,
                    32, 33, 33.5, 34, 34.5, 35, 37, 40, 43, 44, 45,
                    47, 50, 53, 57, 60, 67, 70
                ) THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping rank_level=% for row taxon_id=% (ancestor=%)', anc_data.rank_level, t_rec.taxon_id, this_ancestor;
                    END IF;
                    CONTINUE;
                END IF;

                effective_level := replace(CAST(anc_data.rank_level AS TEXT), '.', '_');

                -- Build dynamic SQL via string concat + quote_ident(...) + quote_nullable(...)
                row_sql :=
                    'UPDATE \"expanded_taxa\" SET '
                    || quote_ident('L' || effective_level || '_taxonID') || ' = '
                        || quote_nullable(this_ancestor)
                    || ', '
                    || quote_ident('L' || effective_level || '_name') || ' = '
                        || quote_nullable(anc_data.name)
                    || ' WHERE \"taxonID\" = ' || quote_nullable(t_rec.taxon_id::text);

                IF debugging THEN
#|LN|200|
                    RAISE NOTICE 'Row taxon_id=% => rank_level=% => built SQL: %',
                                 t_rec.taxon_id, anc_data.rank_level, row_sql;
                END IF;

                EXECUTE row_sql;

            EXCEPTION WHEN OTHERS THEN
                RAISE NOTICE 'Error updating row => base taxon_id=%, ancestor=%, anc_data=(%,%,%), row_sql=[%]',
                  t_rec.taxon_id, this_ancestor, anc_data.rank_level, anc_data.rank, anc_data.name, row_sql;
                RAISE;
            END;
        END LOOP;
    END LOOP;
END;
\$\$;
"

# ===[ 5) Populate expanded_taxa ]=============================================
log_message "Step 4: SELECT expand_taxa_procedure() to populate."

execute_sql "
SELECT expand_taxa_procedure();
"

log_message "Population of expanded_taxa complete. Running: \\d \"expanded_taxa\""
#|LN|225|
execute_sql "\d \"expanded_taxa\""

send_notification "expand_taxa.sh: Step 4 complete (expanded_taxa populated)."

# ===[ 6) Create indexes (only on L10..L70) ]===================================
log_message "Step 5: Creating indexes on L10_taxonID, L20_taxonID, ..., L70_taxonID plus base columns."

for L in "${INDEX_LEVELS[@]}"; do
  SAFE_L=$(echo "${L}" | sed 's/\./_/g')
  execute_sql "
  CREATE INDEX IF NOT EXISTS idx_expanded_taxa_L${SAFE_L}_taxonID
    ON \"expanded_taxa\"(\"L${SAFE_L}_taxonID\");
  "
done

execute_sql "
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_taxonID    ON \"expanded_taxa\"(\"taxonID\");
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_rankLevel  ON \"expanded_taxa\"(\"rankLevel\");
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_name       ON \"expanded_taxa\"(\"name\");
"

log_message "Index creation done. Running: \\d \"expanded_taxa\""
execute_sql "\d \"expanded_taxa\""

send_notification "expand_taxa.sh: Step 5 complete (indexes created)."
#|LN|250|

# ===[ 7) VACUUM ANALYZE ]====================================================
log_message "Step 6: VACUUM ANALYZE \"expanded_taxa\" (final step)."

execute_sql "
VACUUM (ANALYZE) \"expanded_taxa\";
"

send_notification "expand_taxa.sh: Step 6 complete (VACUUM ANALYZE done)."
log_message "expand_taxa.sh complete. Exiting."
          </file>
        </dir>
        <dir path="xa/a/models">
          <file path="taxa/models/__init__.py">
from .expanded_taxa import TaxaExpanded
from .expanded_taxa_cmn import ExpandedTaxaCmn
from .coldp_models import (
    ColdpVernacularName,
    ColdpDistribution, 
    ColdpMedia,
    ColdpReference,
    ColdpTypeMaterial
)

__all__ = [
    'TaxaExpanded',
    'ExpandedTaxaCmn',
    'ColdpVernacularName',
    'ColdpDistribution',
    'ColdpMedia',
    'ColdpReference',
    'ColdpTypeMaterial'
]
          </file>
          <file path="taxa/models/coldp_models.py">
from sqlalchemy import (
    Column, String, Text, Boolean, Date, Numeric, Integer
)
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class ColdpVernacularName(Base):
    __tablename__ = "coldp_vernacular_name"
    taxonID        = Column(String(10), primary_key=True)
    sourceID       = Column(String(10))
    name           = Column(Text, nullable=False)
    transliteration= Column(Text)
    language       = Column(String(3))      # ISO‑639‑3
    preferred      = Column(Boolean)
    country        = Column(String(2))      # ISO‑3166‑1‑alpha‑2
    area           = Column(Text)
    sex            = Column(String(20))
    referenceID    = Column(String(64))
    remarks        = Column(Text)

class ColdpDistribution(Base):
    __tablename__ = "coldp_distribution"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(10), index=True)
    sourceID       = Column(String(10))
    areaID         = Column(String(10))
    area           = Column(Text)
    gazetteer      = Column(String(10))
    status         = Column(String(25))     # e.g. native, introduced
    referenceID    = Column(String(64))
    remarks        = Column(Text)

class ColdpMedia(Base):
    __tablename__ = "coldp_media"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(10), index=True)
    sourceID       = Column(String(10))
    url            = Column(Text, nullable=False)
    type           = Column(String(50))     # stillImage, sound, video …
    format         = Column(String(50))     # MIME type or file suffix
    title          = Column(Text)
    created        = Column(Date)
    creator        = Column(Text)
    license        = Column(String(100))
    link           = Column(Text)           # landing page
    remarks        = Column(Text)

class ColdpReference(Base):
    __tablename__ = "coldp_reference"
    ID             = Column(String(64), primary_key=True)   # UUID or short key
    alternativeID  = Column(String(64))
    sourceID       = Column(String(10))
    citation       = Column(Text)
    type           = Column(String(30))
    author         = Column(Text)
    editor         = Column(Text)
    title          = Column(Text)
    titleShort     = Column(Text)
    containerAuthor= Column(Text)
    containerTitle = Column(Text)
    containerTitleShort = Column(Text)
    issued         = Column(String(50))
    accessed       = Column(String(50))
    collectionTitle= Column(Text)
    collectionEditor= Column(Text)
    volume         = Column(String(30))
    issue          = Column(String(30))
    edition        = Column(String(30))
    page           = Column(String(50))
    publisher      = Column(Text)
    publisherPlace = Column(Text)
    version        = Column(String(30))
    isbn           = Column(String(20))
    issn           = Column(String(20))
    doi            = Column(String(100))
    link           = Column(Text)
    remarks        = Column(Text)

class ColdpTypeMaterial(Base):
    """
    ColDP entity `TypeMaterial` (called TypeSpecimen in the user request).
    """
    __tablename__ = "coldp_type_material"
    ID              = Column(String(64), primary_key=True)
    nameID          = Column(String(10), index=True)
    sourceID        = Column(String(10))
    citation        = Column(Text)
    status          = Column(String(50))
    referenceID     = Column(String(64))
    page            = Column(String(50))
    country         = Column(String(2))
    locality        = Column(Text)
    latitude        = Column(Numeric(9,5))
    longitude       = Column(Numeric(9,5))
    altitude        = Column(String(50))
    sex             = Column(String(12))
    host            = Column(Text)
    associatedSequences = Column(Text)
    date            = Column(Date)
    collector       = Column(Text)
    institutionCode = Column(String(25))
    catalogNumber   = Column(String(50))
    link            = Column(Text)
    remarks         = Column(Text)
          </file>
          <file path="taxa/models/expanded_taxa.py">
from sqlalchemy import (
    Column, Integer, String, Text, Boolean, Float, Index
)
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class TaxaExpanded(Base):
    __tablename__ = "expanded_taxa"

    taxonID       = Column(Integer, primary_key=True, nullable=False)
    rankLevel     = Column(Float, index=True)
    rank          = Column(String(255))
    name          = Column(String(255), index=True)
    taxonActive   = Column(Boolean, index=True)

    # Ancestral columns
    L5_taxonID    = Column(Integer)
    L5_name       = Column(String(255))
    L5_commonName = Column(String(255))
    L10_taxonID    = Column(Integer)
    L10_name       = Column(String(255))
    L10_commonName = Column(String(255))
    L11_taxonID    = Column(Integer)
    L11_name       = Column(String(255))
    L11_commonName = Column(String(255))
    L12_taxonID    = Column(Integer)
    L12_name       = Column(String(255))
    L12_commonName = Column(String(255))
    L13_taxonID    = Column(Integer)
    L13_name       = Column(String(255))
    L13_commonName = Column(String(255))
    L15_taxonID    = Column(Integer)
    L15_name       = Column(String(255))
    L15_commonName = Column(String(255))
    L20_taxonID    = Column(Integer)
    L20_name       = Column(String(255))
    L20_commonName = Column(String(255))
    L24_taxonID    = Column(Integer)
    L24_name       = Column(String(255))
    L24_commonName = Column(String(255))
    L25_taxonID    = Column(Integer)
    L25_name       = Column(String(255))
    L25_commonName = Column(String(255))
    L26_taxonID    = Column(Integer)
    L26_name       = Column(String(255))
    L26_commonName = Column(String(255))
    L27_taxonID    = Column(Integer)
    L27_name       = Column(String(255))
    L27_commonName = Column(String(255))
    L30_taxonID    = Column(Integer)
    L30_name       = Column(String(255))
    L30_commonName = Column(String(255))
    L32_taxonID    = Column(Integer)
    L32_name       = Column(String(255))
    L32_commonName = Column(String(255))
    L33_taxonID    = Column(Integer)
    L33_name       = Column(String(255))
    L33_commonName = Column(String(255))
    L33_5_taxonID    = Column(Integer)
    L33_5_name       = Column(String(255))
    L33_5_commonName = Column(String(255))
    L34_taxonID    = Column(Integer)
    L34_name       = Column(String(255))
    L34_commonName = Column(String(255))
    L34_5_taxonID    = Column(Integer)
    L34_5_name       = Column(String(255))
    L34_5_commonName = Column(String(255))
    L35_taxonID    = Column(Integer)
    L35_name       = Column(String(255))
    L35_commonName = Column(String(255))
    L37_taxonID    = Column(Integer)
    L37_name       = Column(String(255))
    L37_commonName = Column(String(255))
    L40_taxonID    = Column(Integer)
    L40_name       = Column(String(255))
    L40_commonName = Column(String(255))
    L43_taxonID    = Column(Integer)
    L43_name       = Column(String(255))
    L43_commonName = Column(String(255))
    L44_taxonID    = Column(Integer)
    L44_name       = Column(String(255))
    L44_commonName = Column(String(255))
    L45_taxonID    = Column(Integer)
    L45_name       = Column(String(255))
    L45_commonName = Column(String(255))
    L47_taxonID    = Column(Integer)
    L47_name       = Column(String(255))
    L47_commonName = Column(String(255))
    L50_taxonID    = Column(Integer)
    L50_name       = Column(String(255))
    L50_commonName = Column(String(255))
    L53_taxonID    = Column(Integer)
    L53_name       = Column(String(255))
    L53_commonName = Column(String(255))
    L57_taxonID    = Column(Integer)
    L57_name       = Column(String(255))
    L57_commonName = Column(String(255))
    L60_taxonID    = Column(Integer)
    L60_name       = Column(String(255))
    L60_commonName = Column(String(255))
    L67_taxonID    = Column(Integer)
    L67_name       = Column(String(255))
    L67_commonName = Column(String(255))
    L70_taxonID    = Column(Integer)
    L70_name       = Column(String(255))
    L70_commonName = Column(String(255))

# Important indexes for lookups
Index("idx_expanded_taxa_L10_taxonID", "L10_taxonID")
          </file>
          <file path="taxa/models/expanded_taxa_cmn.py">
from sqlalchemy import (
    Column, Integer, String, Text, Boolean, Float, Index
)
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class ExpandedTaxaCmn(Base):
    """
    Copy of expanded_taxa with additional common‑name columns.
    Only the *preferred English* common name is stored per taxon.
    """
    __tablename__ = "expanded_taxa_cmn"

    taxonID       = Column(Integer, primary_key=True, nullable=False)
    rankLevel     = Column(Float, index=True)
    rank          = Column(String(255))
    name          = Column(String(255), index=True)
    commonName    = Column(String(255))               # NEW
    taxonActive   = Column(Boolean, index=True)

    # Ancestral columns – dots converted to underscores to match SQL
    L5_taxonID    = Column(Integer)
    L5_name       = Column(Text)
    L5_commonName = Column(String(255))
    L10_taxonID    = Column(Integer)
    L10_name       = Column(Text)
    L10_commonName = Column(String(255))
    L11_taxonID    = Column(Integer)
    L11_name       = Column(Text)
    L11_commonName = Column(String(255))
    L12_taxonID    = Column(Integer)
    L12_name       = Column(Text)
    L12_commonName = Column(String(255))
    L13_taxonID    = Column(Integer)
    L13_name       = Column(Text)
    L13_commonName = Column(String(255))
    L15_taxonID    = Column(Integer)
    L15_name       = Column(Text)
    L15_commonName = Column(String(255))
    L20_taxonID    = Column(Integer)
    L20_name       = Column(Text)
    L20_commonName = Column(String(255))
    L24_taxonID    = Column(Integer)
    L24_name       = Column(Text)
    L24_commonName = Column(String(255))
    L25_taxonID    = Column(Integer)
    L25_name       = Column(Text)
    L25_commonName = Column(String(255))
    L26_taxonID    = Column(Integer)
    L26_name       = Column(Text)
    L26_commonName = Column(String(255))
    L27_taxonID    = Column(Integer)
    L27_name       = Column(Text)
    L27_commonName = Column(String(255))
    L30_taxonID    = Column(Integer)
    L30_name       = Column(Text)
    L30_commonName = Column(String(255))
    L32_taxonID    = Column(Integer)
    L32_name       = Column(Text)
    L32_commonName = Column(String(255))
    L33_taxonID    = Column(Integer)
    L33_name       = Column(Text)
    L33_commonName = Column(String(255))
    L33_5_taxonID    = Column(Integer)
    L33_5_name       = Column(Text)
    L33_5_commonName = Column(String(255))
    L34_taxonID    = Column(Integer)
    L34_name       = Column(Text)
    L34_commonName = Column(String(255))
    L34_5_taxonID    = Column(Integer)
    L34_5_name       = Column(Text)
    L34_5_commonName = Column(String(255))
    L35_taxonID    = Column(Integer)
    L35_name       = Column(Text)
    L35_commonName = Column(String(255))
    L37_taxonID    = Column(Integer)
    L37_name       = Column(Text)
    L37_commonName = Column(String(255))
    L40_taxonID    = Column(Integer)
    L40_name       = Column(Text)
    L40_commonName = Column(String(255))
    L43_taxonID    = Column(Integer)
    L43_name       = Column(Text)
    L43_commonName = Column(String(255))
    L44_taxonID    = Column(Integer)
    L44_name       = Column(Text)
    L44_commonName = Column(String(255))
    L45_taxonID    = Column(Integer)
    L45_name       = Column(Text)
    L45_commonName = Column(String(255))
    L47_taxonID    = Column(Integer)
    L47_name       = Column(Text)
    L47_commonName = Column(String(255))
    L50_taxonID    = Column(Integer)
    L50_name       = Column(Text)
    L50_commonName = Column(String(255))
    L53_taxonID    = Column(Integer)
    L53_name       = Column(Text)
    L53_commonName = Column(String(255))
    L57_taxonID    = Column(Integer)
    L57_name       = Column(Text)
    L57_commonName = Column(String(255))
    L60_taxonID    = Column(Integer)
    L60_name       = Column(Text)
    L60_commonName = Column(String(255))
    L67_taxonID    = Column(Integer)
    L67_name       = Column(Text)
    L67_commonName = Column(String(255))
    L70_taxonID    = Column(Integer)
    L70_name       = Column(Text)
    L70_commonName = Column(String(255))

# Helpful composite index for frequent ancestor look‑ups
Index("idx_expanded_taxa_cmn_L10_taxonID", "L10_taxonID")
          </file>
        </dir>
        <dir path="xa/a/tools">
          <file path="taxa/tools/extract_ColDP_samples.sh">
#!/usr/bin/env bash
set -euo pipefail

INPUT_DIR="/datasets/taxa/catalogue_of_life/2024/ColDP"
OUTPUT_FILE="/home/caleb/repo/ibridaDB/dbTools/taxa/ColDP_raw_samples.txt"

# Start with an empty output file
> "$OUTPUT_FILE"

for filepath in "$INPUT_DIR"/*; do
  # Only process regular files that are not .png
  if [[ -f "$filepath" && "${filepath##*.}" != "png" ]]; then
    filename=$(basename "$filepath")
    {
      echo "<$filename>"
      echo "<head -n 10>"
      head -n 10 "$filepath"
      echo "</$filename>"
      echo "</head -n 10>"
      echo  # blank line between entries
    } >> "$OUTPUT_FILE"
  fi
done
          </file>
        </dir>
      </dir>
    </dir>
    <external_files>
      <file path="/home/caleb/repo/ibridaDB/dev/CoL/ColDP_overview.md">
---- ASSISTANT ----
In brief: Catalogue of Life (COL) publishes its yearly and monthly check‑lists in four main interchange formats. For your specific goal—bringing English (and optionally multi‑language) common names into expanded_taxa—the Catalogue of Life Data Package (ColDP) is the most complete and least lossy choice: it ships a dedicated VernacularName table keyed by taxonID, exactly what you need. The alternative Darwin Core Archive (DwC‑A) also contains common names, but they are carried in an optional vernacularname.txt extension that omits several useful fields ColDP provides. TextTree is meant for human‑readable diffs and does not contain vernacular data, while ACEF is a deprecated legacy format. Below you will find (i) a concise description of each dump type and where the common names live, (ii) an integration spec and schema sketch for your expanded_taxa‑derived copy, and (iii) notes on other COL tables you may want to mine later (distribution, media, references).

⸻

1 · Catalogue of Life dump formats

Format	File supplied by COL	Purpose & structure	Where common names live	Pros / cons
ColDP (Catalogue of Life Data Package)	2024_coldp.zip	ZIP bundle of tab‑separated files + metadata.yaml; follows ColDP v1.1 specification. Tables include Name.txt, Taxon.txt, Synonym.txt, Reference.txt, Distribution.txt, VernacularName.txt, etc. ￼ ￼	Dedicated VernacularName entity keyed by taxonID, with fields name, language, preferred, country, area, referenceID, etc. ￼	Richest, loss‑less, explicit foreign keys; excellent for SQL import.
DwC‑A (Darwin Core Archive)	2023_dwca.zip	ZIP archive with meta.xml, a core taxon.txt and optional extensions; follows TDWG Darwin Core Text Guidelines. ￼ ￼	Optional vernacularname.txt extension (same rowType as GBIF vernacular extension) links back to taxonID. ￼ ￼	Widely supported tooling (R, Python python-dwca-reader, GBIF IPT); fewer vernacular metadata fields than ColDP.
TextTree	(not yet downloaded)	Single UTF‑8 text file with indented hierarchy lines; synonyms prefixed with *; rank in <angle‑brackets>. ￼ ￼	No separate vernacular section—only scientific names and synonymy.	Human‑readable and small; unsuitable for programmatic common‑name joins.
ACEF (Annual Checklist Exchange Format)	older releases only	Set of CSV tables (Accepted Species, Synonyms, etc.) defined in 2014 spec. ￼ ￼	Columns EnglishName, PreferredName, but spec is frozen and lacks language codes.	Legacy; use only for compatibility with older pipelines.

Other niche exports COL offers (Newick, Graphviz DOT, Excel) are tree‑visualisation conveniences and do not add vernacular data. ￼

⸻

2 · Integration strategy for expanded_taxa

2.1 Data‑loading pipeline
	1.	Stage ColDP tables

unzip 2024_coldp.zip -d /tmp/coldp_2024

Load with COPY (PostgreSQL) or \copy from psql, e.g.

CREATE TABLE coldp_vernacular (
  taxonID INTEGER,
  name TEXT,
  transliteration TEXT,
  language CHAR(3),
  preferred BOOLEAN,
  country CHAR(2),
  area TEXT,
  sex TEXT,
  referenceID TEXT,
  remarks TEXT
);
\copy coldp_vernacular FROM '/tmp/coldp_2024/VernacularName.txt' (FORMAT csv, DELIMITER E'\t', NULL '');

Do the same for Taxon.txt if you need to cross‑check rank or status.

	2.	Create a protected copy of expanded_taxa

CREATE TABLE expanded_taxa_cmns AS TABLE expanded_taxa;
ALTER TABLE expanded_taxa_cmns
  ADD COLUMN commonName TEXT,
  -- expanded ancestry columns already exist; add matching commonName cols
  ADD COLUMN L10_commonName TEXT,
  ... ;  -- repeat for each expanded level you care about


	3.	Populate commonName (core taxon itself)

UPDATE expanded_taxa_cmns t
SET   commonName = v.name
FROM  (
  SELECT DISTINCT ON (taxonID)
         taxonID, name
  FROM   coldp_vernacular
  WHERE  language = 'eng' -- or your preferred ISO‑639‑3 code
  ORDER  BY taxonID, preferred DESC NULLS LAST
) v
WHERE t.taxonID = v.taxonID;


	4.	Populate ancestral common‑name columns
You already have L10_taxonID, L20_taxonID, … ; loop over them:

DO $$
DECLARE lvl TEXT;
BEGIN
  FOREACH lvl IN ARRAY ARRAY['L10','L20','L30','L40','L50','L70'] LOOP
    EXECUTE format($f$
      UPDATE expanded_taxa_cmns t
      SET    %I_commonName = v.name
      FROM   (
        SELECT DISTINCT ON (taxonID)
               taxonID, name
        FROM   coldp_vernacular
        WHERE  language = 'eng'
        ORDER  BY taxonID, preferred DESC NULLS LAST
      ) v
      WHERE  t.%I_taxonID = v.taxonID;
    $f$, lvl, lvl);
  END LOOP;
END$$;


	5.	Handle taxa with >1 common name
Option A (preferred) Keep everything normalized:

CREATE TABLE vernacular_names (
  taxonID INTEGER NOT NULL,
  name TEXT,
  language CHAR(3),
  preferred BOOLEAN,
  country CHAR(2),
  area TEXT
);
INSERT INTO vernacular_names
  SELECT taxonID, name, language, preferred, country, area
  FROM   coldp_vernacular;

Then store only one “display name” in expanded_taxa_cmns (rule of thumb: preferred = TRUE AND language='eng').
Option B Store an array in each _commonName column (TEXT[]) and aggregate with string_agg. PostgreSQL array or JSONB both work, but you will incur extra storage and your pipeline code must be array‑aware.

2.2 Interface considerations
	•	Joining key – COL taxonID is a stable integer that does not collide with iNaturalist’s numeric IDs, so if you want to align COL names with your iNat‑derived taxa you must map IDs (e.g. via scientific-name + authorship). If you have already ingested COL as a reference authority, mapping is trivial.
	•	Updates – COL releases monthly; store source_release in your staging table so refreshing does not silently change common names.
	•	Language coverage – ColDP provides ISO‑639‑3 codes; you can easily extend UI locale support later.

⸻

3 · Why ColDP is preferable for common names
	•	Rich vernacular metadata (preferred, country, area, transliteration) not present in DwC‑A. ￼
	•	Direct 1:1 foreign key on taxonID; no need to parse meta.xml.
	•	Table list is stable and versioned under semantic‑version tags (current stable 1.1, Sept 2024). ￼
	•	The same ZIP also gives you Distribution, Media, Reference, TypeSpecimen, which you may leverage later for habitat filtering or specimen thumbnails. ￼

A DwC‑A workflow remains useful if you already parse Darwin Core in other pipelines, but expect to down‑cast metadata. TextTree is purely visual; ACEF is obsolete. ￼ ￼ ￼

⸻

4 · Other COL tables worth considering (future work)

Table	Potential use
Distribution.txt	Populate region‑aware filters in your export scripts (e.g. in‑region vs out‑of‑region species).
Media.txt	Provide links to authoritative images for UI hover‑cards.
Reference.txt	Attach citation metadata to taxa for client display.
TypeSpecimen.txt	Could inform QA pipelines for model predictions.

All of these are already present in 2024_coldp.zip. ￼

⸻

5 · Next suggested steps
	1.	Download the missing TextTree only if you want a quick visual diff tool; it is not needed for vernacular data.
	2.	Implement the SQL snippets above inside your ingestion wrapper (or a dedicated Alembic migration) and test on a staging DB.
	3.	Once the expanded_taxa_cmns view is validated by downstream scripts, you can deprecate the older table or keep both side‑by‑side for backward compatibility.
	4.	If you wish to internationalise, adjust the sub‑query filter language='eng' and repeat for other codes (spa, fra, etc.), writing them into separate columns or a JSONB of {lang: name}.

Feel free to follow up when you are ready to formalise the filtering logic in your export side; I can help draft precise SQL functions or a Python ORM model for the vernacular‐name joins.

⸻

Key references
	•	ColDP specification & schema — GitHub repo  ￼
	•	COL yearly download page listing ColDP, DwC‑A, TextTree  ￼
	•	COL “Using the Catalogue of Life” (format overview, TextTree, ACEF)  ￼ ￼
	•	ColDP VernacularName field definitions  ￼
	•	TextTree template (file layout)  ￼
	•	ColDP vs other formats (BISS/TDWG paper)  ￼
	•	Darwin Core Archive how‑to (GBIF IPT)  ￼
	•	Example DwC‑A containing vernacularname.txt (VASCAN vignette)  ￼
	•	DwC Taxon schema (vernacularName term)  ￼
	•	ACEF 2014 table/field list (PDF)  ￼

These ten sources cover each format’s definition and vernacular‑name support.

---- END ASSISTANT ----


---- USER ----

okay, ColDP it is-- that's a great resource, even better than I realized! A few follow-ups:

1) can you expand on what this means?
```
Joining key – COL taxonID is a stable integer that does not collide with iNaturalist’s numeric IDs, so if you want to align COL names with your iNat‑derived taxa you must map IDs (e.g. via scientific-name + authorship). If you have already ingested COL as a reference authority, mapping is trivial.
```
Our iNat-derived taxa uses iNaturalist's numeric IDs, so do these not map perfectly with the COL taxonID? Not sure what you mean by collide here. I was assuming that the int keys would align, at least for active taxa. We can definitely instead map via scientific names (I don't have authorship data for iNaturalist taxa). 

But here you make it sound like we can just use taxonID:
```
3 · Why ColDP is preferable for common names
	•	Rich vernacular metadata (preferred, country, area, transliteration) not present in DwC‑A. 
	•	Direct 1:1 foreign key on taxonID; no need to parse meta.xml.
	•	Table list is stable and versioned under semantic‑version tags (current stable 1.1, Sept 2024). 
	•	The same ZIP also gives you Distribution, Media, Reference, TypeSpecimen, which you may leverage later for habitat filtering or specimen thumbnails. 
```
So I think maybe you're saying that ColDP provides taxonID, which is the same taxonID ints as our iNat-derived taxonID int keys?

2) for `Handle taxa with >1 common name`, let's take option 1. we will only store one “display name” in expanded_taxa_cmns, preferred = TRUE AND language='eng' seems like the perfect rule of thumb.

3) while we only need the single preferred common names for the core/ancestral commonName columns in the protected expanded_taxa table, I want to go ahead and bring the other suggested COL tables into new ibridaDB tables:
```
Table
Potential use
Distribution.txt
Populate region‑aware filters in your export scripts (e.g. in‑region vs out‑of‑region species).
Media.txt
Provide links to authoritative images for UI hover‑cards.
Reference.txt
Attach citation metadata to taxa for client display.
TypeSpecimen.txt
Could inform QA pipelines for model predictions.
```
it will be easier to ingest, document, and define python ORMs for these now and have them available for our later frontend work: I think we will mostly likely want, at least, the authoritative images for hover cards (as you suggest) and citation metadata for client display.. possibly type specimen will also be useful, although I will have to look into what that table actually provides to understand how we might use that. 

can you provide python ORMs for the new tables? we don't need to generate ingest scripts quite yet, I will hand off that work to another LLM with full context about my filesystem and e.g. DB connection strings. But I would like you to provide full ORMs for each of the tables. You don't need to provide an expanded_taxa ORM as I already have one, although it is a bit out-of-date and so you will need to review for correctness. Previously we used the TaxaExpanded SQLAlchemy class to generate the expanded taxa table, however we now use the following shell script:
```dbTools/taxa/expand/expand_taxa.sh
#!/usr/bin/env bash
# -----------------------------------------------------------------------------
# expand_taxa.sh
# -----------------------------------------------------------------------------
# Creates a new "expanded_taxa" table by expanding ancestry from the existing
# "taxa" table into structured columns ("L{level}_taxonID", "L{level}_name", etc.).
#
# This version uses *string concatenation* with quote_ident(...) and quote_nullable(...),
# bypassing placeholders entirely. This is the "sure" approach: no risk of
# placeholders vanishing, since we embed the actual values directly into the
# final SQL string.
#
# Steps:
#   1) Drop 'expanded_taxa' if exists; create base columns with quotes.
#   2) Add columns for each rank level ("L5_taxonID", "L5_name", etc.).
#   3) Create expand_taxa_procedure() as a function:
#      - We skip rank levels not in RANK_LEVELS (no 100).
#      - If debugging is enabled (DEBUG_EXPAND_TAXA=true), we RAISE NOTICE
#        about the row's data and the final SQL statement.
#      - We *string-concatenate* the column references, so no placeholders are used.
#   4) SELECT expand_taxa_procedure() to populate the table.
#   5) Create indexes on "L10_taxonID"... "L70_taxonID", plus "taxonID", "rankLevel", "name".
#   6) VACUUM (ANALYZE), notifications, done.
#
# Usage:
#   DEBUG_EXPAND_TAXA=true ./expand_taxa.sh
# -----------------------------------------------------------------------------

# ===[ 1) Setup & Logging ]====================================================
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Log messages (with timestamps) to both console and file
log_message() {
    local timestamp
    timestamp="$(date +'%Y-%m-%dT%H:%M:%S%z')"
    echo "[$timestamp] $1" | tee -a "${LOG_FILE}"
}

# Redirect stdout/stderr to console+log
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

# Source your common functions (for execute_sql, send_notification, etc.)
source "/home/caleb/repo/ibridaDB/dbTools/export/v0/common/functions.sh"

# Environment / defaults
DB_CONTAINER="${DB_CONTAINER:-ibridaDB}"
DB_NAME="${DB_NAME:-ibrida-v0-r1}"
DB_USER="${DB_USER:-postgres}"

# If DEBUG_EXPAND_TAXA=true, we pass a GUC variable into Postgres to enable debug
DEBUG_EXPAND="${DEBUG_EXPAND_TAXA:-false}"  # "true" or "false"

# rank levels to expand (no 100). We'll handle 5, 10, 11, ... 70
RANK_LEVELS=(5 10 11 12 13 15 20 24 25 26 27 30 32 33 33.5 34 34.5 35 37 40 43 44 45 47 50 53 57 60 67 70)

# We'll create indexes only on L10..L70
INDEX_LEVELS=(10 20 30 40 50 60 70)

log_message "Beginning expand_taxa.sh for DB: ${DB_NAME} (DEBUG_EXPAND_TAXA=${DEBUG_EXPAND})"

# ===[ 2) Create expanded_taxa schema ]========================================
log_message "Step 1: Dropping old expanded_taxa and creating base columns with quotes."

execute_sql "
DROP TABLE IF EXISTS \"expanded_taxa\" CASCADE;
CREATE TABLE \"expanded_taxa\" (
    \"taxonID\"       INTEGER PRIMARY KEY,
    \"rankLevel\"     DOUBLE PRECISION,
    \"rank\"          VARCHAR(255),
    \"name\"          VARCHAR(255),
    \"taxonActive\"   BOOLEAN
    -- We'll add \"LXX_taxonID\", \"LXX_name\", \"LXX_commonName\" columns next
);
"

# ===[ 3) Add columns for each rank level ]====================================
log_message "Step 2: Adding L{level}_taxonID, L{level}_name, L{level}_commonName columns."

ADD_COLS=""
for L in "${RANK_LEVELS[@]}"; do
  SAFE_L=$(echo "${L}" | sed 's/\./_/g')
  ADD_COLS+=" ADD COLUMN \"L${SAFE_L}_taxonID\" INTEGER,
             ADD COLUMN \"L${SAFE_L}_name\" VARCHAR(255),
             ADD COLUMN \"L${SAFE_L}_commonName\" VARCHAR(255),"
done

# Remove trailing comma
ADD_COLS="${ADD_COLS%,}"

execute_sql "
ALTER TABLE \"expanded_taxa\"
${ADD_COLS};
"

# ===[ 4) Create expand_taxa_procedure() function ]============================
log_message "Step 3: Creating expand_taxa_procedure() with string-concatenation for dynamic columns."

# We'll incorporate a GUC "myapp.debug_expand" to signal debug mode in PL/pgSQL
if [ "${DEBUG_EXPAND}" = "true" ]; then
  execute_sql "SET myapp.debug_expand = 'on';"
else
  execute_sql "SET myapp.debug_expand = 'off';"
fi

execute_sql "
DROP FUNCTION IF EXISTS expand_taxa_procedure() CASCADE;

CREATE OR REPLACE FUNCTION expand_taxa_procedure()
RETURNS void
LANGUAGE plpgsql
AS \$\$
DECLARE
    t_rec RECORD;
    ancestor_ids TEXT[];
    this_ancestor TEXT;
    anc_data RECORD;
    effective_level TEXT;
    row_sql TEXT;
    debugging boolean := false;
BEGIN
    -- We'll read our GUC to see if debug is on
    BEGIN
        IF current_setting('myapp.debug_expand') = 'on' THEN
            debugging := true;
        END IF;
    EXCEPTION
        WHEN others THEN
            debugging := false;  -- if the GUC is not set, do nothing
    END;

    -- Only retrieve active taxa rows, ignoring inactive ones
    FOR t_rec IN
        SELECT taxon_id, ancestry, rank_level, rank, name, active
        FROM taxa
        WHERE active = true
    LOOP
        -- Insert base row
        INSERT INTO \"expanded_taxa\"(\"taxonID\", \"rankLevel\", \"rank\", \"name\", \"taxonActive\")
        VALUES (t_rec.taxon_id, t_rec.rank_level, t_rec.rank, t_rec.name, t_rec.active);

        IF t_rec.ancestry IS NOT NULL AND t_rec.ancestry <> '' THEN
            ancestor_ids := string_to_array(t_rec.ancestry, '/');
        ELSE
            ancestor_ids := ARRAY[]::TEXT[];
        END IF;

        -- Include self
        ancestor_ids := ancestor_ids || t_rec.taxon_id::TEXT;

        FOREACH this_ancestor IN ARRAY ancestor_ids
        LOOP
            BEGIN
                IF this_ancestor IS NULL THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping NULL ancestor for row taxon_id=%', t_rec.taxon_id;
                    END IF;
                    CONTINUE;
                END IF;

                SELECT rank_level, rank, name
                  INTO anc_data
                  FROM taxa
                 WHERE taxon_id = this_ancestor::INTEGER
                 LIMIT 1;

                IF NOT FOUND OR anc_data.name IS NULL THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping ancestor=% for row taxon_id=%: not found or name is NULL', this_ancestor, t_rec.taxon_id;
                    END IF;
                    CONTINUE;
                END IF;

                IF anc_data.rank_level NOT IN (
                    5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30,
                    32, 33, 33.5, 34, 34.5, 35, 37, 40, 43, 44, 45,
                    47, 50, 53, 57, 60, 67, 70
                ) THEN
                    IF debugging THEN
                        RAISE NOTICE 'Skipping rank_level=% for row taxon_id=% (ancestor=%)', anc_data.rank_level, t_rec.taxon_id, this_ancestor;
                    END IF;
                    CONTINUE;
                END IF;

                effective_level := replace(CAST(anc_data.rank_level AS TEXT), '.', '_');

                -- Build dynamic SQL via string concat + quote_ident(...) + quote_nullable(...)
                row_sql :=
                    'UPDATE \"expanded_taxa\" SET '
                    || quote_ident('L' || effective_level || '_taxonID') || ' = '
                        || quote_nullable(this_ancestor)
                    || ', '
                    || quote_ident('L' || effective_level || '_name') || ' = '
                        || quote_nullable(anc_data.name)
                    || ' WHERE \"taxonID\" = ' || quote_nullable(t_rec.taxon_id::text);

                IF debugging THEN
                    RAISE NOTICE 'Row taxon_id=% => rank_level=% => built SQL: %',
                                 t_rec.taxon_id, anc_data.rank_level, row_sql;
                END IF;

                EXECUTE row_sql;

            EXCEPTION WHEN OTHERS THEN
                RAISE NOTICE 'Error updating row => base taxon_id=%, ancestor=%, anc_data=(%,%,%), row_sql=[%]',
                  t_rec.taxon_id, this_ancestor, anc_data.rank_level, anc_data.rank, anc_data.name, row_sql;
                RAISE;
            END;
        END LOOP;
    END LOOP;
END;
\$\$;
"

# ===[ 5) Populate expanded_taxa ]=============================================
log_message "Step 4: SELECT expand_taxa_procedure() to populate."

execute_sql "
SELECT expand_taxa_procedure();
"

log_message "Population of expanded_taxa complete. Running: \\d \"expanded_taxa\""
execute_sql "\d \"expanded_taxa\""

send_notification "expand_taxa.sh: Step 4 complete (expanded_taxa populated)."

# ===[ 6) Create indexes (only on L10..L70) ]===================================
log_message "Step 5: Creating indexes on L10_taxonID, L20_taxonID, ..., L70_taxonID plus base columns."

for L in "${INDEX_LEVELS[@]}"; do
  SAFE_L=$(echo "${L}" | sed 's/\./_/g')
  execute_sql "
  CREATE INDEX IF NOT EXISTS idx_expanded_taxa_L${SAFE_L}_taxonID
    ON \"expanded_taxa\"(\"L${SAFE_L}_taxonID\");
  "
done

execute_sql "
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_taxonID    ON \"expanded_taxa\"(\"taxonID\");
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_rankLevel  ON \"expanded_taxa\"(\"rankLevel\");
CREATE INDEX IF NOT EXISTS idx_expanded_taxa_name       ON \"expanded_taxa\"(\"name\");
"

log_message "Index creation done. Running: \\d \"expanded_taxa\""
execute_sql "\d \"expanded_taxa\""

send_notification "expand_taxa.sh: Step 5 complete (indexes created)."

# ===[ 7) VACUUM ANALYZE ]====================================================
log_message "Step 6: VACUUM ANALYZE \"expanded_taxa\" (final step)."

execute_sql "
VACUUM (ANALYZE) \"expanded_taxa\";
"

send_notification "expand_taxa.sh: Step 6 complete (VACUUM ANALYZE done)."
log_message "expand_taxa.sh complete. Exiting."
```
Here's the corresponding sqlalchemy model for this table-- again, you need to double-check this for correctness. I know we the name is incorrect (below names the table taxa_expanded, the actual name is not expanded_taxa, the protected copy will be expanded_taxa_cmn):
```dbTools/taxa/models/expanded_taxa.py
from sqlalchemy import Boolean, Column, Integer, String, Text
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()


class TaxaExpanded(Base):
    __tablename__ = "taxa_expanded"

    taxon_id = Column(Integer, primary_key=True, nullable=False)
    name = Column(String(64), index=True)
    common_name = Column(Text)
    rank = Column(Text)
    rank_level = Column(Integer, index=True)
    taxon_active = Column(Boolean, index=True)
    L5_taxon_id = Column(Integer)
    L5_name = Column(Text)  # Ancestral string columns should be text
    L5_common_name = Column(Text)  # In MySQL, this just stores a pointer to the string
    L10_taxon_id = Column(
        Integer
    )  # This allows us to fit within the row width byte limit imposed by InnoDB
    L10_name = Column(Text)
    L10_common_name = Column(Text)
    L11_taxon_id = Column(Integer)
    L11_name = Column(Text)
    L11_common_name = Column(Text)
    L12_taxon_id = Column(Integer)
    L12_name = Column(Text)
    L12_common_name = Column(Text)
    L13_taxon_id = Column(Integer)
    L13_name = Column(Text)
    L13_common_name = Column(Text)
    L15_taxon_id = Column(Integer)
    L15_name = Column(Text)
    L15_common_name = Column(Text)
    L20_taxon_id = Column(Integer)
    L20_name = Column(Text)
    L20_common_name = Column(Text)
    L24_taxon_id = Column(Integer)
    L24_name = Column(Text)
    L24_common_name = Column(Text)
    L25_taxon_id = Column(Integer)
    L25_name = Column(Text)
    L25_common_name = Column(Text)
    L26_taxon_id = Column(Integer)
    L26_name = Column(Text)
    L26_common_name = Column(Text)
    L27_taxon_id = Column(Integer)
    L27_name = Column(Text)
    L27_common_name = Column(Text)
    L30_taxon_id = Column(Integer)
    L30_name = Column(Text)
    L30_common_name = Column(Text)
    L32_taxon_id = Column(Integer)
    L32_name = Column(Text)
    L32_common_name = Column(Text)
    L33_taxon_id = Column(Integer)
    L33_name = Column(Text)
    L33_common_name = Column(Text)
    L33_5_taxon_id = Column(Integer)
    L33_5_name = Column(Text)
    L33_5_common_name = Column(Text)
    L34_taxon_id = Column(Integer)
    L34_name = Column(Text)
    L34_common_name = Column(Text)
    L34_5_taxon_id = Column(Integer)
    L34_5_name = Column(Text)
    L34_5_common_name = Column(Text)
    L35_taxon_id = Column(Integer)
    L35_name = Column(Text)
    L35_common_name = Column(Text)
    L37_taxon_id = Column(Integer)
    L37_name = Column(Text)
    L37_common_name = Column(Text)
    L40_taxon_id = Column(Integer)
    L40_name = Column(Text)
    L40_common_name = Column(Text)
    L43_taxon_id = Column(Integer)
    L43_name = Column(Text)
    L43_common_name = Column(Text)
    L44_taxon_id = Column(Integer)
    L44_name = Column(Text)
    L44_common_name = Column(Text)
    L45_taxon_id = Column(Integer)
    L45_name = Column(Text)
    L45_common_name = Column(Text)
    L47_taxon_id = Column(Integer)
    L47_name = Column(Text)
    L47_common_name = Column(Text)
    L50_taxon_id = Column(Integer)
    L50_name = Column(Text)
    L50_common_name = Column(Text)
    L53_taxon_id = Column(Integer)
    L53_name = Column(Text)
    L53_common_name = Column(Text)
    L57_taxon_id = Column(Integer)
    L57_name = Column(Text)
    L57_common_name = Column(Text)
    L60_taxon_id = Column(Integer)
    L60_name = Column(Text)
    L60_common_name = Column(Text)
    L67_taxon_id = Column(Integer)
    L67_name = Column(Text)
    L67_common_name = Column(Text)
    L70_taxon_id = Column(Integer)
    L70_name = Column(Text)
    L70_common_name = Column(Text)
    L100_taxon_id = Column(Integer)
    L100_name = Column(Text)
    L100_common_name = Column(Text)
```
The above model contains some references to mysql compatibility.. that's from an old project, we are standardizing on psql/postgis for ibridaDB (nothing else). To prepare the above sqlalchemy model for the new `expanded_taxa_cmn` table, do we just need to change the name? any missing columns? any incorrect types? Use the shell script as the reference here; that's what we actually used to generate the expanded_taxa table on our database. I don't think this ORM needs to directly reference a new VernacularNames ORM, does it?


I will also show you the below notes from my repo-- we identified several rank levels that have ambiguous names (>1 possible name for the rank), I'm pretty sure that only L5, L10, L20 have these:
```
# Taxon ranks
## code_to_name
*maps taxon rank polli-style code to rank names*
code_to_name = {
    'L5': 'subspecies',
    'L10': 'species',
    'L11': 'complex',
    'L12': 'subsection', 
    'L13': 'section',
    'L15': 'subgenus',
    'L20': 'genus',
    'L24': 'subtribe',
    'L25': 'tribe',
    'L26': 'supertribe',
    'L27': 'subfamily',
    'L30': 'family',
    'L32': 'epifamily',
    'L33': 'superfamily',
    'L33_5': 'zoosubsection',
    'L34': 'zoosection',
    'L34_5': 'parvorder',
    'L35': 'infraorder',
    'L37': 'suborder',
    'L40': 'order',
    'L43': 'superorder',
    'L44': 'subterclass',
    'L45': 'infraclass',
    'L47': 'subclass',
    'L50': 'class',
    'L53': 'superclass',
    'L57': 'subphylum',
    'L60': 'phylum',
    'L67': 'subkingdom',
    'L70': 'kingdom'
}
### ambiguous ranks
We assume that the possibly ambiguous ranks are of the above ranks downstream. However, note that the following levels could be ambiguous:
*Possible ranks:*
- L5: form, infrahybrid, subspecies
- L10: hybrid, species
- L20: genus, genushybrid
```

Can you propose ColDP_VernacularNames, ColDP_Distribution, ColDP_Media, ColDP_Reference, and ColDP_TypeSpecimen ORMs? You can adjust those table names as you wish to be more proper, these are just suggestions.

Here are the files that we extracted from the ColDP zipfile:
```
caleb@blade:/datasets/taxa/catalogue_of_life/2024/ColDP$ ls
Distribution.tsv  Media.tsv      NameRelation.tsv  reference.json  source               SpeciesInteraction.tsv    TaxonProperty.tsv  VernacularName.tsv
logo.png          metadata.yaml  NameUsage.tsv     Reference.tsv   SpeciesEstimate.tsv  TaxonConceptRelation.tsv  TypeMaterial.tsv
caleb@blade:/datasets/taxa/catalogue_of_life/2024/ColDP$ ls source
1005.yaml  1032.yaml  1053.yaml  1080.yaml  1093.yaml  1107.yaml  1124.yaml  1138.yaml  1152.yaml  1168.yaml  1180.yaml  1193.yaml  1204.yaml    2141.yaml    2317.yaml
...
1031.yaml  1052.yaml  1078.yaml  1092.yaml  1106.yaml  1123.yaml  1134.yaml  1150.yaml  1167.yaml  1179.yaml  1192.yaml  1203.yaml  2130.yaml    2304.yaml
```
I will provide samples (head -n 10) from all of the extracted files (except for source/, I provide head -n 50 from only one of the yamls) below to help guide you:
```ColDP_raw_samples.txt
<Distribution.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:areaID	col:area	col:gazetteer	col:status	col:referenceID	col:remarks
BLL2J		CN		iso	native		
BLL2J		TW		iso	native		
3LWT2		CR		iso	native		
444RB			Uruguay (Rivera)	text	native		
69PYD			North America	text	native		
8VVZD		MXC		tdwg	native		
8VVZD		MXG		tdwg	native		
8VVZD		MXE		tdwg	native		
8VVZD		MXT		tdwg	native		
</Distribution.tsv>
</head -n 10>

<Media.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:url	col:type	col:format	col:title	col:created	col:creator	col:license	col:link	col:remarks
</Media.tsv>
</head -n 10>

<metadata.yaml>
<head -n 10>
---
key: 299029
doi: 10.48580/dg9ld
title: Catalogue of Life
alias: COL24
description: |-
  The Catalogue of Life is an assembly of expert-based global species checklists with the aim to build a comprehensive catalogue of all known species of organisms on Earth. Continuous progress is made towards completion, but for now, it probably includes just over 80% of the world's known species. The Catalogue of Life estimates 2.3M extant species on the planet recognised by taxonomists at present time. This means that for many groups it continues to be deficient, and users may notice that many species are still missing from the Catalogue.

  ### What's new in 2024 Annual Checklist?

</metadata.yaml>
</head -n 10>

<NameRelation.tsv>
<head -n 10>
col:nameID	col:relatedNameID	col:sourceID	col:type	col:referenceID	col:page	col:remarks
CX5Y	6ZBQ2		basionym			
4DQ2L	6ZDLT		basionym			
4DQ2M	6ZDLY		basionym			
85QXH	BKZY		basionym			
4J2RF	6XKWB		basionym			
67CMV	CDYT5		basionym			
67CN7	4KGYM		basionym			
5VCMV	3L8K9		basionym			
67CMQ	6DV6D		basionym			
</NameRelation.tsv>
</head -n 10>

<NameUsage.tsv>
<head -n 10>
col:ID	col:alternativeID	col:nameAlternativeID	col:sourceID	col:parentID	col:basionymID	col:status	col:scientificName	col:authorship	col:rank	col:notho	col:originalSpelling	col:uninomial	col:genericName	col:infragenericEpithet	col:specificEpithet	col:infraspecificEpithet	col:cultivarEpithet	col:combinationAuthorship	col:combinationAuthorshipID	col:combinationExAuthorship	col:combinationExAuthorshipID	col:combinationAuthorshipYear	col:basionymAuthorship	col:basionymAuthorshipID	col:basionymExAuthorship	col:basionymExAuthorshipID	col:basionymAuthorshipYear	col:namePhrase	col:nameReferenceID	col:publishedInYear	col:publishedInPage	col:publishedInPageLink	col:gender	col:genderAgreement	col:etymology	col:code	col:nameStatus	col:accordingToID	col:accordingToPage	col:accordingToPageLink	col:referenceID	col:scrutinizer	col:scrutinizerID	col:scrutinizerDate	col:extinct	col:temporalRangeStart	col:temporalRangeEnd	col:environment	col:species	col:section	col:subgenus	col:genus	col:subtribe	col:tribe	col:subfamily	col:family	col:superfamily	col:suborder	col:order	col:subclass	col:class	col:subphylum	col:phylum	col:kingdom	col:ordinal	col:branchLength	col:link	col:nameRemarks	col:remarks
673FW			2232	3CP83	3CP83	synonym	Anisophyllum hyssopifolium	(L.) Haw.	species				Anisophyllum		hyssopifolium			Haw.					L.						0a8270ab-617f-402d-b3bf-c3f0fe6b0a0a							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:1042669-2		
9L2J9			2232	3F5B8	3F5B8	synonym	Asperula aparine var. aparine		variety				Asperula		aparine	aparine													308d3079-d3d3-4523-af56-80e4507eade8							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:77168438-1		
8RX53			1130	9WRMR		synonym	Saccoia escoffierae var. triangulata	Sacco, 1895	variety				Saccoia		escoffierae	triangulata		Sacco				1895							75d294da-5cc2-4241-b178-c5686ae93d97		42, pl. 1, fig. 116					zoological					28419c41-a4be-4567-9131-512c920576bd																										https://www.molluscabase.org/aphia.php?p=taxdetails&id=1545847		
555CQ			1141	C9FM		synonym	Telanthera flavescens	(Kunth) Moq.	species				Telanthera		flavescens			Moq.					Kunth						c59b73dc-e818-4ec0-8131-4f317c9139c6							botanical																																	
7TDYP			1130	3PP2B	9BD4Q	synonym	Triphora sarissa	Dall, 1889	species				Triphora		sarissa			Dall				1889														zoological																															https://www.molluscabase.org/aphia.php?p=taxdetails&id=1481479		
C33N7			1175	858KS		accepted	Amalocythere fulgida	Guan, 1978	species				Amalocythere		fulgida			Guan				1978							8db3f7fe-e3df-49ec-978c-717af25cea87							zoological	established				8db3f7fe-e3df-49ec-978c-717af25cea87	Brandão, Simone Nunes		2016-06-30	true			marine																			https://www.marinespecies.org/ostracoda/aphia.php?p=taxdetails&id=773107		
85BKX			2144	84JVC		accepted	Coronalpheus	Wicksten, 1999	genus			Coronalpheus						Wicksten				1999														zoological	acceptable				bc596268-7fff-4a3c-868b-3c392a7be32b			2021-04-26	false																						https://www.itis.gov/servlet/SingleRpt/SingleRpt?search_topic=TSN&search_value=1147818		
B3XCS			1141	5ZXM8		ambiguous synonym	Convolvulus rupestris	Buch	species				Convolvulus		rupestris			Buch											5f264aa0-ccf7-4da8-ad97-0ef3f4913f7e		193					botanical																																	
3HNHY			2232	8VXZW		accepted	Guzmania variegata	L.B.Sm.	species				Guzmania		variegata			L.B.Sm.											6683ea9d-d407-472a-8daa-943b551b3eaf							botanical																															https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:115257-2		Ecuador to Peru
</NameUsage.tsv>
</head -n 10>

<reference.json>
<head -n 10>
[
{"id":"1382","author":[{"family":"Weiss, Norbert, ed."}],"issued":{"date-parts":[[2001]]},"title":"Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM","container-title":"International Journal of Systematic and Evolutionary Microbiology, vol. 51, no. 2"},
{"id":"1383","author":[{"family":"Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al."}],"issued":{"date-parts":[[1992]]},"title":"Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b","container-title":"Archives in Microbiology, vol. 157"},
{"id":"1384","author":[{"family":"Partensky, F., W. R. Hess, and D. Vaulot"}],"issued":{"date-parts":[[1999]]},"title":"Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance","container-title":"Microbiology and Molecular Biology Reviews, vol. 63, no. 1"},
{"id":"171","author":[{"family":"Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.)"}],"issued":{"date-parts":[[1986]]},"container-title":"Bergey's Manual of Systematic Bacteriology. vol. 2"},
{"id":"1331","author":[{"family":"Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds."}],"issued":{"date-parts":[[1980]]},"title":"Approved Lists of Bacterial Names","container-title":"International Journal of Systematic Bacteriology, vol. 30, no.1"},
{"id":"4731","author":[{"family":"Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds."}],"issued":{"date-parts":[[1989]]},"container-title":"Approved Lists of Bacterial Names"},
{"author":[{"family":"S"}],"issued":{"date-parts":[[2000,1,1]]},"title":"SN2000/Levine, 1982"},
{"author":[{"family":"S"}],"issued":{"date-parts":[[2000,1,1]]},"title":"SN2000/Perkins, 2000"},
{"title":"As per Gigaductus"},
</reference.json>
</head -n 10>

<Reference.tsv>
<head -n 10>
col:ID	col:alternativeID	col:sourceID	col:citation	col:type	col:author	col:editor	col:title	col:titleShort	col:containerAuthor	col:containerTitle	col:containerTitleShort	col:issued	col:accessed	col:collectionTitle	col:collectionEditor	col:volume	col:issue	col:edition	col:page	col:publisher	col:publisherPlace	col:version	col:isbn	col:issn	col:doi	col:link	col:remarks
12ddc4b3-d551-4c38-8d20-4a2246b9e8d6		2144	Weiss, Norbert, ed. (2001). Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM. International Journal of Systematic and Evolutionary Microbiology, Vol. 51, No. 2.		Weiss, Norbert, ed.		Validation List no. 79: Validation of publication of new names and new combinations previously effectively published outside of the IJSEM			International Journal of Systematic and Evolutionary Microbiology, vol. 51, no. 2		2001															
5eb14d90-e68f-4fda-be6c-453c6965707b		2144	Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al. (1992). Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b. Archives in Microbiology, Vol. 157.		Chisholm, S. W., S. L. Frankel, R. Goericke, R. J. Olsen, B. Palenik, J. B. Waterbury, et al.		Prochlorococcus marinus nov. gen. nov. sp.: an oxyphototrophic marine prokaryote containing divinyl chlorophyll a and b			Archives in Microbiology, vol. 157		1992															
6b833931-90b9-4dca-b98c-d3e300075c7e		2144	Partensky, F., W. R. Hess, and D. Vaulot. (1999). Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance. Microbiology and Molecular Biology Reviews, Vol. 63, No. 1.		Partensky, F., W. R. Hess, and D. Vaulot		Prochlorococcus, a Marine Photosynthetic Prokaryote of Global Significance			Microbiology and Molecular Biology Reviews, vol. 63, no. 1		1999															
c28f7f52-c223-4410-90ec-9142da3e34ab		2144	Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.). (1986). Bergey’s Manual of Systematic Bacteriology. Vol. 2.		Sneath, Peter H.A., Nicholas S. Mair, M. Elisabeth Sharpe, and John G. Holt (eds.)					Bergey's Manual of Systematic Bacteriology. vol. 2		1986															
fa90c9ed-a347-4ae8-9feb-d2dec6bd8f97		2144	Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds. (1980). Approved Lists of Bacterial Names. International Journal of Systematic Bacteriology, Vol. 30, no.1.		Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds.		Approved Lists of Bacterial Names			International Journal of Systematic Bacteriology, vol. 30, no.1		1980															
be12e305-5fb7-4184-b33d-b6601e9c4140		2144	Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds. (1989). Approved Lists of Bacterial Names.		Skerman, V. B. D., Vicki McGowan, and P. H. A. Sneath, eds.					Approved Lists of Bacterial Names		1989															
c75a73a2-0c85-4d63-a46a-c7f46d5f7913		2007	Seenivasan, R.; Sausen, N.; Medlin, L. K.; Melkonian, M. (2013). Picomonas judraskeda gen. et sp. nov.: the first identified member of the Picozoa phylum nov., a widespread group of picoeukaryotes, formerly known as 'picobiliphytes'. PLoS ONE. 8(3): e59565.																								
2ea6bf13-11f2-474f-b9ab-4e30333d3ade		2007	Article title: Phylogeny of novel naked filose and reticulose Cercozoa: Granofilosea cl. n. and Proteomyxidea revised.																								
535b1b3f-d59c-4da8-bc3b-abf664243b17		2007	Yabuki, A.; Chao, E. E.; Ishida, K.-I.; Cavalier-Smith, T. (2012). Microheliella maris (Microhelida ord. n.), an ultrastructurally highly distinctive new axopodial protist species and genus, and the unity of phylum Heliozoa. Protist. 163(3): 356-388.																								
</Reference.tsv>
</head -n 10>

<SpeciesEstimate.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:estimate	col:type	col:referenceID	col:remarks
BRKNQ		5385	species living	74	
622DH		317	species living	30	
4XK		85	species living	74	
FJ5		37	species living	30	
625XT		2	species living	55	
HJR		93	species living	30	
9LT		46	species living	30	
CZK		79	species living	55	
8NKJ8		71	species living	29	
</SpeciesEstimate.tsv>
</head -n 10>

<SpeciesInteraction.tsv>
<head -n 10>
col:taxonID	col:relatedTaxonID	col:sourceID	col:relatedTaxonScientificName	col:type	col:referenceID	col:remarks
</SpeciesInteraction.tsv>
</head -n 10>

<TaxonConceptRelation.tsv>
<head -n 10>
col:taxonID	col:relatedTaxonID	col:sourceID	col:type	col:referenceID	col:remarks
</TaxonConceptRelation.tsv>
</head -n 10>

<TaxonProperty.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:property	col:value	col:ordinal	col:referenceID	col:page	col:remarks
</TaxonProperty.tsv>
</head -n 10>

<TypeMaterial.tsv>
<head -n 10>
col:ID	col:nameID	col:sourceID	col:citation	col:status	col:referenceID	col:page	col:country	col:locality	col:latitude	col:longitude	col:altitude	col:sex	col:host	col:associatedSequences	col:date	col:collector	col:institutionCode	col:catalogNumber	col:link	col:remarks
</TypeMaterial.tsv>
</head -n 10>

<VernacularName.tsv>
<head -n 10>
col:taxonID	col:sourceID	col:name	col:transliteration	col:language	col:preferred	col:country	col:area	col:sex	col:referenceID	col:remarks
B6LM6		bacteria	bacteria	eng						
B6LM6		bacterias	bacterias	spa						
R5LB		Sandcarpet	Sandcarpet	eng						
76SYT		Alcajes	Alcajes							
76SYT		Rajamatraca	Rajamatraca							
333V8		Patana	Patana	cbq						
6C59B		Clavellina	Clavellina							
3XSNW		Pitayita	Pitayita							
3XSJY		Cochilinque	Cochilinque							
</VernacularName.tsv>
</head -n 10>

<source/1005.yaml>
<head -n 50>
---
key: 1005
doi: 10.48580/dfrdl-37p
title: Catalogue of Craneflies of the World
alias: CCW
description: "The Catalogue of the Craneflies of the World (CCW) covers all genera,\
  \ subgenera, species, subspecies, and synonyms of the World Tipuloidea (Insecta\
  \ – Diptera – Nematocera – families Pediciidae, Limoniidae, Cylindrotomidae, Tipulidae).\
  \ It also includes up to date information on the distribution of the species and\
  \ subspecies, specified by countries and, for the larger countries, states, provinces\
  \ and islands. The website’s list of references has over 7.400 titles, about 4.300\
  \ of which available as easy downloadable pdf."
issued: 2021-05-07
version: May 2021
contact:
  given: P
  family: Oosterbroek
creator:
 -
  given: P.
  family: Oosterbroek
contributor:
 -
  city: Leiden
  country: NL
  address: "Leiden, Netherlands"
  organisation: Naturalis Biodiversity Center
keyword: []
containerKey: 299029
containerTitle: Catalogue of Life
containerCreator:
 -
  orcid: 0000-0001-6197-9951
  given: Olaf
  family: Bánki
  city: Amsterdam
  country: NL
  note: COL Managing Director
  address: "Amsterdam, Netherlands"
  organisation: Catalogue of Life
 -
  orcid: 0000-0003-2137-2690
  given: Yury
  family: Roskov
  city: Champaign
  state: IL
  country: US
  note: COL Executive Editor
  address: "Champaign, IL, United States of America"
  organisation: Illinois Natural History Survey				
</source/1005.yaml>
</head -n 50>
```
I am noticing that the taxonIDs used in e.g. NameUsage.tsv aren't the same as our iNat-derived int taxonIDs, so unless you see an appropriate mapping in the samples above then I suppose we will need to join on scientific names?

---- END USER ---

---- ASSISTANT ----
Key points up‑front 
* Catalogue of Life (COL) assigns its own five‑character, alphanumeric taxonID values that are unique within COL but have no numerical relationship to iNaturalist’s consecutive integer taxon_ids. Therefore a 1‑to‑1 foreign‑key join is impossible—you must map the two vocabularies through scientific names (plus optional authorship or rank context).
* When you ingest ColDP, load the VernacularName table and pick preferred, English rows to fill the new commonName fields, then keep the entire set of vernacular rows in a separate, normalised table for possible multi‑language use. ￼
* Besides VernacularName, ColDP ships first‑class tables for Distribution, Media, Reference, and TypeMaterial. Their schemas map cleanly to Postgres types and are safe to mirror in ibridaDB now for future front‑end use. ￼

Below you will find

1. A detailed explanation of the identifier mismatch and a robust mapping recipe.
2. An updated SQLAlchemy ORM for your expanded_taxa_cmn table (removes the obsolete L100_* columns, corrects types, and points to the right table name).
3. Clean, Postgres‑oriented ORMs for VernacularName, Distribution, Media, Reference, and TypeMaterial (you can rename classes as you wish).

⸻

1 · Why COL taxonID ≠ iNaturalist taxon_id

1.1 Identifier provenance
	•	COL’s identifiers are internal “checklist keys” minted when each source checklist is merged; they are opaque strings such as 8VVZD or C33N7.
	•	iNaturalist uses monotonically increasing integers (e.g. Serpentes = 85553) that double as primary keys in their Rails database and in the public API.
	•	Even where iNat says a branch is “sourced to Catalogue of Life” the linkage is maintained only as taxon‑framework metadata, not by sharing primary keys.

1.2 Collision vs. mismatch

The two identifier spaces cannot collide numerically because they live in different namespaces, but they also never coincide: COL’s 8VVZD is not the same taxon as iNat 874562 even if both refer to Danaus plexippus.

1.3 Practical mapping recipe

Step	Action
1	Load ColDP NameUsage.tsv into a staging table (coldp_nameusage) keyed on scientificName, authorship, and rank.
2	Build an index on lower‑cased, whitespace‑normalised scientificName in both datasets (COL + iNat).
3	Exact‑string join on canonical binomial/trinomial plus rank; where multiple COL hits occur pick status='accepted'.
4	For residuals, fall back to fuzzy join using the Levenshtein distance of canonical names and compare higher‑rank ancestors to resolve homonyms.
5	Persist a cross‑walk table (inat_to_coldp) with columns inat_taxon_id, coldp_taxon_id, match_quality, matched_name, coldp_name.

This approach yields deterministic keys for foreign‑key joins while remaining update‑safe when COL publishes a new annual checklist.

⸻

2 · Revised ORM for expanded_taxa_cmn

from sqlalchemy import (
    Column, Integer, String, Text, Boolean, Float, Index
)
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class ExpandedTaxaCmn(Base):
    """
    Copy of expanded_taxa with additional common‑name columns.
    Only the *preferred English* common name is stored per taxon.
    """
    __tablename__ = "expanded_taxa_cmn"

    taxonID       = Column(Integer, primary_key=True, nullable=False)
    rankLevel     = Column(Float, index=True)
    rank          = Column(String(255))
    name          = Column(String(255), index=True)
    commonName    = Column(String(255))               # NEW
    taxonActive   = Column(Boolean, index=True)

    # Ancestral columns – dots converted to underscores to match SQL
{% for lvl in [5,10,11,12,13,15,20,24,25,26,27,30,32,33,'33_5',34,'34_5',35,37,40,43,44,45,47,50,53,57,60,67,70] %}
    L{{lvl}}_taxonID    = Column(Integer)
    L{{lvl}}_name       = Column(Text)
    L{{lvl}}_commonName = Column(String(255))
{% endfor %}

# Helpful composite index for frequent ancestor look‑ups
Index("idx_expanded_taxa_cmn_L10_taxonID", "L10_taxonID")

Changes vs. your legacy TaxaExpanded
	•	__tablename__ corrected.
	•	rankLevel now Float (Postgres double precision).
	•	Removed the entire L100_* trio to match the shell script.
	•	Base and ancestor common‑name columns use String(255)—same length as your name fields.

⸻

3 · New ColDP ORMs

from sqlalchemy import (
    Column, String, Text, Boolean, Date, Numeric, Integer
)
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class ColdpVernacularName(Base):
    __tablename__ = "coldp_vernacular_name"
    taxonID        = Column(String(10), primary_key=True)
    sourceID       = Column(String(10))
    name           = Column(Text, nullable=False)
    transliteration= Column(Text)
    language       = Column(String(3))      # ISO‑639‑3
    preferred      = Column(Boolean)
    country        = Column(String(2))      # ISO‑3166‑1‑alpha‑2
    area           = Column(Text)
    sex            = Column(String(20))
    referenceID    = Column(String(64))
    remarks        = Column(Text)

class ColdpDistribution(Base):
    __tablename__ = "coldp_distribution"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(10), index=True)
    sourceID       = Column(String(10))
    areaID         = Column(String(10))
    area           = Column(Text)
    gazetteer      = Column(String(10))
    status         = Column(String(25))     # e.g. native, introduced
    referenceID    = Column(String(64))
    remarks        = Column(Text)

class ColdpMedia(Base):
    __tablename__ = "coldp_media"
    id             = Column(Integer, primary_key=True, autoincrement=True)
    taxonID        = Column(String(10), index=True)
    sourceID       = Column(String(10))
    url            = Column(Text, nullable=False)
    type           = Column(String(50))     # stillImage, sound, video …
    format         = Column(String(50))     # MIME type or file suffix
    title          = Column(Text)
    created        = Column(Date)
    creator        = Column(Text)
    license        = Column(String(100))
    link           = Column(Text)           # landing page
    remarks        = Column(Text)

class ColdpReference(Base):
    __tablename__ = "coldp_reference"
    ID             = Column(String(64), primary_key=True)   # UUID or short key
    alternativeID  = Column(String(64))
    sourceID       = Column(String(10))
    citation       = Column(Text)
    type           = Column(String(30))
    author         = Column(Text)
    editor         = Column(Text)
    title          = Column(Text)
    titleShort     = Column(Text)
    containerAuthor= Column(Text)
    containerTitle = Column(Text)
    containerTitleShort = Column(Text)
    issued         = Column(String(50))
    accessed       = Column(String(50))
    collectionTitle= Column(Text)
    collectionEditor= Column(Text)
    volume         = Column(String(30))
    issue          = Column(String(30))
    edition        = Column(String(30))
    page           = Column(String(50))
    publisher      = Column(Text)
    publisherPlace = Column(Text)
    version        = Column(String(30))
    isbn           = Column(String(20))
    issn           = Column(String(20))
    doi            = Column(String(100))
    link           = Column(Text)
    remarks        = Column(Text)

class ColdpTypeMaterial(Base):
    """
    ColDP entity `TypeMaterial` (called TypeSpecimen in the user request).
    """
    __tablename__ = "coldp_type_material"
    ID              = Column(String(64), primary_key=True)
    nameID          = Column(String(10), index=True)
    sourceID        = Column(String(10))
    citation        = Column(Text)
    status          = Column(String(50))
    referenceID     = Column(String(64))
    page            = Column(String(50))
    country         = Column(String(2))
    locality        = Column(Text)
    latitude        = Column(Numeric(9,5))
    longitude       = Column(Numeric(9,5))
    altitude        = Column(String(50))
    sex             = Column(String(12))
    host            = Column(Text)
    associatedSequences = Column(Text)
    date            = Column(Date)
    collector       = Column(Text)
    institutionCode = Column(String(25))
    catalogNumber   = Column(String(50))
    link            = Column(Text)
    remarks         = Column(Text)

Implementation notes
	•	Primary keys—taxonID alone is not guaranteed unique in Media and Distribution, so synthetic integer PKs (id) are provided.
	•	Text vs. String—long free‑form columns use Text; controlled‑vocabulary or short identifiers use String.
	•	All Date fields assume ISO‑8601 strings in the TSV; feel free to cast to DateTime if time‑of‑day is ever present.
	•	If you load reference.json (CSL array), deserialize to the same coldp_reference table—store JSON in a side column if you want to preserve the CSL structure.

⸻

4 · Next steps
	1.	Back‑fill inat_to_coldp mapping using the recipe in §1.3—this unlocks direct joins between iNat observations and COL’s richer metadata.
	2.	Run a small pilot on, say, 100 species to verify that the preferred English commonName chosen by the SQL rule matches your expectations.
	3.	Wire the new ORM classes into your Alembic migration scripts and generate empty tables in a staging database so that the downstream LLM tasked with writing loaders has a concrete target schema.

Feel free to circle back when you want help formalising the export‑side filtering logic or adding additional ColDP entities (e.g. SpeciesInteraction) to the model layer.

⸻

References
	•	ColDP specification & schema diagram.
	•	ColDP GitHub README (entity list and field explanations).
	•	ColDP publishing guidelines (recommended columns for VernacularName, Media, TypeMaterial). ￼
	•	ColDP v1.1 release notes (TypeMaterial multi‑value rules). ￼
	•	“Using the Catalogue of Life” overview of ColDP vs DwC‑A.
	•	iNaturalist “Taxon Frameworks” documentation (external authorities and mapping).
	•	iNaturalist API reference & numeric taxon_id explanation.
	•	GBIF best‑practice note on Catalogue of Life as composite checklist.
---- END ASSISTANT ----

      </file>
      <file path="/home/caleb/repo/ibridaDB/dev/ancestor_aware.md">
# 1. Overview

We plan to introduce an **ancestor‐aware approach** into our **iNaturalist data export pipeline** to better support hierarchical classification tasks. Currently, our pipeline focuses only on species that meet a minimum threshold of research‐grade observations in the bounding box and does not systematically gather all their ancestral taxa (e.g., genus, family, order). This design limits the data’s usefulness in scenarios where the model must “know when to stop”—i.e., return a coarser taxonomic label for rare or partially identified specimens. By explicitly collecting each species’s ancestors, we can generate training data that captures the broad taxonomic context for each in‐threshold species, plus partial observations for species that do not meet the threshold but are still informative at higher ranks. Furthermore, new user requirements—such as allowing a user to specify a root rank (not always L70) and automatically wiping low‐occurrence taxa—underscore the need for a flexible, robust mechanism to unify coarse and fine ranks in a single workflow.

# 2. Requirements & Goals

1. **Ancestor Inclusion**  
   - For each species that meets the regional threshold (`MIN_OBS` of research‐grade observations), we must add all relevant ancestral taxa (genus, family, order, etc.) up to a specified root rank. This ensures we do not discard potentially valuable coarser labels.

2. **Root Rank Flexibility**  
   - The user may define a “clade root” to limit how far up we gather ancestors (e.g., only up to L40=order if `CLADE` is at L50=class).  
   - If the user provides a multi‐root `METACLADE`, we repeat the logic for each root.  

3. **Preserving Partial Observations**  
   - Observations of rare species (below the threshold) must still be included if they share an ancestor with a species that meets the threshold. Example: a species that has only 20 research‐grade observations might be worthless for species‐level classification, but still valuable for genus/family modeling.  

4. **Low‐Occurrence Taxon Wiping**  
   - Even after we gather the full lineage, if a particular taxon (say a genus with few total occurrences) fails to meet a user‐configured threshold, we wipe its label from the relevant observations. This prevents the model from trying to learn extremely rare or ill‐defined ranks, while still retaining the rest of the observation’s taxonomic ranks if they exceed the threshold.

5. **Integration with Existing Pipeline**  
   - The solution must integrate cleanly with the existing code structure: `regional_base.sh` for building the base set of species, and `cladistic.sh` for applying final logic.  
   - We also must consider how `SKIP_REGIONAL_BASE=true` and table naming might need to accommodate new parameters or extended logic, ensuring we do not skip creation of a base table that should differ.

# 3. Proposed Approaches

Below are conceptual solutions to implement the ancestor‐aware feature, balancing correctness, performance, and maintainability.

1. **Phase 1 (Species Selection):**  
   - Unchanged at first: gather species that meet `MIN_OBS` in the bounding box. This set becomes \( S \).  
   - For each species in \( S \), we identify its ancestors up to a user‐defined or clade‐derived root rank. This step queries `expanded_taxa` to retrieve `L20_taxonID`, `L30_taxonID`, etc.

2. **Phase 2 (Ancestor Inclusion):**  
   - Compute the union of all ancestor IDs for the species in \( S \). Denote that union as \( T \).  
   - Combine \( S \cup T \) to form `_all_taxa`.  
   - `_all_taxa_obs` is then defined as **all** observations whose `taxon_id` is in `_all_taxa`. If `INCLUDE_OUT_OF_REGION_OBS=true`, we include those observations globally; if not, we reapply the bounding box.  

3. **Phase 3 (Filtering & Wiping):**  
   - Once `_all_taxa_obs` is formed, we apply any logic that wipes certain ranks if they fail an absolute threshold. For instance, if a genus `G` has fewer than `MIN_OBS_GENUS` occurrences, we set `L20_taxonID=NULL` on all relevant rows, effectively turning them into coarser labels.  
   - We might also unify or refine the rank threshold with the existing `RG_FILTER_MODE` concept.

4. **Table Naming & Skip Logic:**  
   - If `INCLUDE_OUT_OF_REGION_OBS` or other new variables impact the final set of `_all_taxa`, we can incorporate them into the naming pattern of the base table, e.g., `<REGION_TAG>_min${MIN_OBS}_IOORtrue_ancestors_all_taxa`. This reduces confusion about whether a table truly matches the current pipeline parameters.  
   - Alternatively, we could skip storing `_all_taxa` entirely and build the final `_all_taxa_obs` in one pass. But storing `_all_taxa` can be helpful for debugging or subsequent reuse.

5. **Performance & Indices:**  
   - Gathering ancestors for each species might be expensive if we do it row by row. We can rely on `expanded_taxa` columns and a single or few set-based queries. For instance, do a join on `expanded_taxa` once, unnest relevant columns (up to the root rank), and deduplicate.

# 4. Decision & Summary

We will adopt a **single pass** approach to ancestor inclusion at the end of `regional_base.sh`:

- **After** we identify the in-threshold species, we gather their ancestors via a SQL query that unrolls `L10_taxonID`, `L20_taxonID`, etc. up to the chosen root rank.  
- The union of those IDs with the species set becomes `_all_taxa`.  
- Then `_all_taxa_obs` is formed by including all observations referencing any ID in `_all_taxa`, subject to bounding-box toggles.  
- Next, in `cladistic.sh`, we optionally wipe certain ranks (genus/family/etc.) if they fail a usage threshold. We also apply `RG_FILTER_MODE` for research vs. non‐research filtering.

**Key Gains:**
- We correctly keep partial-labeled or rare species observations.  
- Observations referencing a genus or family are still included, even if that rank wasn’t physically observed in bounding box, because it is the ancestor of a species in the region.  
- `INCLUDE_OUT_OF_REGION_OBS` remains a toggle controlling whether we gather out-of-region records for the selected `_all_taxa`.  

# 5. Optional: Future Extensions

1. **Multi‐Rank Thresholds**  
   - We might eventually define distinct `MIN_OBS` for genus vs. family vs. species. This would refine or unify partial-labeled data.  
2. **Selective Mix of “Ancestor Only”**  
   - In some cases, the user might not want to preserve, say, all L70=kingdom observations. We could define a cut at `L50` or `L40`.  
3. **Precomputed Caches**  
   - For large datasets, we might precompute each species’s ancestry in a separate table to avoid repeated unnest queries.  

# 6. Implementation Plan

Below is the plan for introducing ancestor‐aware functionality and partial‐rank handling. We integrate the new user requirements: (1) gathering ancestral taxa for each species that meets the bounding‐box threshold, up to a user‐specified root rank, and (2) automatically “wiping” taxonIDs that fail an extended usage threshold. We also clarify table naming conventions so that skip logic in `main.sh` can be consistently applied.

## 6.1 Step‐by‐Step Outline

1. **Extend `regional_base.sh` to produce two base tables**:  
   - **`<REGION_TAG>_min${MIN_OBS}_all_sp`**: This is the current table of *just* species (rank=10) that pass the research‐grade threshold in region.  
   - **`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors`**: A table that includes each species from the first table plus all of its ancestral taxonIDs (up to the root rank).  
     - **CLARIFY**: If `CLADE`=“angiospermae” is at L57, we only gather ancestors up to L50. If `METACLADE` is multi‐root, we do the same for each root.  
     - To build this, we join `<REGION_TAG>_min${MIN_OBS}_all_sp` to `expanded_taxa e` on `e."taxonID" = species_id`, then unnest or gather columns L10, L20, etc., up to the user’s root rank, and insert them into the final table.  

2. **Form the final `_all_taxa_obs` using the union of those IDs**:  
   - Once we have `<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors` (the union of species plus any ancestors), we can produce `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs` the same way we do now, except the condition is:  
     - `observations.taxon_id` in `(SELECT taxon_id FROM <REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors)`  
     - If `INCLUDE_OUT_OF_REGION_OBS=false`, we re‐apply bounding box. If `true`, we do not.  
   - **CLARIFY**: We must confirm how skip logic and table naming incorporate `INCLUDE_OUT_OF_REGION_OBS`. Possibly name the table `"_all_sp_and_ancestors_obs_ioorFalse"` if `INCLUDE_OUT_OF_REGION_OBS=false`, etc.  

3. **Refine `cladistic.sh`** to handle partial labeling logic:  
   1. **RG_FILTER_MODE** remains as is for controlling research‐grade vs. non‐research, species vs. genus, etc.  
   2. **Introduce the new “taxon usage threshold”**: e.g. `MIN_OCCURRENCES_PER_RANK`. If a rank’s usage is below that threshold, we nullify that rank for each relevant observation.  
      - We can do this after the table creation by an `UPDATE` pass: “UPDATE `<EXPORT_GROUP>_observations` SET L20_taxonID = NULL if L20_taxonID is below threshold, etc.” Or we embed a join in the creation query that checks usage counts.  
      - The usage count for each taxon can be computed by grouping `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs` or `<EXPORT_GROUP>_observations`. If a particular `L20_taxonID` or `L30_taxonID` has fewer than `MIN_OCCURRENCES_PER_RANK` occurrences, it is wiped.  
   3. The final CSV export subqueries remain the same (two subqueries unioned: capped research species vs. everything else).

4. **Adjust Table Naming & Skip Logic**:  
   - To avoid collisions:  
     1. **`<REGION_TAG>_min${MIN_OBS}_all_sp`** for the species list.  
     2. **`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors`** for the union of species + their ancestors.  
     3. **`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors_obs`** (with `_ioorTrue` or `_ioorFalse` suffix if we want) for the final base observations table.  
   - Then in `main.sh`, if `SKIP_REGIONAL_BASE=true`, we check for the presence of exactly the right table name. If we see partial or missing suffix, we know it’s not the same config.

5. **Implement “Root Rank” Logic**:  
   - For a single `CLADE` or `MACROCLADE` (like “amphibia” at L50=20978), gather ancestors only up to L40, or up to user’s specified rank. Possibly store this in an env var `ANCESTOR_ROOT_RANKLEVEL`. If `METACLADE` is multi‐root, we do it for each root and union them.  
   - This approach ensures we do not ascend beyond the clade definition. We can store a small dictionary of known ranklevels for the user’s clade roots, or parse from `clade_defns.sh`.

6. **Performance & Indices**:  
   - Each step might do large set operations if we have many species. We can consider an approach:  
     1. Build a temp table for the species set.  
     2. Join to `expanded_taxa` once, unnest columns L10–L70. Filter out columns above `ANCESTOR_ROOT_RANKLEVEL`.  
     3. Insert into the final `_all_sp_and_ancestors`.  
   - If performance is an issue, we might add a partial index or store a precomputed table of “taxon -> all ancestors up to rank=??.” But we can start with the simpler approach first.

## 6.2 Validation & Testing

- **Comparisons** between old pipeline vs. new pipeline with `RG_FILTER_MODE=ONLY_RESEARCH` should yield similar results for species, except we now see additional ancestor taxonIDs in `_all_sp_and_ancestors`.  
- Check the partial-labeled expansions by verifying that extremely rare species rows appear with `L10_taxonID`=NULL in final outputs if `MIN_OCCURRENCES_PER_RANK` is not met.  

## 6.3 Proposed “Incremental” Implementation

1. **Add** code in `regional_base.sh`:
   - Create `..._all_sp`.  
   - Gather ancestors into `..._all_sp_and_ancestors`.  
   - Then produce the final `..._all_sp_and_ancestors_obs`.  
2. **Update** `main.sh` skip logic to search for the new table name. Possibly unify naming with `ANCESTOR_ROOT_RANKLEVEL` or `INCLUDE_OUT_OF_REGION_OBS`.  
3. **Refine** `cladistic.sh`:
   - Insert a step or function that checks usage counts for each rank (20, 30, etc.) and overwrites them with NULL if below threshold. This can be done via an `UPDATE` or a left join with a usage table.  
   - Retain the union approach for final CSV export.  

---

# 7. Additional Planning / Checklists

**7.1 Confirm Known Variables & Defaults**  
1. `ANCESTOR_ROOT_RANKLEVEL`: Derive from user’s clade definitions. If user picks “amphibia” at L50, set this to 40 if they only want up to order.  
2. `MIN_OCCURRENCES_PER_RANK`: If not specified, default to the same `MIN_OBS`.  
3. If `CLARIFY:` is needed for multi‐root `METACLADES`, confirm how we store or pass multiple root ranklevels.

**7.2 Table Name Finalization**  
- Need consistent naming. e.g. `NAfull_min50_sp`, `NAfull_min50_sp_and_ancestors`, `NAfull_min50_sp_and_ancestors_obs_ioorTrue`. This ensures we never skip incorrectly.

**7.3 Potential Edge Cases**  
1. A species might have no recognized ancestors up to the root rank if the DB is incomplete. We handle that gracefully by an empty union.  
2. Rare rank usage. If a genus is used 5 times, but the user sets `MIN_OCCURRENCES_PER_RANK=10`, we wipe L20 for those rows. They might still keep L30 or L40 if those are above threshold.  
3. Multi‐root `METACLADES`. We gather ancestors for each root, union them, and proceed.

**7.4 Next Steps**  
- After finalizing the plan, we’ll proceed to **Phase 3**: coding. We’ll create a new `regional_base.sh` block for ancestor inclusion, rename or unify table creation, and augment `cladistic.sh` for partial rank wiping.

---

# 8. Implementation Progress & Updates

- **[x]** Completed detailed design in Phase 2 (sections 6–7).
- **[x]** Implemented new logic in `regional_base.sh` to:
  - Create `<REGION_TAG>_min${MIN_OBS}_all_sp` (just in-threshold species).
  - Gather all ancestors up to the user’s specified root rank (or each root rank if multi-root) to form `<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors`.
  - Produce `<REGION_TAG>_min${MIN_OBS}_sp_and_ancestors_obs` (or similarly named) if `INCLUDE_OUT_OF_REGION_OBS=true/false`.
- **[x]** Updated `main.sh` skip logic to incorporate the new table names for accurate detection.
- **[x]** Updated `cladistic.sh` to do partial rank wiping (if `MIN_OCCURRENCES_PER_RANK` is set).  
  - Implemented an `UPDATE` step after `<EXPORT_GROUP>_observations` creation to nullify rank columns that fail usage thresholds.
- **[ ]** **CLARIFY**: The multi-root `METACLADE` approach merges each root rank’s ancestors via union. If user sets 2–3 separate roots, we do a union of all ancestor IDs. This is implemented, but user must confirm test results.

**New Env Vars**:
- **`ANCESTOR_ROOT_RANKLEVEL`** (optional) – user can override automatic root detection if they want to limit the lineage more strictly than the clade’s rank. 
- **`MIN_OCCURRENCES_PER_RANK`** – controls partial label wiping; defaults to same as `MIN_OBS` if unspecified.

# 9. Final Results & Discussion

With these changes:
- **Research**: Rare species are no longer outright removed; even if their species label is invalid for training, they’re retained at coarser ranks.
- **Partial Observations**: We can “wipe” sub-threshold labels, ensuring that extremely rare taxa do not pollute the dataset.
- **Ancestry**: Our new approach systematically gathers each in-threshold species’s ancestors, so the pipeline is now **ancestor‐aware** instead of ignoring unobserved higher-rank taxa.

**Performance**: For very large datasets, we may consider precomputing taxon→ancestors or adding indexes. However, for moderate data, the approach should be sufficiently fast.

**Next Steps**:
- Thorough QA and test runs, especially with multi-root `METACLADE`s. 
- Possibly unify the partial-labeled approach with `RG_FILTER_MODE` if user demands more advanced logic (like skipping research-grade for certain ranks).


---

Can you clarify what you mean in this comment  in clade_helpers.sh?
```
# NOTE: We do not forcibly integrate with existing "get_clade_condition()"
# in clade_defns.sh. Instead, you can call parse_clade_expression() if you
# want to do deeper multi-root logic.
```

Furthermore, on this point:
```
	2.	check_root_independence():
	•	Conceptual approach to gather each root’s ancestry from expanded_taxa, confirm disjoint sets.
	•	Currently placeholders. Implementation details would involve real SQL queries.
```
a conceptual approach is not acceptable. We need to generate the complete implementation of this method:
```
# -------------------------------------------------------------
# C) check_root_independence()
# -------------------------------------------------------------
# This function ensures that each root in a multi-root scenario
# is truly independent. That is, no root is an ancestor or descendant
# of another. We do so by building the set of ancestor IDs for each root,
# then verifying disjointness pairwise.
#
# usage: check_root_independence "myDatabaseName" rootArray
#   rootArray: an array of "rank=taxonID" strings, e.g. "50=47158"
# We assume we can run a quick query on 'expanded_taxa' to gather
# the ~30 possible ancestor columns for each root's row, then compare sets.
#
# If overlap is found, we can either abort or print a warning. We'll choose to abort here.
#
function check_root_independence() {
  local dbName="$1"
  shift
  local roots=("$@")  # e.g. ("50=47158" "60=9999")

  if [ "${#roots[@]}" -le 1 ]; then
    # Nothing to check
    return 0
  fi

  # We'll build arrays of sets. For each root r_i, gather its expanded ancestry.
  declare -A rootSets  # a map from index to "list of taxonIDs"

  for i in "${!roots[@]}"; do
    local pair="${roots[$i]}"
    local rank="${pair%%=*}"
    local tid="${pair##*=}"

    # We'll do a single row fetch in expanded_taxa where taxonID=tid,
    # gather all columns "L5_taxonID", "L10_taxonID", ... "L70_taxonID".
    # Then store them in a set in memory.
    # We'll do a naive approach: psql call, parse results, etc.

    # Real code might do:
    # row=$(docker exec ...)
    # Then parse. For now, we conceptualize a pseudo-result.

    # For demonstration, let's pretend we run:
    # row_of_ancestors might look like "47158|47157|...|<some nulls>"
    # We'll parse them into an array, ignoring nulls.

    # We'll store them in e.g. rootSets["$i"] as "47158 47157 1" etc.
    # PSEUDOCODE:
    # (No real code, just conceptual)

    # rootSets["$i"]="${list_of_ancestors}"

    # For the sake of demonstration, we'll skip real queries.

    # <snip>
    :
  done

  # Now compare pairwise sets for overlap
  for ((i=0; i<${#roots[@]}; i++)); do
    for ((j=i+1; j<${#roots[@]}; j++)); do
      # Compare rootSets["$i"] and rootSets["$j"] for intersection
      # If non-empty => abort
      # PSEUDOCODE:
      # overlapCheck ...
      # if [ "$foundOverlap" = "true" ]; then
      #   echo "ERROR: Overlap detected between root i and j"
      #   return 1
      # fi
      :
    done
  done

  return 0
}

```

You may wish to analyze the exact structure of the expanded_taxa table to assist in the completion:
```
ibrida-v0-r1=# \d expanded_taxa
                        Table "public.expanded_taxa"
      Column      |          Type          | Collation | Nullable | Default
------------------+------------------------+-----------+----------+---------
 taxonID          | integer                |           | not null |
 rankLevel        | double precision       |           |          |
 rank             | character varying(255) |           |          |
 name             | character varying(255) |           |          |
 taxonActive      | boolean                |           |          |
 L5_taxonID       | integer                |           |          |
 L5_name          | character varying(255) |           |          |
 L5_commonName    | character varying(255) |           |          |
 L10_taxonID      | integer                |           |          |
 L10_name         | character varying(255) |           |          |
 L10_commonName   | character varying(255) |           |          |
 L11_taxonID      | integer                |           |          |
 L11_name         | character varying(255) |           |          |
 L11_commonName   | character varying(255) |           |          |
 L12_taxonID      | integer                |           |          |
 L12_name         | character varying(255) |           |          |
 L12_commonName   | character varying(255) |           |          |
 L13_taxonID      | integer                |           |          |
 L13_name         | character varying(255) |           |          |
 L13_commonName   | character varying(255) |           |          |
 L15_taxonID      | integer                |           |          |
 L15_name         | character varying(255) |           |          |
 L15_commonName   | character varying(255) |           |          |
 L20_taxonID      | integer                |           |          |
 L20_name         | character varying(255) |           |          |
 L20_commonName   | character varying(255) |           |          |
 L24_taxonID      | integer                |           |          |
 L24_name         | character varying(255) |           |          |
 L24_commonName   | character varying(255) |           |          |
 L25_taxonID      | integer                |           |          |
 L25_name         | character varying(255) |           |          |
 L25_commonName   | character varying(255) |           |          |
 L26_taxonID      | integer                |           |          |
 L26_name         | character varying(255) |           |          |
 L26_commonName   | character varying(255) |           |          |
 L27_taxonID      | integer                |           |          |
 L27_name         | character varying(255) |           |          |
 L27_commonName   | character varying(255) |           |          |
 L30_taxonID      | integer                |           |          |
 L30_name         | character varying(255) |           |          |
 L30_commonName   | character varying(255) |           |          |
 L32_taxonID      | integer                |           |          |
 L32_name         | character varying(255) |           |          |
 L32_commonName   | character varying(255) |           |          |
 L33_taxonID      | integer                |           |          |
 L33_name         | character varying(255) |           |          |
 L33_commonName   | character varying(255) |           |          |
 L33_5_taxonID    | integer                |           |          |
 L33_5_name       | character varying(255) |           |          |
 L33_5_commonName | character varying(255) |           |          |
 L37_taxonID      | integer                |           |          |
 L37_name         | character varying(255) |           |          |
 L37_commonName   | character varying(255) |           |          |
 L40_taxonID      | integer                |           |          |
 L40_name         | character varying(255) |           |          |
 L40_commonName   | character varying(255) |           |          |
 L43_taxonID      | integer                |           |          |
 L43_name         | character varying(255) |           |          |
 L43_commonName   | character varying(255) |           |          |
 L44_taxonID      | integer                |           |          |
 L44_name         | character varying(255) |           |          |
 L44_commonName   | character varying(255) |           |          |
 L45_taxonID      | integer                |           |          |
 L45_name         | character varying(255) |           |          |
 L45_commonName   | character varying(255) |           |          |
 L47_taxonID      | integer                |           |          |
 L47_name         | character varying(255) |           |          |
 L47_commonName   | character varying(255) |           |          |
 L50_taxonID      | integer                |           |          |
 L50_name         | character varying(255) |           |          |
 L50_commonName   | character varying(255) |           |          |
 L53_taxonID      | integer                |           |          |
 L53_name         | character varying(255) |           |          |
 L53_commonName   | character varying(255) |           |          |
 L57_taxonID      | integer                |           |          |
 L57_name         | character varying(255) |           |          |
 L57_commonName   | character varying(255) |           |          |
 L60_taxonID      | integer                |           |          |
 L60_name         | character varying(255) |           |          |
 L60_commonName   | character varying(255) |           |          |
 L67_taxonID      | integer                |           |          |
 L67_name         | character varying(255) |           |          |
 L67_commonName   | character varying(255) |           |          |
 L70_taxonID      | integer                |           |          |
 L70_name         | character varying(255) |           |          |
 L70_commonName   | character varying(255) |           |          |
Indexes:
    "expanded_taxa_pkey" PRIMARY KEY, btree ("taxonID")
    "idx_expanded_taxa_l10_taxonid" btree ("L10_taxonID")
    "idx_expanded_taxa_l20_taxonid" btree ("L20_taxonID")
    "idx_expanded_taxa_l30_taxonid" btree ("L30_taxonID")
    "idx_expanded_taxa_l40_taxonid" btree ("L40_taxonID")
    "idx_expanded_taxa_l50_taxonid" btree ("L50_taxonID")
    "idx_expanded_taxa_l60_taxonid" btree ("L60_taxonID")
    "idx_expanded_taxa_l70_taxonid" btree ("L70_taxonID")
```
Otherwise you should have enough information about the interfaces and requirements for the check_root_independence() method to implement it fully. 

Return the complete implementation of the method. Assume that the expanded_taxa table is always available. You could use the execute_sql() method from function.sh for the execution of any queries (assuming that this function can return in the form we need), or add a new function if needed to execute the actual SQL commands:
```
#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

....
```
I'm assuming that the DB_CONTAINER, DB_USER, and DB_NAME env vars will be defined in the scope that this function is called. The expanded_taxa table will always be available on the same database as all the other tables we are working with (so just use the DB_ env vars used elsewhere in the flow).

Were there any other 'conceptual' incomplete implementations in your most recent response? If so, you need to return full implementations; taking extra time to think if necessary. 

If not, please return the complete implementation of Section C of clade_helpers.sh:
```
# -------------------------------------------------------------
# C) check_root_independence()
# -------------------------------------------------------------
# This function ensures that each root in a multi-root scenario
# is truly independent. That is, no root is an ancestor or descendant
# of another. We do so by building the set of ancestor IDs for each root,
# then verifying disjointness pairwise.
#
# usage: check_root_independence "myDatabaseName" rootArray
#   rootArray: an array of "rank=taxonID" strings, e.g. "50=47158"
# We assume we can run a quick query on 'expanded_taxa' to gather
# the ~30 possible ancestor columns for each root's row, then compare sets.
#
# If overlap is found, we can either abort or print a warning. We'll choose to abort here.
#
function check_root_independence() {
  local dbName="$1"
  shift
  local roots=("$@")  # e.g. ("50=47158" "60=9999")

  if [ "${#roots[@]}" -le 1 ]; then
    # Nothing to check
    return 0
  fi

  # We'll build arrays of sets. For each root r_i, gather its expanded ancestry.
  declare -A rootSets  # a map from index to "list of taxonIDs"

  for i in "${!roots[@]}"; do
    local pair="${roots[$i]}"
    local rank="${pair%%=*}"
    local tid="${pair##*=}"

    # We'll do a single row fetch in expanded_taxa where taxonID=tid,
    # gather all columns "L5_taxonID", "L10_taxonID", ... "L70_taxonID".
    # Then store them in a set in memory.
    # We'll do a naive approach: psql call, parse results, etc.

    # Real code might do:
    # row=$(docker exec ...)
    # Then parse. For now, we conceptualize a pseudo-result.

    # For demonstration, let's pretend we run:
    # row_of_ancestors might look like "47158|47157|...|<some nulls>"
    # We'll parse them into an array, ignoring nulls.

    # We'll store them in e.g. rootSets["$i"] as "47158 47157 1" etc.
    # PSEUDOCODE:
    # (No real code, just conceptual)

    # rootSets["$i"]="${list_of_ancestors}"

    # For the sake of demonstration, we'll skip real queries.

    # <snip>
    :
  done

  # Now compare pairwise sets for overlap
  for ((i=0; i<${#roots[@]}; i++)); do
    for ((j=i+1; j<${#roots[@]}; j++)); do
      # Compare rootSets["$i"] and rootSets["$j"] for intersection
      # If non-empty => abort
      # PSEUDOCODE:
      # overlapCheck ...
      # if [ "$foundOverlap" = "true" ]; then
      #   echo "ERROR: Overlap detected between root i and j"
      #   return 1
      # fi
      :
    done
  done

  return 0
}
```

Now, looking towards the next steps:
```
	2.	Proposed Next Steps
	•	Step A: Integrate clade_helpers.sh usage in regional_base.sh. For multi-root detection, we call parse_clade_expression(get_clade_condition), partition species, etc.
	•	Step B: Expand check_root_independence() with real SQL queries to gather each root’s ancestor set from expanded_taxa.
	•	Step C: Add boundary logic (get_major_rank_floor()) or minor-rank logic as user toggles in regional_base.sh.
	•	Step D: In cladistic.sh, ensure partial-rank wiping remains consistent with any newly introduced rank columns or edge cases.

```

In your response, please return the complete implementation of Section C on clade_helpers.sh AND prepare a refined, highly specific implementation plan for the remaining steps. Please identify any outstanding clarifications that you will need to return full, robust reimplementations for all remaining steps.
      </file>
      <file path="/home/caleb/repo/ibridaDB/dev/regional_base_reusability.md">
- The upper boundary used for ancestor search, which is determine by the CLADE/METACLADE/MACROCLADE, is used for the regional-base tables ()"${REGION_TAG}_min${MIN_OBS}_all_sp_and_ancestors\"), not on one of the clade-export specific tables in cladistic.sh. So this is at odds with the previous design (before we added ancestor-aware logic), where previously the _all_sp table only varied by the REGION_TAG/MIN_OBS. This is probably OK, and might even be necessary for the purposes of determining the exact set of ancestral taxonIDs that need to be included in the base tables when looking to export more than just research-grade observations (i.e. observations with an uncontested species-level label) but it is a bit of a departure from the previous design. So we need to confirm what the set of taxa in the _all_sp_and_ancestors table depends upon (I think it is only the REGION_TAG/MIN_OBS/boundary ranks), and we can potentially mitigate by adjusting the generated base tables names to include the highest root rank (or highest root ranks, in the case of metaclades with multiple root ranks) used in the ancestor search; this will properly version the regional base tables and prevent reuse of base tables when the ancestor scopes differ.
  - So this means that the boundaries of the ancestor search for generating the regional _all_sp_and_ancestors is defined with respect to the configured clade/metaclade for a job, and so the regional base table might need to be recreated for successive job using clades/metaclades with different root ranks.
    - Really, the ancestor-aware logic should be implemented on the cladistic.sh tables.
    - The regional base table names do not fully capture the 'versining', so e.g. a NAfull_min50_all_sp_and_ancestors table generated from a PTA (metaclade) job would not be reusable for a successive job that used a MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)' macroclade, since the PTA root ranks are lower than the L60 rank-- so that regional base table would be missing L50 ancestors. 
      - This would actually be OK in theory but it might break some downstream assumptions, so it would be better to recreate the regional base table for each successive job if that job uses a different root rank.
      - TODO: Confirm that it is only the root rank, not the root taxonID, that is used to define the ancestor search for the regional base tables.
        - If the regional base table _all_sp_and_ancestors only varies by the REGION_TAG/MIN_OBS/boundary ranks, then we could mitigate by adjusting the generated base tables names to include the highest root rank used in the ancestor search.
        - Otherwise, we would need to include the CLADE/METACLADE/MACROCLADE in the regional base table name.
  - regional base table is an increasingly inappropraite name for this table. It was fine when the tables always just included the species in the region that passed the MIN_OBS threshold/the corresponding observations, but the contents of the table are now dependent on the CLADE/METACLADE/MACROCLADE.
    - This issue was averted for INCLUDE_OUT_OF_REGION_OBS, because the regional base observations table always include all observations for the species in the region that passed the MIN_OBS threshold (and now for all their ancestors in the scope of the ancestor search, too).
      - And then if INCLUDE_OUT_OF_REGION_OBS=false, then we re-applied the bounding box for the final table.
    - There might be a similar mitigation approach we could take for ancestor search here. A much more inclusive set of observations for, i.e. _all_sp_and_ancestors would include all species in the region that passed the MIN_OBS threshold and all the ancestors of those species up to but not including L100 (state of matter), i.e. unrestricted ancestor search. _sp_and_ancestors_obs would include all observations where taxon_id=[<a taxonID from _all_sp_and_ancestors].
      - By default, only search for the major-rank ancestors, i.e. L20, L30, L40, L50, L57, L60, L70. So INCLUDE_MINOR_RANKS_IN_ANCESTORS=false. If INCLUDE_MINOR_RANKS_IN_ANCESTORS=true, then include minor ranks in the unbounded ancestor search, and adjust the table names (_all_sp_and_ancestors_incl_minor_ranks, _sp_and_ancestors_obs_incl_minor_ranks). Searching minor ranks can occur significant performance penalty as only major ranks are indexed, and we will not need to use this in the intermediate future.
      - Possibly do a second ancestor search with only the set of ancestors that are up to the boundary rank for that export? This would be used for the filtering for the final table for that export job.
      - But then, for the final table, we'd need to apply a filter to exclude the observations that fall outside of the scope of the job/clade-specific ancestor search. It gets kind of complicated to define this filter-- do we consider the base taxonID of the observation? Not necessarily, since sometimes we wipe labels of some ranks if the no. observations for that taxonID are too low in the final step (depending on MIN_OCCURRENCES_PER_RANK, which is an alternate floor for wiping labels for higher ranks if the no. occurences for those taxonIDs are too low, while still keeping the observations if they have passing labels for some ranks that are not wiped, e.g. an observation has a non-research grade species label, so we wipe L10_taxonID to null, and it has a L20_taxonID label that is very rare, so occurs less than MIN_OCCURRENCES_PER_RANK in the final table, but it has a usable L30_taxonID label and we want to downstream model to be able to learn that a sample like 'that' should be learned as being in that family (L30).
    - My current standing is that we should do the ancestor search with the constrained upper boundary, and accept that we will lose some reusability between regional base tables between exports that use different clades/metaclades (but maybe can reuse if the ranks of the clades stay constant between jobs, need to confirm).
      - This simplifies the logic in cladistic.sh and preserves the functionality of the existing filters in cladistic.sh.

---

Below is a **high‐level architectural analysis** of the complication you raised about **root‐rank–dependent upper boundaries** for ancestor searches in the _regional base_ tables. We then outline **three possible strategies** (with trade‐offs) to handle it, so you can choose a design that best fits your workflow. Finally, we discuss a **table‐naming** convention to avoid confusion when reusing tables across jobs.

---

## 1. The Complication: “Regional Base” vs. “Clade‐Dependent Boundary”

### Original Assumption
Your “regional base” tables (e.g. `NAfull_min50_all_sp_and_ancestors`) historically **only** depended on:
- **REGION_TAG** (to define bounding box)
- **MIN_OBS** threshold (minimum species observations)

They thus served as a general “base” for subsequent tasks—especially in the old design, before introducing ancestor‐aware logic.

### New Reality
With the new ancestor‐aware approach, **which** ancestors get included in the “base” table can also depend on:
- The **root rank** (or ranks, for multi‐root metaclades) that define how high in the taxonomy you gather ancestors.  
- The **INCLUDE_MINOR_RANKS_IN_ANCESTORS** toggle (whether you skip or include sublevels like L11, L13, etc.).

This means the “base” table’s contents are **no longer** universal. If your user picks a metaclade with root rank=50, you might gather ancestors only up to rank<40; or if the user picks root rank=60, you gather up to rank<50. Another job may want the entire “Arthropoda” domain or even an unbounded approach. Consequently, your “base” table changes based on the clade or macroclade used.

**Implication**: A single “NAfull_min50_all_sp_and_ancestors” table is **no longer** guaranteed to be correct or complete for all clade exports. If it was built for a certain rank boundary, it might miss some ancestors needed for a different rank boundary in a later job. This breaks the old assumption that the same “base” can be safely reused.

---

## 2. Three Potential Strategies

### **Strategy A: Create a “Maximal” Ancestor Base Once, Then Filter Later**
1. Always gather **all** ancestors up to the broadest possible rank—for example, up to L70 (or L60) for every species that meets MIN_OBS.  
2. This produces an **all‐encompassing** base table, large but fully inclusive.  
3. In your subsequent “cladistic” step (or some final filtering step), you apply the user’s rank boundaries to remove any ancestors above that user’s chosen limit.

**Pros**  
- You get a single “_all_sp_and_ancestors” table per region (and MIN_OBS) that can be reused across all jobs.  
- No repeated table creation for each clade/macroclade.  
- Maintains the older “regional_base” idea: the table only depends on region & MIN_OBS.

**Cons**  
- Potentially **bigger** table with more ancestors than needed for a narrower clade.  
- Slightly **slower** queries and merges in some workflows, because you must store and then filter out unneeded ancestors.

### **Strategy B: Always Build Job‐Specific “Regional Base” Tables**
1. The “_all_sp_and_ancestors” table is **no longer** purely region‐based; it also includes the job’s rank boundary.  
2. If a user picks a new clade with a different rank boundary, we **rebuild** that table.  
3. Name the table to reflect both region/min obs **and** the boundary rank(s). For multi‐root, store the highest rank or even each root rank in the name.

**Pros**  
- Each job’s table is precisely tailored to the chosen clade and rank boundary.  
- Minimal extraneous data.  
- No potential confusion about whether an old table had the right upper boundary.

**Cons**  
- Table creation is repeated for each new job/clade. This is more time‐consuming.  
- If you want to **reuse** tables across multiple similar exports, you need to ensure the boundaries match exactly—and if not, you must build a new table.

### **Strategy C: Hybrid Approach**
1. Maintain a “maximal” ancestor table up to L60 or L70 for each region.  
2. For narrower clades (like root=50 or 57), do an additional quick pass that **filters** out ancestors beyond that rank from the final step.  
   - This can happen in your “cladistic.sh” or in a second ephemeral table.  
3. If someone later chooses a new clade with a bigger boundary, you might have to rebuild the “maximal” table if it wasn’t big enough.

This approach tries to strike a **balance**—you keep a fairly inclusive base so you do not have to rebuild for every small change in rank, but if someone picks an even higher rank than you accounted for, you do a one‐off rebuild.

---

## 3. Table‐Naming Conventions to Avoid Confusion

No matter which strategy you adopt, you may want to embed the relevant boundary/flags into the table name if it can vary from job to job. For instance:

1. **If Strategy B** (job‐specific):  
   - `NAfull_min50_all_sp_and_ancestors_root50` or  
   - `NAfull_min50_all_sp_and_ancestors_macroL60`  
   - Possibly also `_minorTrue` if `INCLUDE_MINOR_RANKS_IN_ANCESTORS=true`.

2. **If Strategy A** (maximal base):  
   - `NAfull_min50_all_sp_and_ancestors_upToL70`  
   - Then you know it’s big enough for any clade up to rank=70.

Either way, ensure the name **uniquely** identifies the maximum rank boundary or any toggles (like “minor rank included”), so you do not accidentally skip regeneration for a job that needs a different boundary.

---

## 4. Additional Observations

1. **Performance & Indexing**  
   - If you gather many “unnecessary” ancestors, you might see performance slowdowns. Conversely, building many separate base tables for each job has overhead. Balancing these trade‐offs depends on how frequently you run these jobs, how large your dataset is, and how often your rank boundaries differ.

2. **Potential for a “cladistic” Pre‐Step**  
   - As you noted, if the “regional base” is truly region + MIN_OBS only, then you can do the clade‐specific ancestor unrolling in `cladistic.sh`. But that means the “regional base” table might not contain the ancestor taxonIDs at all, which in turn means the “cladistic” step must do the entire unrolling from species to top rank.  
   - Some teams prefer that approach so the “regional base” table remains purely region + species threshold only. Others want that ancestor logic **up front** (especially if they do multiple exports that rely on the same ancestor set).

3. **Minor vs. Major Ranks**  
   - If you do a “maximal” approach, do you also include minor ranks in your final base table? If you rarely need them, you might skip them in that “maximal” approach to avoid ballooning table size.  
   - Alternatively, you create two “maximal” base variants: `_inclMinor` and `_majorOnly`.

4. **Reusability**  
   - The core question: **Is reusability across multiple exports important enough** to maintain a single maximal table? Or is your pipeline ephemeral enough that building a job‐specific table is acceptable? If you do many exports with different rank boundaries, building a single superset table might be more efficient overall.

---

## 5. Summary: “Yes, We’re Departing from the Old Reuse Model”

- **You are correct** that once you tie the ancestor search to the user’s chosen root rank(s), you lose a one‐size‐fits‐all “regional base.”  
- **This is normal** for advanced multi‐clade or multi‐metaclade logic. Many solutions **either** do all unrolling in a final filtering step (so the base remains purely region + species) **or** accept that the base is job‐specific and rename it accordingly.

Ultimately, **the simplest resolution** is to **pick a strategy** for how large (or small) you want your “regional base” table to be:

- If you want a single multi‐purpose base, go for **Strategy A** and gather ancestors up to L60 or L70 for everyone. Then rely on `cladistic.sh` to further refine the boundary.  
- If you prefer a tight alignment of “base table == the exact set of ancestors for a particular job,” adopt **Strategy B**.  
- If you have moderate variety in rank boundaries, but not an enormous range, you could do **Strategy C**—some partial superset approach.

In any case, you’ll want to revise your table naming convention to reflect the **boundary ranks** (and possibly the minor‐rank inclusion setting) to avoid accidental reuse of an incompatible table.
      </file>
      <file path="/home/caleb/repo/ibridaDB/docker/stausee/entrypoint.sh">
#!/bin/bash
set -e

# Just log and exit - let Docker's default entrypoint handle PostgreSQL
echo "Entrypoint script executed at $(date)"
      </file>
      <file path="/home/caleb/repo/ibridaDB/docker/stausee/notes.md">
Moving the ibrida psql database to stausee-pool's 'database' dataset.
    'database' dataset is tuned for performance with psql:
```md
3. `stausee-pool/database` (Database Storage):
   - Purpose: PostgreSQL database files
   - Optimizations:
     - recordsize=8K: Matches database page size
     - logbias=latency: Optimized for write performance
     - sync=standard: Ensures data integrity
     - quota=2T: Controlled growth
   - Best for:
     - PostgreSQL data directory
     - High-IOPS workloads
     - Transaction-heavy applications
```
Note that the 'database' dataset is mounted at `mango/database` (`/database` lns here).
      </file>
      <file path="/home/caleb/repo/ibridaDB/docs/README.md">
# ibridaDB

**ibridaDB** is a modular, reproducible database system designed to ingest, process, and export biodiversity observations from the [iNaturalist open data dumps](https://www.inaturalist.org/). It leverages PostgreSQL with PostGIS to efficiently store and query geospatial data and includes specialized pipelines for:

- **Data Ingestion:** Importing CSV dumps, calculating geospatial geometries, and updating metadata.
- **Elevation Integration:** Optionally enriching observations with elevation data derived from MERIT DEM tiles.
- **Data Export:** Filtering observations by region and taxonomic clade, performing advanced ancestor searches, and exporting curated CSV files for downstream model training.

This repository contains all the code, Docker configurations, and documentation required to build, run, and extend ibridaDB.

---

## Table of Contents

- [ibridaDB](#ibridadb)
  - [Table of Contents](#table-of-contents)
  - [Overview](#overview)
  - [Architecture](#architecture)
  - [Directory Structure](#directory-structure)
  - [Ingestion Pipeline](#ingestion-pipeline)
  - [Elevation Data Integration](#elevation-data-integration)
  - [Export Pipeline](#export-pipeline)
  - [Docker Build and Deployment](#docker-build-and-deployment)
  - [Configuration \& Environment Variables](#configuration--environment-variables)
  - [Adding a New Release](#adding-a-new-release)
  - [Release notes](#release-notes)
  - [License](#license)
  - [Final Notes](#final-notes)

---

## Overview

**ibridaDB** automates the process of:
- Reproducing a spatially enabled database from iNaturalist open data dumps.
- Optionally enriching the database with elevation data from MERIT DEM.
- Exporting curated subsets of observations for downstream training of specimen identification models.

The system is versioned both in terms of **database structure** (Version, e.g., "v0") and **data release** (Release, e.g., "r1"). These concepts allow you to reproduce different releases of the database while keeping the underlying schema consistent.

---

## Architecture

The overall workflow of ibridaDB is divided into three main stages:

1. **Ingestion:**  
   - **CSV Import:** Load observations, photos, taxa, and observers from CSV files.
   - **Geometry Calculation:** Compute geospatial geometries (using latitude/longitude) with PostGIS.
   - **Metadata Update:** Set version, release, and origin metadata on each table.
   - **Optional Elevation Processing:** If enabled, create a PostGIS raster table for DEM tiles, load MERIT DEM data, and update each observation with an elevation value.

2. **Export:**  
   - **Regional Base Tables:** Build tables that restrict species by geographic bounding boxes and minimum observation counts.
   - **Cladistic Filtering:** Further subset the observations based on taxonomic clade (or metaclade) conditions and other quality filters.
   - **Final CSV Export:** Generate CSV files with additional information (including photo metadata) and optionally include elevation data.

3. **Dockerized Deployment:**  
   - The system runs in a Docker container using a custom image that extends the official PostGIS image to include the `raster2pgsql` CLI tool (necessary for elevation data processing).

A high-level diagram of the export flow is shown below:

```mermaid
flowchart TB
    A["Wrapper Script<br/>(e.g., r1/wrapper.sh)"] --> B["Main Export Script<br/>(common/main.sh)"]
    B --> C{"Skip Regional Base?"}
    C -- "true" --> D["Reuse Existing Tables"]
    C -- "false" --> E["regional_base.sh<br/>(Create/Update Base Tables)"]
    D --> F["cladistic.sh<br/>(Apply Cladistic Filters & Export CSV)"]
    E --> F["cladistic.sh<br/>(Apply Cladistic Filters & Export CSV)"]
    F --> G["Export Summary Generated"]
```

---

## Directory Structure

The repository is organized as follows:

```
ibridaDB/
├── dbTools/
│   ├── ingest/
│   │   └── v0/
│   │       ├── common/
│   │       │   ├── geom.sh              # Geometry calculations
│   │       │   ├── vers_origin.sh       # Version and origin metadata updates
│   │       │   └── main.sh              # Core ingestion logic
│   │       ├── r0/                      # Parameters for the initial release (r0)
│   │       │   └── wrapper.sh
│   │       ├── r1/                      # Parameters for the r1 release
│   │       │   └── wrapper.sh
│   │       └── utils/
│   │           ├── add_release.sh       # Legacy release update script
│   │           └── elevation/           # Elevation pipeline tools
│   │               ├── create_elevation_table.sql
│   │               ├── create_elevation_table.sh
│   │               ├── load_dem.sh
│   │               ├── main.sh          # Orchestrates elevation ingestion
│   │               ├── update_elevation.sh
│   │               └── wrapper.sh
│   └── export/
│       └── v0/
│           ├── common/
│           │   ├── functions.sh         # Shared export functions (including get_obs_columns())
│           │   ├── clade_defns.sh         # Clade condition definitions
│           │   ├── clade_helpers.sh       # Helpers for multi-root and clade processing
│           │   ├── regional_base.sh       # Regional base table generation
│           │   ├── cladistic.sh           # Cladistic filtering and final CSV export
│           │   └── main.sh                # Main export orchestration
│           └── r1/                        # Release-specific export wrappers
│               ├── wrapper_amphibia_all_exc_nonrg_sp.sh
│               ├── wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
│               ├── wrapper_pta_all_exc_nonrg_sp.sh
│               └── wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh
├── docker/
│   ├── Dockerfile                       # Custom Docker image build (with raster2pgsql)
│   └── (other Docker-related files, e.g., docker-compose.yml)
└── README.md                            # This high-level documentation
```

---

## Ingestion Pipeline

The ingestion pipeline is contained in `dbTools/ingest/v0/` and performs the following:

1. **Database Initialization:**  
   - Uses wrapper scripts (e.g., `r1/wrapper.sh`) to set parameters (DB name, source info, etc.).
   - The main script (`common/main.sh`) creates the database, imports CSV files, sets up tables and indexes, and computes geometries via `geom.sh`.

2. **Metadata Updates:**  
   - Updates the `origin`, `version`, and `release` columns in each table (via `vers_origin.sh`).

3. **Elevation Integration (Optional):**  
   - When `ENABLE_ELEVATION=true` is set in the wrapper, the elevation pipeline in `utils/elevation/` is invoked.
   - This pipeline creates an `elevation_raster` table, loads MERIT DEM tiles (using `raster2pgsql`), and updates `observations.elevation_meters`.

**Quick Start Example for Ingestion:**

```bash
chmod +x dbTools/ingest/v0/common/main.sh dbTools/ingest/v0/common/geom.sh dbTools/ingest/v0/common/vers_origin.sh dbTools/ingest/v0/r1/wrapper.sh
# To ingest a new release with elevation:
ENABLE_ELEVATION=true dbTools/ingest/v0/r1/wrapper.sh
```

---

## Elevation Data Integration

The elevation pipeline (located in `dbTools/ingest/v0/utils/elevation/`) provides the following functionality:

- **Create Elevation Table:**  
  - Runs `create_elevation_table.sh` to ensure the `elevation_raster` table exists.

- **Load DEM Data:**  
  - Uses `load_dem.sh` to extract and load MERIT DEM tiles into the database using `raster2pgsql`.  
  - **Note:** This requires the custom Docker image built with `raster2pgsql` (see Docker Build section below).

- **Update Elevation:**  
  - Runs `update_elevation.sh` to populate `observations.elevation_meters` using spatial joins with the raster data.
  
The pipeline is activated by setting `ENABLE_ELEVATION=true` in your ingest wrapper.

---

## Export Pipeline

The export pipeline (located in `dbTools/export/v0/`) allows you to generate specialized CSV exports from your ibridaDB database. Key features include:

1. **Regional Base Table Generation:**  
   - `regional_base.sh` creates base tables filtering species by a geographic bounding box (defined by `REGION_TAG`) and by a minimum number of research-grade observations (`MIN_OBS`).
   - It supports an option (`INCLUDE_OUT_OF_REGION_OBS`) to include all observations for selected species, along with computing an `in_region` boolean.

2. **Cladistic Filtering:**  
   - `cladistic.sh` further filters the regional observations by taxonomic clade (using `CLADE`, `METACLADE`, or `MACROCLADE` defined in `clade_defns.sh`).
   - The script also supports advanced options such as partial rank wiping (using `MIN_OCCURRENCES_PER_RANK` and `INCLUDE_MINOR_RANKS_IN_ANCESTORS`) and research-grade filtering (using `RG_FILTER_MODE`).

3. **CSV Export:**  
   - The final CSV export includes explicit columns from the observations and photo tables.
   - A new column `elevation_meters` is now included in the export if enabled by the environment variable `INCLUDE_ELEVATION_EXPORT` (set to true by default for new releases).
   - The CSV is produced via a partition-based random sampling method, ensuring that in-region research-grade observations are preferentially selected (controlled by `PRIMARY_ONLY` and `MAX_RN`).

**Quick Start Example for Export:**

```bash
chmod +x dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
# Run export (with elevation enabled) using the wrapper:
dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
```

---

## Docker Build and Deployment

**Custom Docker Image:**

The official PostGIS Docker image does not include the `raster2pgsql` CLI tool. To support elevation processing, we build a custom image that:

- Uses a multi-stage Docker build to install the PostGIS package (which includes `raster2pgsql`) in a builder stage.
- Copies the `raster2pgsql` binary into the final image.
- Is used in our Docker Compose configuration.

**Build and Push Instructions:**

```bash
cd docker
docker build -t frontierkodiak/ibridadb:latest . --no-cache
docker login
docker push frontierkodiak/ibridadb:latest
```

**Docker Compose:**

Our `docker-compose.yml` maps necessary volumes for:
- Exporting data (`/datasets/ibrida-data/exports`)
- Ingesting metadata (`/datasets/ibrida-data/intake`)
- Providing DEM data (`/datasets/dem`)

Refer to the `docker/stausee/docker-compose.yml` file for details.

---

## Configuration & Environment Variables

Both the ingest and export pipelines are configured via a rich set of environment variables. Key ones include:

- **Database Configuration:**  
  - `DB_USER`, `VERSION_VALUE`, `RELEASE_VALUE`, `ORIGIN_VALUE`, `DB_NAME`, `DB_CONTAINER`

- **Ingestion-Specific:**  
  - `SOURCE`, `METADATA_PATH`, `ENABLE_ELEVATION`, `DEM_DIR`, `EPSG`, `TILE_SIZE`

- **Export-Specific:**  
  - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`
  - Taxonomic filters: `CLADE`, `METACLADE`, `MACROCLADE`
  - Advanced export toggles:  
    - `INCLUDE_OUT_OF_REGION_OBS`  
    - `RG_FILTER_MODE`  
    - `MIN_OCCURRENCES_PER_RANK`  
    - `INCLUDE_MINOR_RANKS_IN_ANCESTORS`  
    - **`INCLUDE_ELEVATION_EXPORT`** – controls whether `elevation_meters` is included in the final export.

- **Paths:**  
  - `HOST_EXPORT_BASE_PATH`, `CONTAINER_EXPORT_BASE_PATH`, `EXPORT_SUBDIR`, `BASE_DIR`

For full details, please consult the export and ingest wrapper scripts in their respective directories.

---

## Adding a New Release

To add a new data release:
1. Create a new release directory (e.g., `dbTools/ingest/v0/r2/` and `dbTools/export/v0/r2/`).
2. Copy an existing wrapper script (from r1) into the new directory.
3. Update parameters such as:
   - `SOURCE` (e.g., change from "Dec2024" to "Feb2025")
   - `RELEASE_VALUE` (e.g., from "r1" to "r2")
   - Any new configuration (e.g., enable elevation by setting `ENABLE_ELEVATION=true` in ingest and `INCLUDE_ELEVATION_EXPORT=true` in export).
4. Run the ingestion/export processes using the new wrapper scripts.

## Release notes

Current versions:
- v0r0: June 2024 iNat data release
- v0r1: December 2024 iNat data release (adds anomaly_score column to observations table)
  - (in-place update) added elevation_meters column to observations tables
- v0r2: February 2025 iNat data release (built with elevation data)

---

## License

[Insert License Information Here]

---

## Final Notes

- **Documentation Updates:**  
  This README provides a high-level overview. For detailed configuration of the ingest and export pipelines, please refer to:
  - `dbTools/ingest/v0/INGEST.md` (in progress)  
  - `dbTools/export/v0/export.md`

- **Contributions:**  
  Contributions and suggestions are welcome. Please submit pull requests or open issues if you encounter problems or have ideas for enhancements.

---

Happy Ingesting and Exporting!
      </file>
      <file path="/home/caleb/repo/ibridaDB/docs/export.md">
# ibridaDB Export Reference (v1)

This document describes how to configure and run an **ibridaDB** export job using our current export pipeline. The export process is driven by a set of **environment variables** that control which observations are included, how they are filtered, and where the outputs are written. These variables are typically set in a release‐specific wrapper script (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`).

> **Note:** This document has been updated to include new functionality such as ancestor‐based filtering, partial rank wiping, the `in_region` flag with out‐of‐region observation handling, and an optional inclusion of elevation data (via the `elevation_meters` column). See also the [Ingest Documentation](../ingest/INGEST.md) for details on how elevation data is integrated into the database.

---

## Table of Contents

- [ibridaDB Export Reference (v1)](#ibridadb-export-reference-v1)
  - [Table of Contents](#table-of-contents)
  - [Introduction \& Pipeline Overview](#introduction--pipeline-overview)
  - [Quick Start](#quick-start)
  - [Environment Variables](#environment-variables)
    - [Database Configuration](#database-configuration)
    - [Export Parameters](#export-parameters)
    - [Additional Flags and Advanced Settings](#additional-flags-and-advanced-settings)
    - [Paths](#paths)
  - [Export Flow \& Scripts](#export-flow--scripts)
  - [Output Files](#output-files)
  - [Future Work and Notes](#future-work-and-notes)

---

## Introduction & Pipeline Overview

The **ibridaDB Export Pipeline** extracts curated subsets of observations from a spatially enabled PostgreSQL/PostGIS database. It is designed to support advanced filtering based on:

- **Geographic Region:** Defined by a bounding box corresponding to a region tag (e.g., `"NAfull"`).
- **Species Observations:** A minimum number of research-grade observations per species (`MIN_OBS`) is required to be included.
- **Taxonomic Clade:** Filtering by a clade, metaclade, or macroclade is performed based on definitions in `clade_defns.sh`.
- **Quality & Ancestor Filtering:** The pipeline supports research-grade filtering (`RG_FILTER_MODE`), optional partial-rank wiping (using `MIN_OCCURRENCES_PER_RANK` and `INCLUDE_MINOR_RANKS_IN_ANCESTORS`), and computes an `in_region` boolean for each observation.
- **Elevation Data (Optional):** When enabled (via `INCLUDE_ELEVATION_EXPORT`), the final exported CSV will include an `elevation_meters` column immediately after `longitude`.

The export process follows these broad stages:

1. **Wrapper Script:** Sets all required environment variables.
2. **Main Export Script (`common/main.sh`):**
   - Validates variables and creates the export directory.
   - Ensures necessary PostgreSQL extensions and roles exist.
   - Calls **regional_base.sh** to generate region-based tables.
   - Invokes **cladistic.sh** to filter by clade and produce the final export table.
   - Generates a summary file with environment details and final statistics.
3. **Regional Base Generation (`common/regional_base.sh`):**
   - Sets the region's bounding box.
   - Creates tables that capture species meeting the `MIN_OBS` threshold and computes an `in_region` flag.
   - Builds ancestor tables based on clade definitions.
4. **Cladistic Filtering & CSV Export (`common/cladistic.sh`):**
   - Joins the regional base observations to taxonomic data from `expanded_taxa`.
   - Applies research-grade filters, partial-rank wiping, and random sampling (using `MAX_RN`) per species.
   - Exports a final CSV that explicitly lists observation and photo columns (including `elevation_meters` if enabled).

A high-level diagram of the export flow is shown below:

```mermaid
flowchart TB
    A["Wrapper Script<br/>(e.g., r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh)"] --> B["Main Export Script<br/>(common/main.sh)"]
    B --> C{"Control Flags?"}
    C -- "SKIP_ALL_SP_TABLE=true" --> D["Reuse Existing Species Table"]
    C -- "SKIP_ALL_SP_TABLE=false" --> E["Create/Update Species Table"]
    C -- "SKIP_ANCESTORS_TABLE=true" --> F["Reuse Existing Ancestor Tables"]
    C -- "SKIP_ANCESTORS_TABLE=false" --> G["Create New Ancestor Tables"]
    D --> H["cladistic.sh<br/>(Filter Observations & Export CSV)"]
    E --> H
    F --> H
    G --> H
    H --> I["Export Summary & Log Generation"]
```

---

## Quick Start

1. **Clone or navigate** to the `dbTools/export/v0` directory.
2. **Configure a wrapper script** (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`) to set your export parameters. For example:
   - Set region-specific parameters: `REGION_TAG`, `MIN_OBS`, `MAX_RN`, and `PRIMARY_ONLY`.
   - Specify the clade filter: `CLADE` or `METACLADE`.
   - Enable out-of-region observation handling: `INCLUDE_OUT_OF_REGION_OBS`.
   - Optionally, enable elevation export: `INCLUDE_ELEVATION_EXPORT=true` (if your database includes elevation data).
3. **Run the wrapper script** to initiate the export process:
   ```bash
   ./r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
   ```
4. **Review the output**:
   - A CSV file named `<EXPORT_GROUP>_photos.csv` will be saved to the export directory.
   - A summary file (`<EXPORT_GROUP>_export_summary.txt`) will be generated containing export parameters, statistics, and timing information.

---

## Environment Variables

The export pipeline is configured via several environment variables, which are typically set in your wrapper script.

### Database Configuration

- **`DB_USER`**  
  *Description:* PostgreSQL user (e.g., `"postgres"`).

- **`VERSION_VALUE`**  
  *Description:* Database version identifier (e.g., `"v0"`).

- **`RELEASE_VALUE`**  
  *Description:* Data release identifier (e.g., `"r1"`).  
  *Note:* For older releases (e.g., `"r0"`), certain features (such as `anomaly_score` and `elevation_meters`) may be absent.

- **`ORIGIN_VALUE`**  
  *Description:* Data provenance (e.g., `"iNat-Dec2024"`).

- **`DB_NAME`**  
  *Description:* Name of the database (e.g., `"ibrida-v0-r1"`).

- **`DB_CONTAINER`**  
  *Description:* Name of the Docker container running PostgreSQL (e.g., `"ibridaDB"`).

### Export Parameters

- **`REGION_TAG`**  
  *Description:* A key that defines the region's bounding box (e.g., `"NAfull"`).  
  *Usage:* Used in `regional_base.sh` to set geographic coordinates.

- **`MIN_OBS`**  
  *Description:* Minimum number of research-grade observations required per species for inclusion.  
  *Default:* `50`.

- **`MAX_RN`**  
  *Description:* Maximum number of research-grade observations to sample per species in the final CSV.  
  *Default:* `2500` (or your desired value).

- **`PRIMARY_ONLY`**  
  *Description:* If `true`, only the primary photo (position=0) is included; if `false`, all photos are exported.

- **`CLADE` / `METACLADE` / `MACROCLADE`**  
  *Description:* Defines the taxonomic filter. For example, `CLADE="amphibia"` or `METACLADE="pta"` (primary terrestrial arthropods).

- **`EXPORT_GROUP`**  
  *Description:* A label for the final export; used to name the final observations table and CSV file (e.g., `"amphibia_all_exc_nonrg_sp_oor_elev"`).

### Additional Flags and Advanced Settings

- **`PROCESS_OTHER`**  
  *Description:* A generic flag for additional processing (default: `false`).

- **`SKIP_REGIONAL_BASE`**  
  *Description:* Master flag that controls whether to reuse existing regional base tables rather than recreating them.  
  *Note:* This acts as a default for the more granular flags below if they are not explicitly set.

- **`SKIP_ALL_SP_TABLE`**  
  *Description:* Controls whether to reuse or recreate the base species table. When running multiple exports for different clades against the same region with the same `MIN_OBS`, set this to `true` to reuse the common species table.  
  *Default:* Inherits value from `SKIP_REGIONAL_BASE` if not explicitly set.

- **`SKIP_ANCESTORS_TABLE`**  
  *Description:* Controls whether to reuse or recreate the clade-specific ancestor tables. These tables are unique to each clade, so typically this should be `false` when running exports for different clades.  
  *Default:* Inherits value from `SKIP_REGIONAL_BASE` if not explicitly set.

- **`INCLUDE_OUT_OF_REGION_OBS`**  
  *Description:* If `true`, once a species is selected by `MIN_OBS`, all observations for that species (globally) are included; otherwise, only those within the bounding box are used.  
  *Note:* An `in_region` boolean is computed for each observation.

- **`RG_FILTER_MODE`**  
  *Description:* Controls how research-grade versus non-research observations are filtered.  
  *Possible values:* `ONLY_RESEARCH`, `ALL`, `ALL_EXCLUDE_SPECIES_NON_RESEARCH`, `ONLY_NONRESEARCH`, etc.

- **`MIN_OCCURRENCES_PER_RANK`**  
  *Description:* Minimum occurrences required per rank (e.g., L20, L30, L40) before that rank is retained.  
  *Usage:* Used to optionally wipe out low-occurrence partial rank labels.

- **`INCLUDE_MINOR_RANKS_IN_ANCESTORS`**  
  *Description:* If `true`, includes minor ranks in the ancestor search; otherwise, only major ranks are considered.

- **`INCLUDE_ELEVATION_EXPORT`**  
  *Description:* If `true` (the default for new releases), the final export will include the `elevation_meters` column (placed immediately after `longitude`).  
  *Note:* If the underlying database is older (e.g., release `"r0"`), set this to `false`.

### Paths

- **`HOST_EXPORT_BASE_PATH`**  
  *Description:* Host filesystem path where export files will be written (e.g., `"/datasets/ibrida-data/exports"`).

- **`CONTAINER_EXPORT_BASE_PATH`**  
  *Description:* Container path corresponding to `HOST_EXPORT_BASE_PATH` (e.g., `"/exports"`).

- **`EXPORT_SUBDIR`**  
  *Description:* A subdirectory constructed from variables (e.g., `"v0/r1/primary_only_50min_2500max"`).

- **`BASE_DIR`**  
  *Description:* Root directory of the export tools (e.g., `/home/caleb/repo/ibridaDB/dbTools/export/v0`).

---

## Export Flow & Scripts

The export process consists of several key steps:

1. **Wrapper Script:**  
   - A release-specific wrapper (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`) sets all necessary environment variables (including the new `INCLUDE_ELEVATION_EXPORT` toggle) and then calls the main export script.

2. **Main Export Script (`common/main.sh`):**  
   - Validates required variables and creates the export directory.
   - Installs necessary PostgreSQL extensions (such as `dblink`) and creates roles if needed.
   - Invokes **regional_base.sh** to generate region-specific base tables.
   - Calls **cladistic.sh** to join base tables with the taxonomic hierarchy (from `expanded_taxa`), apply quality and clade filters, and build the final export table.
   - Generates a summary file that documents the export parameters, final observation counts, and timing information.
   - Optionally copies the wrapper script into the output directory for reproducibility.

3. **Regional Base Generation (`common/regional_base.sh`):**  
   - Determines the geographic bounding box using `REGION_TAG` (from `region_defns.sh`).
   - Creates or reuses the species table (`<REGION_TAG>_min${MIN_OBS}_all_sp`) based on `SKIP_ALL_SP_TABLE`.
   - Builds or reuses an ancestor table (`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors_<cladeID>_<mode>`) based on `SKIP_ANCESTORS_TABLE`.
   - Generates or reuses a second table (`<REGION_TAG>_min${MIN_OBS}_sp_and_ancestors_obs_<cladeID>_<mode>`) based on `SKIP_ANCESTORS_TABLE`.
   - The `INCLUDE_OUT_OF_REGION_OBS` flag governs whether the observation table is filtered by the bounding box or not.

4. **Cladistic Filtering & CSV Export (`common/cladistic.sh`):**  
   - Joins the observation table with `expanded_taxa` using clade conditions defined in `clade_defns.sh` (and processed by `clade_helpers.sh`).
   - Applies research-grade filtering based on `RG_FILTER_MODE` and uses a partition-based random sampling (controlled by `MAX_RN` and `PRIMARY_ONLY`).
   - Explicitly enumerates columns in the final export, including `elevation_meters` (if `INCLUDE_ELEVATION_EXPORT=true`), ensuring that the column appears immediately after `longitude`.
   - Exports the final dataset as a CSV file with a header and tab-delimited fields.
   - Debug SQL is executed to confirm the final column list used.

---

## Output Files

After a successful export, you will find:

- A CSV file named `<EXPORT_GROUP>_photos.csv` in the export subdirectory (e.g., `/exports/v0/r1/primary_only_50min_2500max`).
- A summary file named `<EXPORT_GROUP>_export_summary.txt` that documents:
  - The values of key environment variables.
  - Final observation, taxa, and observer counts.
  - Timing information for each stage of the export process.
- Optionally, a copy of the export wrapper script (if `WRAPPER_PATH` is set).

---

## Future Work and Notes

- **Enhanced Documentation:**  
  Future revisions will further detail the logic of ancestor searches and the random sampling strategy used for research-grade observations.
  
- **Additional Filtering Options:**  
  We plan to refine the `RG_FILTER_MODE` and allow further customizations (e.g., combining multiple quality filters).

- **Schema Updates:**  
  As new releases are introduced (e.g., additional columns beyond `anomaly_score` or `elevation_meters`), the export documentation will be updated to reflect the schema changes.

- **User Feedback:**  
  Contributions, bug reports, and suggestions for improvements are welcome.

---

Happy Exporting!
      </file>
      <file path="/home/caleb/repo/ibridaDB/docs/ingest.md">
# ibridaDB Ingestion Documentation

This document provides a detailed overview of the ibridaDB ingestion pipeline. It covers the steps to load iNaturalist data into a spatially enabled PostgreSQL/PostGIS database, how the elevation integration is handled, and how to run the ingestion process using the provided wrapper scripts.

---

## 1. Overview of the Ingestion Flow

The ingestion pipeline for ibridaDB is designed to:
- **Initialize the Database:** Create a new database using a template (typically a PostGIS-enabled template).
- **Import Data:** Load CSV data for observations, photos, taxa, and observers from iNaturalist data dumps.
- **Compute Geometries:** Generate geospatial geometry columns from the latitude and longitude fields.
- **Update Metadata:** Populate additional columns such as `origin`, `version`, and `release` on each table.
- **Integrate Elevation (Optional):**  
  - **Fresh Ingestion Scenario:** If you are initializing a new database and want to include elevation data, the main ingestion script (in `common/main.sh`) can trigger the elevation pipeline automatically when `ENABLE_ELEVATION=true` is set.
  - **Existing Database Scenario:** If your database already exists and lacks elevation data, you can run the elevation wrapper (in `utils/elevation/wrapper.sh`) to update the observations with DEM-derived elevation values.

After the ingestion steps are complete, the database is ready for further processing or for exporting subsets of observations using the export pipeline.

---

## 2. Environment Variables

The ingestion pipeline is controlled by several environment variables. The key ones include:

### Database and General Settings
- **`DB_USER`**  
  PostgreSQL user (typically `"postgres"`).

- **`DB_TEMPLATE`**  
  Template database name (e.g., `"template_postgis"`) used to create the new database.

- **`NUM_PROCESSES`**  
  Number of parallel processes to use for tasks such as geometry calculations and metadata updates.

- **`BASE_DIR`**  
  Root directory of the ingestion tools (e.g., `/home/caleb/repo/ibridaDB/dbTools/ingest/v0`).

- **`SOURCE`**  
  Source identifier for the iNaturalist data (e.g., `"Dec2024"`, `"Feb2025"`).

- **`METADATA_PATH`**  
  Path to the CSV files containing iNaturalist data (e.g., `/datasets/ibrida-data/intake/Dec2024`).

### Versioning and Release
- **`ORIGIN_VALUE`**  
  Describes the data provenance (e.g., `"iNat-Dec2024"`).

- **`VERSION_VALUE`**  
  Database version identifier (e.g., `"v0"`).

- **`RELEASE_VALUE`**  
  Data release identifier (e.g., `"r1"`).

- **`DB_NAME`**  
  Name of the new database (e.g., `"ibrida-v0-r1"`).

- **`DB_CONTAINER`**  
  Name of the Docker container running PostgreSQL (e.g., `"ibridaDB"`).

### Elevation Integration Settings
- **`ENABLE_ELEVATION`**  
  If set to `"true"`, the ingestion process will invoke the elevation pipeline.  
  - *Fresh Ingestion:* When creating a new database, the main ingestion script calls the elevation pipeline after geometry calculation.  
  - *Existing DB Update:* You may also run the elevation wrapper separately to add or update elevation values.
- **`DEM_DIR`**  
  Path to the directory containing the MERIT DEM `.tar` files (e.g., `"/datasets/dem/merit"`).
- **`EPSG`**  
  EPSG code for the DEM data (default: `"4326"`).
- **`TILE_SIZE`**  
  Tile size for processing DEM data (default: `"100x100"`).

---

## 3. Ingestion Flow Details

### A. Database Initialization and CSV Import

1. **Database Creation:**  
   - The ingestion pipeline starts by dropping any existing database with the target name and creating a fresh database using the specified template.
2. **Table Creation:**  
   - Tables are created using a provided structure SQL file (e.g., `r1/structure.sql`).
3. **Data Import:**  
   - The pipeline imports CSV files for observations, photos, taxa, and observers into the respective tables using PostgreSQL’s `COPY` command.
4. **Index Creation:**  
   - Key indexes are created to optimize spatial and text-based queries (including geospatial indexes on the geometry column).

### B. Geometry Calculation

- The script `common/geom.sh` is executed in parallel to compute the `geom` column on the `observations` table from the `latitude` and `longitude` fields.
- A PostGIS GIST index is then created on the new geometry column to speed up spatial queries.

### C. Metadata Update

- The script `common/vers_origin.sh` is run to add and populate the `origin`, `version`, and `release` columns on all tables.
- This process is executed in parallel across tables to speed up the update.

### D. Elevation Integration

The elevation pipeline can be integrated in one of two ways:

#### 1. During Fresh Ingestion
- **Integration via Main Ingestion Script:**  
  When the environment variable `ENABLE_ELEVATION` is set to `"true"`, the main ingestion script (`common/main.sh`) calls the elevation pipeline after geometry calculation. This pipeline performs the following steps:
  - **Create Elevation Table:**  
    The script `utils/elevation/create_elevation_table.sh` ensures that the `elevation_raster` table exists.
  - **Load DEM Data:**  
    The script `utils/elevation/load_dem.sh` extracts DEM tiles from `.tar` archives and loads them into the `elevation_raster` table using `raster2pgsql` (which requires the custom Docker image).
  - **Update Elevation Values:**  
    Finally, `utils/elevation/update_elevation.sh` updates the `observations.elevation_meters` column for each observation based on a spatial join with the DEM data.

#### 2. Updating an Existing Database
- **Separate Elevation Wrapper:**  
  If you have an existing database (e.g., from a previous release) and you need to add or update elevation values, you can run the elevation wrapper script located at `utils/elevation/wrapper.sh`. This script sets the appropriate environment variables and calls the elevation main script to update the database.

---

## 4. Example Wrapper Usage

Below is an example wrapper script for a fresh ingestion (with elevation enabled):

```bash
#!/bin/bash
# Example: dbTools/ingest/v0/r1/wrapper.sh

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/wrapper_$(date +%Y%m%d_%H%M%S).log"
echo "Starting ingestion at $(date)" > "${LOG_FILE}"

# Export environment variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"
export SOURCE="Feb2025"
export METADATA_PATH="/datasets/ibrida-data/intake/Feb2025"
export ORIGIN_VALUE="iNat-Feb2025"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r2"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r2/structure.sql"

# Enable elevation integration in this ingestion
export ENABLE_ELEVATION=true
export DEM_DIR="/datasets/dem/merit"
export EPSG="4326"
export TILE_SIZE="100x100"

# Execute the main ingestion script
/home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/main.sh

# End of wrapper
echo "Ingestion process complete at $(date)" >> "${LOG_FILE}"
```

For an existing database update, simply run the elevation wrapper:

```bash
#!/bin/bash
# Example: dbTools/ingest/v0/utils/elevation/wrapper.sh

export DB_NAME="ibrida-v0-r1"
export DB_USER="postgres"
export DB_CONTAINER="ibridaDB"
export DEM_DIR="/datasets/dem/merit"
export NUM_PROCESSES=16
export EPSG="4326"
export TILE_SIZE="100x100"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

./utils/elevation/main.sh "$DB_NAME" "$DB_USER" "$DB_CONTAINER" "$DEM_DIR" "$NUM_PROCESSES" "$EPSG" "$TILE_SIZE"
```

---

## 5. Verifying Success

After running the ingestion process, verify that:

- The database is created with all required tables and indexes.
- The CSV files have been successfully imported into the tables.
- The `geom` column on `observations` is correctly computed and indexed.
- If elevation was enabled, the `elevation_raster` table exists and the `observations.elevation_meters` column is populated (for rows with valid DEM coverage).
- Log messages and notifications (if configured) confirm each step’s completion.

---

## Final Notes

- This document covers the ingestion side of ibridaDB. For exporting subsets of the data, please see [export.md](../export/export.md) for detailed instructions on the export pipeline.
- Ensure that you use the custom Docker image (with `raster2pgsql` installed) when running ingestion with elevation data.
- For new releases, adjust your wrapper scripts as needed and consider setting up separate directories (e.g., r1, r2) to maintain versioning consistency.

Happy Ingesting!
      </file>
      <file path="/home/caleb/repo/ibridaDB/docs/roadmap.md">

**positional_accuracy** usage
- Thought: perhaps 'smudge' the elevation_meters value; weighted average of elevation tiles within the circle whose radius is the positional accuracy.
      </file>
      <file path="/home/caleb/repo/ibridaDB/docs/schemas.md">
# ibridaDB Schemas Reference

This document provides a detailed reference for all of the schemas used in ibridaDB. It covers:

1. The core tables imported directly from iNaturalist.
2. The **expanded_taxa** table generated from the iNaturalist `taxa` table.
3. The final export table produced by the export pipeline.
4. Supplementary information on data types, indexing, and version-specific differences.
5. An appendix with sample SQL output (\d results) for quick reference.

This reference is intended to help developers and maintainers quickly understand the structure, data types, and intended usage of each table without needing to manually inspect the database.

---

## Table of Contents

1. [Core iNaturalist Tables](#core-inaturalist-tables)  
   1.1. [Observations](#observations)  
   1.2. [Photos](#photos)  
   1.3. [Observers](#observers)  
   1.4. [Taxa](#taxa)

2. [Expanded Taxa Table](#expanded-taxa-table)  
   2.1. [Purpose and Generation](#purpose-and-generation)  
   2.2. [Schema Details](#schema-details)  
   2.3. [Indexing and Performance Considerations](#indexing-and-performance-considerations)  
   2.4. [Rank-Level Mapping](#rank-level-mapping)

3. [Final Export Table Schema](#final-export-table-schema)  
   3.1. [Overview](#overview)  
   3.2. [Explicit Column List and Descriptions](#explicit-column-list-and-descriptions)  
   3.3. [Conditional Columns: elevation_meters and anomaly_score](#conditional-columns-elevation_meters-and-anomaly_score)  
   3.4. [Example Row / CSV Layout](#example-row--csv-layout)

4. [Supplementary Information](#supplementary-information)  
   4.1. [Data Types and Precision](#data-types-and-precision)  
   4.2. [Indices and Their Purposes](#indices-and-their-purposes)  
   4.3. [Version-Specific Schema Differences](#version-specific-schema-differences)

5. [Appendix: SQL Dumps and \d Outputs](#appendix-sql-dumps-and-d-output)

---

## 1. Core iNaturalist Tables

These tables are imported directly from iNaturalist open data dumps.

### Observations

**Description:**  
Contains each observation record with geospatial and temporal data.

**Key Columns:**

| Column               | Type              | Description |
|----------------------|-------------------|-------------|
| observation_uuid     | uuid              | Unique identifier for each observation. This identifier is used to link photos and can be found on iNaturalist.org. |
| observer_id          | integer           | Identifier for the observer who recorded the observation. |
| latitude             | numeric(15,10)    | Latitude of the observation. High precision (up to 10 digits after the decimal) ensures accuracy. |
| longitude            | numeric(15,10)    | Longitude of the observation. |
| positional_accuracy  | integer           | Uncertainty in meters for the location. |
| taxon_id             | integer           | Identifier linking the observation to a taxon. |
| quality_grade        | varchar(255)      | Observation quality, e.g., "research", "casual", or "needs_id". |
| observed_on          | date              | Date when the observation was made. |
| anomaly_score        | numeric(15,6)     | A computed metric for anomaly detection; available in releases r1 and later. |
| geom                 | geometry          | Geospatial column computed from latitude and longitude. |
| origin               | varchar(255)      | Metadata field populated during ingestion. |
| version              | varchar(255)      | Database structure version. |
| release              | varchar(255)      | Data release identifier. |
| **elevation_meters** | **numeric(10,2)** | *Optional:* Elevation value in meters (if elevation processing is enabled). |

### Photos

**Description:**  
Contains metadata for photos associated with observations.

**Key Columns:**

| Column           | Type           | Description |
|------------------|----------------|-------------|
| photo_uuid       | uuid           | Unique identifier for each photo. |
| photo_id         | integer        | iNaturalist photo ID. |
| observation_uuid | uuid           | Identifier linking the photo to an observation. |
| observer_id      | integer        | Identifier of the observer who took the photo. |
| extension        | varchar(5)     | Image file format (e.g., "jpeg"). |
| license          | varchar(255)   | Licensing information (e.g., Creative Commons). |
| width            | smallint       | Photo width in pixels. |
| height           | smallint       | Photo height in pixels. |
| position         | smallint       | Indicates the order of photos for an observation (position 0 indicates primary photo). |
| origin           | varchar(255)   | Metadata field. |
| version          | varchar(255)   | Database structure version. |
| release          | varchar(255)   | Data release identifier. |

### Observers

**Description:**  
Contains information about the users (observers) who record observations.

**Key Columns:**

| Column      | Type         | Description |
|-------------|--------------|-------------|
| observer_id | integer      | Unique identifier for each observer. |
| login       | varchar(255) | Unique login/username. |
| name        | varchar(255) | Observer's personal name (if provided). |
| origin      | varchar(255) | Metadata field. |
| version     | varchar(255) | Database structure version. |
| release     | varchar(255) | Data release identifier. |

### Taxa

**Description:**  
Contains the taxonomy as provided by iNaturalist.

**Key Columns:**

| Column     | Type              | Description |
|------------|-------------------|-------------|
| taxon_id   | integer           | Unique taxon identifier. |
| ancestry   | varchar(255)      | Encoded ancestral hierarchy (delimited by backslashes). |
| rank_level | double precision  | Numeric level indicating taxonomic rank. |
| rank       | varchar(255)      | Taxonomic rank (e.g., "species", "genus"). |
| name       | varchar(255)      | Scientific name of the taxon. |
| active     | boolean           | Indicates if the taxon is active in the taxonomy. |
| origin     | varchar(255)      | Metadata field. |
| version    | varchar(255)      | Database structure version. |
| release    | varchar(255)      | Data release identifier. |

---

## 2. Expanded Taxa Table

### Purpose and Generation

The **expanded_taxa** table is generated from the iNaturalist `taxa` table by the `expand_taxa.sh` script. Its purpose is to “unpack” the single-column ancestry string into discrete columns (e.g., `L5_taxonID`, `L5_name`, `L5_commonName`, etc.) so that clade-based filtering and ancestor lookups can be performed efficiently without resorting to recursive string parsing.

### Schema Details

**Core Columns:**

| Column      | Type              | Description |
|-------------|-------------------|-------------|
| taxonID     | integer           | Primary key; unique taxon identifier. |
| rankLevel   | double precision  | Numeric indicator of the taxonomic rank. |
| rank        | varchar(255)      | Taxonomic rank label. |
| name        | varchar(255)      | Scientific name of the taxon. |
| commonName        | varchar(255)      | Common name of the taxon. | # NEW, TODO determine how to deal with multiple common names for a single taxa
| taxonActive | boolean           | Indicates whether the taxon is active. |

**Expanded Columns:**

For each rank level in the set `{5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30, 32, 33, 33.5, 34, 34.5, 35, 37, 40, 43, 44, 45, 47, 50, 53, 57, 60, 67, 70}`, the following columns are added:

- `L{level}_taxonID` (integer)
- `L{level}_name` (varchar(255))
- `L{level}_commonName` (varchar(255))

For example, for rank level 10:
- `L10_taxonID`
- `L10_name`
- `L10_commonName`

### Indexing and Performance Considerations

Indexes are created on the most frequently queried expanded columns (typically on `L10_taxonID`, `L20_taxonID`, …, `L70_taxonID`) as well as on the base columns (`taxonID`, `rankLevel`, and `name`). These indexes help to optimize the clade filtering and ancestor lookups performed by the export pipeline.

### Rank-Level Mapping

A supplemental mapping (provided in `clade_helpers.sh`) maps the column prefixes to human-readable rank names. For example:

| Prefix | Rank         |
|--------|--------------|
| L5     | subspecies   |
| L10    | species      |
| L20    | genus        |
| L40    | order        |
| L50    | class        |
| L70    | kingdom      |

A complete mapping is maintained in the code to facilitate any dynamic filtering or display of taxonomic information.

---

## 3. Final Export Table Schema

### Overview

The final export table is generated by the export pipeline (primarily via `cladistic.sh`) and is used for downstream applications such as training specimen identification models. This table is created by joining observations with photo metadata and taxonomic data from `expanded_taxa`. It includes additional computed columns for quality filtering and sampling.

### Explicit Column List and Descriptions

The final export table (named `<EXPORT_GROUP>_observations`) contains the following columns:

#### From the Observations Table

| Column               | Type              | Description |
|----------------------|-------------------|-------------|
| observation_uuid     | uuid              | Unique observation identifier. |
| observer_id          | integer           | Observer identifier. |
| latitude             | numeric(15,10)    | Latitude of the observation. |
| longitude            | numeric(15,10)    | Longitude of the observation. |
| **elevation_meters** | **numeric(10,2)** | *Optional:* Elevation in meters (included if `INCLUDE_ELEVATION_EXPORT=true`). |
| positional_accuracy  | integer           | Location uncertainty in meters. |
| taxon_id             | integer           | Identifier linking to the taxon. |
| quality_grade        | varchar(255)      | Quality grade (e.g., "research"). |
| observed_on          | date              | Date of observation. |
| anomaly_score        | numeric(15,6)     | Anomaly score (only available in r1 and later). |
| in_region            | boolean           | Computed flag indicating if the observation lies within the region bounding box. |
| expanded_taxonID     | integer           | Taxon ID from the expanded_taxa table. |
| expanded_rankLevel   | double precision  | Rank level from expanded_taxa. |
| expanded_name        | varchar(255)      | Taxon name from expanded_taxa. |
| L5_taxonID – L70_taxonID | integer       | A series of columns representing the taxonomic ancestry at various rank levels (e.g., L5_taxonID, L10_taxonID, …, L70_taxonID). |

#### From the Photos Table

| Column       | Type           | Description |
|--------------|----------------|-------------|
| photo_uuid   | uuid           | Unique photo identifier. |
| photo_id     | integer        | Photo identifier (from iNaturalist). |
| extension    | varchar(5)     | Image file format (e.g., "jpeg"). |
| license      | varchar(255)   | Licensing information. |
| width        | smallint       | Photo width (in pixels). |
| height       | smallint       | Photo height (in pixels). |
| position     | smallint       | Photo order indicator (primary photo has position 0). |

#### Additional Computed Column

| Column | Type    | Description |
|--------|---------|-------------|
| rn     | bigint  | Row number (per species partition based on `L10_taxonID`) used to cap the number of research-grade observations per species (controlled by `MAX_RN`). |

### Conditional Columns: elevation_meters and anomaly_score

- **elevation_meters:**  
  This column is included in the final export if the environment variable `INCLUDE_ELEVATION_EXPORT` is set to true and if the current release is not `"r0"`. It is positioned immediately after the `longitude` column.

- **anomaly_score:**  
  Present only in releases where it has been added (e.g., `r1` onward).

### Example Row / CSV Layout

An exported CSV row (tab-delimited) might be structured as follows:

```
observation_uuid    observer_id    latitude    longitude    elevation_meters    positional_accuracy    taxon_id    quality_grade    observed_on    anomaly_score    in_region    expanded_taxonID    expanded_rankLevel    expanded_name    L5_taxonID    L10_taxonID    ...    L70_taxonID    photo_uuid    photo_id    extension    license    width    height    position    rn
```

Each observation row is linked to one or more photo rows; the export process uses a partition-based random sampling (per species) so that only a maximum of `MAX_RN` research-grade observations per species are included.

---

## 4. Supplementary Information

### Data Types and Precision

- **Latitude and Longitude:** Stored as `numeric(15,10)`, which provides high precision (up to 10 digits after the decimal) ensuring accurate geolocation.
- **elevation_meters:** Stored as `numeric(10,2)`, capturing elevation with two decimal places.
- **anomaly_score:** Stored as `numeric(15,6)` for precise anomaly measurements.
- Standard PostgreSQL data types are used for other columns as specified.

### Indices and Their Purposes

- **Core Tables:**  
  Primary keys and indexes are created on identifiers (e.g., `observation_uuid`, `photo_uuid`, `taxon_id`) and frequently queried columns.
- **Observations Geometry:**  
  A GIST index is created on the `geom` column for fast spatial queries.
- **Expanded_Taxa:**  
  Additional indexes are created on key expanded ancestry columns (e.g., `L10_taxonID`, `L20_taxonID`, …, `L70_taxonID`) to optimize clade-based filtering.

### Version-Specific Schema Differences

- **Releases prior to r1:**  
  May not include `anomaly_score` and `elevation_meters`.
- **Current and Future Releases:**  
  Include these columns. Future schema changes will be documented here as needed.

---

## 5. Appendix: SQL Dumps and \d Outputs

Below are example outputs from PostgreSQL’s `\d` command for key tables. These serve as a quick reference for the column names and types.

### Observations Table

```sql
-- \d observations
       Column        |          Type          
---------------------+------------------------
 observation_uuid    | uuid                  
 observer_id         | integer               
 latitude            | numeric(15,10)        
 longitude           | numeric(15,10)        
 positional_accuracy | integer               
 taxon_id            | integer               
 quality_grade       | varchar(255)          
 observed_on         | date                  
 anomaly_score       | numeric(15,6)         
 geom                | geometry              
 origin              | varchar(255)          
 version             | varchar(255)          
 release             | varchar(255)
 elevation_meters    | numeric(10,2)         -- Present if enabled
```

### Photos Table

```sql
-- \d photos
      Column      |          Type          
------------------+------------------------
 photo_uuid       | uuid                   
 photo_id         | integer                
 observation_uuid | uuid                   
 observer_id      | integer                
 extension        | varchar(5)             
 license          | varchar(255)           
 width            | smallint               
 height           | smallint               
 position         | smallint               
 origin           | varchar(255)           
 version          | varchar(255)           
 release          | varchar(255)
```

### Observers Table

```sql
-- \d observers
   Column    |          Type          
-------------+------------------------
 observer_id | integer                
 login       | varchar(255)           
 name        | varchar(255)           
 origin      | varchar(255)           
 version     | varchar(255)           
 release     | varchar(255)
```

### Taxa Table

```sql
-- \d taxa
   Column   |          Type          
------------+------------------------
 taxon_id   | integer                
 ancestry   | varchar(255)           
 rank_level | double precision       
 rank       | varchar(255)           
 name       | varchar(255)           
 active     | boolean                
 origin     | varchar(255)           
 version    | varchar(255)           
 release    | varchar(255)
```

### Expanded_Taxa Table

```sql
-- \d "expanded_taxa"
      Column      |          Type          
------------------+------------------------
 taxonID          | integer    (PK)
 rankLevel        | double precision       
 rank             | varchar(255)           
 name             | varchar(255)           
 taxonActive      | boolean                
 L5_taxonID       | integer                
 L5_name          | varchar(255)           
 L5_commonName    | varchar(255)           
 L10_taxonID      | integer                
 L10_name         | varchar(255)           
 L10_commonName   | varchar(255)           
 ...              | ...                    
 L70_taxonID      | integer                
```

### Final Export Table (Example)

Assuming the export group is named `amphibia_all_exc_nonrg_sp_oor_elev`, an example output is:

```sql
-- \d "amphibia_all_exc_nonrg_sp_oor_elev_observations"
       Column              |          Type          
-----------------------------+------------------------
 observation_uuid            | uuid                  
 observer_id                 | integer               
 latitude                    | numeric(15,10)        
 longitude                   | numeric(15,10)        
 elevation_meters            | numeric(10,2)         -- Only if enabled
 positional_accuracy         | integer               
 taxon_id                    | integer               
 quality_grade               | varchar(255)          
 observed_on                 | date                  
 anomaly_score               | numeric(15,6)         -- Only for r1 and later
 in_region                   | boolean               
 expanded_taxonID            | integer               
 expanded_rankLevel          | double precision       
 expanded_name               | varchar(255)           
 L5_taxonID                  | integer               
 L10_taxonID                 | integer               
 ...                         | ...                   
 L70_taxonID                 | integer               
 photo_uuid                  | uuid                  
 photo_id                    | integer               
 extension                   | varchar(5)            
 license                     | varchar(255)          
 width                       | smallint              
 height                      | smallint              
 position                    | smallint              
 rn                          | bigint                -- For internal sampling
```

---

## Final Notes

- This document serves as the definitive reference for all table schemas within ibridaDB (other than intermediate tables).  
- It is essential for developers working on downstream processing, migration, or debugging tasks.  
- As the system evolves (new releases, additional columns, or modifications to processing logic), please update this document to maintain an accurate reference.
  - NOTE: 'regional base' tables are not documented here but quite likely should be. Necessary for debugging and understanding advances features like ancestor-aware (ancestor search), out-of-region (oor) observations of in-region taxa, etc.
      </file>
    </external_files>
  </files>
</codebase_context>