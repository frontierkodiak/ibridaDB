Directory tree, stemming from root "/home/caleb/repo/ibridaDB/dbTools":
├── FLOW.md (32 lines)
├── README.md (152)
└── schema.md (45)
----
----
Full Path: FLOW.md

```mermaid
flowchart TB

    subgraph Ingest["Database Initialization (ingest/)"]
        i_wrap["Ingest Wrapper<br/>(e.g. r0/wrapper.sh)"]
        i_main["Ingest Main<br/>(common/main.sh)"]
        i_other["Other Common Scripts"]
        db["(ibridaDB PostgreSQL)"]
        i_wrap --> i_main
        i_main --> i_other
        i_other --> db
    end

    subgraph Export["Data Export (export/)"]
        e_wrap["Export Wrapper<br/>(e.g. r1/my_wrapper.sh)"]
        e_main["Export Main<br/>(common/main.sh)"]
        rbase["regional_base.sh<br/>Species + Ancestors"]
        clad["cladistic.sh<br/>RG_FILTER_MODE + partial-labeled"]
        csv_out["CSV + Summary Files"]
        e_wrap --> e_main
        e_main --> rbase
        rbase --> clad
        clad --> csv_out
    end

    i_other --> db
    db --> e_wrap

    style Ingest fill:#f9f,stroke:#333,stroke-width:2px
    style Export fill:#bbf,stroke:#333,stroke-width:2px

```

----
Full Path: README.md

# ibrida Database Reproduction Guide

## Overview
This guide documents the end-to-end process for **reproducing** and **exporting** from the ibrida database, which is derived from iNaturalist open data dumps. The database uses a versioning system with two components:
- **Version (v#)**: Indicates structural changes (schema revisions) to the database.
- **Release (r#)**: Indicates distinct data releases from iNaturalist under the same schema version.

For example:
- **v0r0**: June 2024 iNat data release
- **v0r1**: December 2024 iNat data release (adds `anomaly_score` column to `observations`)

## System Architecture
The pipeline is split into two phases:
1. **Database Initialization** (`ingest/`)
2. **Data Export** (`export/`)

Each phase has:
- Common scripts for shared logic
- Release- or job-specific *wrapper scripts* that set environment variables for that particular run

## 1. Database Initialization (ingest/)
### Directory Structure

```
dbTools/ingest/v0/
├── common/
│   ├── geom.sh         # Geometry calculations
│   ├── vers_origin.sh  # Version/origin updates
│   └── main.sh         # Core ingestion logic
├── r0/
│   ├── wrapper.sh      # June 2024 release
│   └── structure.sql   # schema for r0
└── r1/
    ├── wrapper.sh      # December 2024 release
    └── structure.sql   # schema for r1 (adds anomaly_score)
```

### Running the Ingestion Process
1. **Make scripts executable**:
    ```bash
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/*.sh
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r0/wrapper.sh
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
    ```
2. **Run**:
    ```bash
    # For June 2024 (r0)
    /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r0/wrapper.sh

    # For December 2024 (r1)
    /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
    ```

## 2. Data Export (export/)
The export pipeline allows flexible subsetting of the DB by region, minimum threshold, clade, etc. For additional detail, see [export.md](export.md).

### Directory Structure
```
dbTools/export/v0/
├── common/
│   ├── main.sh            # Orchestrates creation or skipping of base tables; final summary
│   ├── regional_base.sh   # Region-based table creation, ancestor-aware logic
│   ├── cladistic.sh       # Taxonomic filtering, partial-rank wiping, CSV export
│   └── functions.sh       # Shared shell functions
├── r0/
│   └── wrapper.sh         # Example job wrapper for June 2024 export
├── r1/
│   └── wrapper.sh         # Example job wrapper for December 2024 export
└── export.md              # Detailed usage documentation (v1)
```

### Export Workflow
1. **User creates/edits a wrapper script** (e.g., `r1/my_special_wrapper.sh`) to set:
   - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`
   - Optional toggles like `INCLUDE_OUT_OF_REGION_OBS`, `RG_FILTER_MODE`, `ANCESTOR_ROOT_RANKLEVEL`, `MIN_OCCURRENCES_PER_RANK`
   - A unique `EXPORT_GROUP` name
2. **Run** that wrapper. The pipeline will:
   1. **(regional_base.sh)** Build base tables of in-threshold species + ancestors, optionally bounding to region or not, depending on `INCLUDE_OUT_OF_REGION_OBS`.
   2. **(cladistic.sh)** Filter final observations by clade or metaclade, optionally wipe partial ranks, and do a random-sample CSV export.
   3. **(main.sh)** Write a summary file enumerating environment variables, row counts, timing, etc.

3. **Check** `/datasets/ibrida-data/exports` for final CSV output (organized by `VERSION_VALUE` / `RELEASE_VALUE` / any job-specific subdirectory).

### Drafting a New Wrapper
It is **recommended** to create a separate wrapper script for each new export job. For instance:
```bash
#!/bin/bash

export WRAPPER_PATH="$0"

export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_test"

export INCLUDE_OUT_OF_REGION_OBS=false
export RG_FILTER_MODE="ALL"
export ANCESTOR_ROOT_RANKLEVEL=40
export MIN_OCCURRENCES_PER_RANK=30

# other optional vars, e.g. PROCESS_OTHER, SKIP_REGIONAL_BASE, etc.

export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/myamphibia_job"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

source "${BASE_DIR}/common/functions.sh"

/home/caleb/repo/ibridaDB/dbTools/export/v0/common/main.sh
```
Then `chmod +x` this file and run it to generate a new job.

### Example Outputs
The final CSV and summary are placed in a subdirectory (e.g. `v0/r1/myamphibia_job`). A typical summary file `amphibia_test_export_summary.txt` includes:
- Region: NAfull
- MIN_OBS: 50
- RG_FILTER_MODE: ALL
- Observations: 10,402
- Unique Taxa: 927
- Timings for each step

### Further Reading
- **[export.md](export/v0/export.md)** for a deeper parameter reference (v1).
- **clade_defns.sh** for built-in definitions of macroclades, clades, and metaclades.

## Overall Flow
Below is a schematic of the entire ingest→export pipeline. For details on the ingest side, see [Ingestion docs](#database-initialization-ingest):
```
Ingest (ingest/v0/) --> Database --> Export (export/v0/)
```
In the export sub-phase, each new wrapper script can define a distinct job. Summaries and CSVs are stored in `HOST_EXPORT_BASE_PATH` for easy retrieval and analysis.

## Notes on Schema
- **v0r1** adds the `anomaly_score numeric(15,6)` column to `observations`.
- The export scripts automatically check if that column is present based on `RELEASE_VALUE`.
- If partial-labeled data is desired (coarse ranks for rare species), see the advanced features in `regional_base.sh` (ancestor logic) and `cladistic.sh` (partial-rank wiping logic).

**Notes**:
- The ingest side is unchanged for v0→v0r1 except for adding columns and data updates.
- The export side is significantly more flexible now, supporting ancestor‐aware logic and partial-labeled data.  
- Each new export job typically has its own wrapper script referencing the relevant `VERSION_VALUE`, `RELEASE_VALUE`, region, and clade parameters.

----
Full Path: schema.md

```markdown
### Observations
Column | Description
-------|------------
observation_uuid | A unique identifier associated with each observation also available at iNaturalist.org via URLs constructed like this https://www.inaturalist.org/observations/c075c500-b566-44aa-847c-95da8fb8b3c9
observer_id | The identifier of the associated iNaturalist user who recorded the observation
latitude | The latitude where the organism was encountered
longitude | The longitude where the organism was encountered
positional_accuracy | The uncertainty in meters around the latitude and longitude
taxon_id | The identifier of the associated axon the observation has been identified as
quality_grade | `Casual` observations are missing certain data components (e.g. latitude) or may have flags associated with them not shown here (e.g. `location appears incorrect`). Observations flagged as not wild are also considered Casual. All other observations are either `Needs ID` or `Research Grade`. Generally, Research Grade observations have more than one agreeing identifications at the species level, or if there are disagreements at least ⅔ of the identifications are in agreement a the species level
observed_on | The date at which the observation took place
<NOTE> New column added in v0/r1 'anomaly_score' </NOTE>

### Observers
Column | Description
-------|------------
observer_id | A unique identifier associated with each observer also available on https://www.inaturalist.org via URLs constructed like this: https://www.inaturalist.org/users/1
login | A unique login associated with each observer
name | Personal name of the observer, if provided

### Photos
Column | Description
-------|------------
photo_uuid | A unique identifier associated with each photo. Note that photo_uuid can be non-unique across different observations.
photo_id | A photo identifier used on iNaturalist and available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/photos/113756411
observation_uuid | The identifier of the associated observation
observer_id | The identifier of the associated observer who took the photo
extension | The image file format, e.g. `jpeg`
license | All photos in the dataset have open licenses (e.g. Creative Commons) and unlicensed (CC0 / public domain)
width | The width of the photo in pixels
height | The height of the photo in pixels
position | When observations have multiple photos the user can set the position in which the photos should appear. Lower numbers are meant to appear first
>The issue is that some observations include more than one photo, and photos associated with observations that have >1 photo share a photo_id and photo_uuid, which I did not expect. These additional photos (which have their own rows in the 'photos' table) are denoted by the 'position' field, where position ==0 indicates that the photo is the primary photo for the record. If an observation only has one photo, then the associated 'photos' record will have position == 0. Therefore. I'm pretty sure that a composite key of photo_id ++ photo_uuid ++ position will function as a primary key. 

### Taxa
Column | Description
-------|------------
taxon_id | A unique identifier associated with each node in the iNaturalist taxonomy hierarchy. Also available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/taxa/47219
ancestry | The taxon_ids of ancestry of the taxon ordered from the root of the tree to the taxon concatenated together with `\`
rank_level | A number associated with the rank. Taxon rank_levels must be less than the rank level of their parent. For example, a taxon with rank genus and rank_level 20 cannot descend from a taxon of rank species and rank_level 10
rank | A constrained set of labels associated with nodes on the hierarchy. These include the standard Linnaean ranks: Kingdom, Phylum, Class, Order, Family, Genus, Species, and a number of internodes such as Subfamily
name | The scientific name for the taxon
active | When the taxonomy changes, generally taxa aren’t deleted on iNaturalist to avoid breaking links. Instead taxa are made inactive and observations are moved to new active nodes. Occasionally, observations linger on inactive taxa which are no longer active parts of the iNaturalist taxonomy
```

----
Full Path: ingest/v0/readme.md

```markdown
dbTools/ingest/v0/
├── common/
│   ├── geom.sh           # Geometry calculations
│   ├── vers_origin.sh    # Version/origin updates
│   └── main.sh           # Core ingestion logic
├── r0/
│   └── wrapper.sh        # r0-specific parameters
└── r1/
    └── wrapper.sh        # r1-specific parameters
```

# ibrida Database Reproduction Guide

## Overview
This guide documents the step-by-step process for reproducing the ibrida database from iNaturalist open data dumps. The database uses a versioning system with two components:
- **Version (v#)**: Indicates structural changes to the database
- **Release (r#)**: Indicates different data dumps using the same structure

Current versions:
- v0r0: June 2024 iNat data release
- v0r1: December 2024 iNat data release (adds anomaly_score column to observations table)

## Directory Structure
```
dbTools/ingest/v0/
├── common/                # Shared scripts
│   ├── geom.sh           # Geometry calculations
│   ├── vers_origin.sh    # Version/origin updates
│   └── main.sh           # Core ingestion logic
├── r0/
│   └── wrapper.sh        # June 2024 release parameters
└── r1/
    └── wrapper.sh        # December 2024 release parameters
```

## Database Initialization and Data Ingestion
The initialization and ingestion process uses a modular system with wrapper scripts for version-specific parameters and common scripts for shared logic.

### Setup Release-Specific Parameters
Each release has its own wrapper script that defines:
- Database name (e.g., `ibrida-v0r1`)
- Source information
- Version and release values
- Input/output paths

### Running the Ingestion Process
```bash
# Make scripts executable
chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/main.sh
chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/geom.sh
chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/vers_origin.sh
chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh

# Run ingest process for latest release
/home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
```

### Ingestion Process Steps
1. `wrapper.sh` sets release-specific parameters
2. `main.sh` executes the core ingestion logic:
   - Creates the database
   - Sets up tables and indexes
   - Imports data from CSV files
   - Adds and calculates geometry columns (via `geom.sh`)
   - Sets version, release, and origin information (via `vers_origin.sh`)
3. Parallel processing is used for geometry calculations and metadata updates

### Database Schema
Each table includes these metadata columns:
- `version`: Database structure version (e.g., "v0")
- `release`: Data release identifier (e.g., "r0", "r1")
- `origin`: Source and date of the data (e.g., "iNat-Dec2024")

### Important Indices
Core indices:
- Primary key indices on all tables
- Geospatial index on observations (`observations_geom`)
- Foreign key indices for joins
- Full-text search indices for metadata columns
- Composite index for version/release queries (`idx_obs_version_release`)

## Adding a New Release
To add a new release:

1. Create a new release directory and wrapper script:
```bash
mkdir -p /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r{N}
cp /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r{N}/
```

2. Update parameters in the new wrapper script:
- SOURCE
- RELEASE_VALUE
- Other release-specific paths/values

3. Run the ingestion process as described above

## Export Process
[To be added as we implement the export steps...]

----
Full Path: ingest/v0/r0/ingest.sh

#!/bin/bash
### DEV: DEPRECATED, REMOVE ONCE WRAPPER/MAIN IS TESTED/VERIFIED
### DEV: Reference for functionality to port to new modularized system

# Database and user variables
DB_USER="postgres"
DB_TEMPLATE="template_postgis"
NUM_PROCESSES=16
BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

# Source variable
SOURCE="June2024"

# Construct origin value based on source
ORIGIN_VALUE="iNat-${SOURCE}"

# Version variable
VERSION_VALUE="v0"

# Construct database name
DB_NAME="ibrida-${VERSION_VALUE}"

# Function to execute SQL commands
execute_sql() {
  local sql="$1"
  docker exec ibrida psql -U "$DB_USER" -d "$DB_NAME" -c "$sql"
}

# Function to print progress
print_progress() {
  local message="$1"
  echo "======================================"
  echo "$message"
  echo "======================================"
}

# Create database, drop if exists
print_progress "Creating database"
docker exec ibrida psql -U "$DB_USER" -c "DROP DATABASE IF EXISTS \"$DB_NAME\";"
docker exec ibrida psql -U "$DB_USER" -c "CREATE DATABASE \"$DB_NAME\" WITH TEMPLATE $DB_TEMPLATE OWNER $DB_USER;"

# Connect to the database and create tables
print_progress "Creating tables"
execute_sql "
BEGIN;

CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

COMMIT;
"

# Import data
print_progress "Importing data"
execute_sql "
BEGIN;

COPY observations FROM '/metadata/${SOURCE}/observations.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY photos FROM '/metadata/${SOURCE}/photos.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY taxa FROM '/metadata/${SOURCE}/taxa.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY observers FROM '/metadata/${SOURCE}/observers.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;

COMMIT;
"

# Create indexes
print_progress "Creating indexes"
execute_sql "
BEGIN;

CREATE INDEX index_photos_photo_uuid ON photos USING btree (photo_uuid);
CREATE INDEX index_photos_observation_uuid ON photos USING btree (observation_uuid);
CREATE INDEX index_photos_position ON photos USING btree (position);
CREATE INDEX index_photos_photo_id ON photos USING btree (photo_id);
CREATE INDEX index_taxa_taxon_id ON taxa USING btree (taxon_id);
CREATE INDEX index_observers_observers_id ON observers USING btree (observer_id);
CREATE INDEX index_observations_observer_id ON observations USING btree (observer_id);
CREATE INDEX index_observations_quality ON observations USING btree (quality_grade);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);
CREATE INDEX index_taxa_active ON taxa USING btree (active);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);

COMMIT;
"

# Add geom column (parallelized calculation using geom.sh)
print_progress "Adding geom column"
execute_sql "ALTER TABLE observations ADD COLUMN geom public.geometry;"

# Run parallel geom calculations
print_progress "Running parallel geom calculations"
"${BASE_DIR}/geom.sh" "$DB_NAME" "observations" "$NUM_PROCESSES" "$BASE_DIR"

# Create geom index
print_progress "Creating geom index"
execute_sql "
BEGIN;

CREATE INDEX observations_geom ON observations USING GIST (geom);

COMMIT;
"

# Vacuum analyze
print_progress "Vacuum analyze"
execute_sql "VACUUM ANALYZE;"

# Add origin and version columns in parallel
print_progress "Adding origin and version columns"
execute_sql "
BEGIN;

ALTER TABLE taxa ADD COLUMN origin VARCHAR(255);
ALTER TABLE observers ADD COLUMN origin VARCHAR(255);
ALTER TABLE observations ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN version VARCHAR(255);
ALTER TABLE observations ADD COLUMN version VARCHAR(255);
ALTER TABLE observers ADD COLUMN version VARCHAR(255);
ALTER TABLE taxa ADD COLUMN version VARCHAR(255);

COMMIT;
"

# Run parallel updates for origin and version columns
print_progress "Running parallel updates for origin and version columns"
"${BASE_DIR}/vers_origin.sh" "$DB_NAME" "$NUM_PROCESSES" "$ORIGIN_VALUE" "$VERSION_VALUE"

# Create indexes for origin and version columns
print_progress "Creating indexes for origin and version columns"
execute_sql "
BEGIN;

CREATE INDEX index_taxa_origins ON taxa USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_taxa_name ON taxa USING GIN (to_tsvector('simple', name));
CREATE INDEX index_observers_origins ON observers USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_observations_origins ON observations USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_origins ON photos USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_version ON photos USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observations_version ON observations USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observers_version ON observers USING GIN (to_tsvector('simple', version));
CREATE INDEX index_taxa_version ON taxa USING GIN (to_tsvector('simple', version));

COMMIT;
"

print_progress "Database setup complete"


----
Full Path: ingest/v0/r0/wrapper.sh

#!/bin/bash

# Database and user variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

# Source variable
export SOURCE="June2024"
export METADATA_PATH="/metadata/${SOURCE}"

# Version and origin values
export ORIGIN_VALUE="iNat-${SOURCE}"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r0"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r0/structure.sql"

# Execute main script
"${BASE_DIR}/common/main.sh"    

----
Full Path: ingest/v0/common/main.sh

#!/bin/bash

# This script expects the following variables to be set by the wrapper:
# - DB_USER
# - DB_TEMPLATE
# - NUM_PROCESSES
# - BASE_DIR
# - SOURCE
# - ORIGIN_VALUE
# - VERSION_VALUE
# - RELEASE_VALUE
# - DB_NAME
# - DB_CONTAINER
# - METADATA_PATH
# - STRUCTURE_SQL

# Validate required variables
required_vars=(
    "DB_USER" "DB_TEMPLATE" "NUM_PROCESSES" "BASE_DIR" 
    "SOURCE" "ORIGIN_VALUE" "VERSION_VALUE" "DB_NAME" 
    "DB_CONTAINER" "METADATA_PATH" "STRUCTURE_SQL"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to execute SQL commands on default postgres database
execute_sql_postgres() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

# Create database
print_progress "Creating database"
execute_sql_postgres "DROP DATABASE IF EXISTS \"$DB_NAME\";"
execute_sql_postgres "CREATE DATABASE \"$DB_NAME\" WITH TEMPLATE $DB_TEMPLATE OWNER $DB_USER;"

# Create tables from structure file
print_progress "Creating tables"
cat "${STRUCTURE_SQL}" | docker exec -i ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}"

# Import data
print_progress "Importing data"
execute_sql "
BEGIN;

COPY observations FROM '${METADATA_PATH}/observations.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY photos FROM '${METADATA_PATH}/photos.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY taxa FROM '${METADATA_PATH}/taxa.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY observers FROM '${METADATA_PATH}/observers.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;

COMMIT;
"

# Create indexes
print_progress "Creating indexes"
execute_sql "
BEGIN;

CREATE INDEX index_photos_photo_uuid ON photos USING btree (photo_uuid);
CREATE INDEX index_photos_observation_uuid ON photos USING btree (observation_uuid);
CREATE INDEX index_photos_position ON photos USING btree (position);
CREATE INDEX index_photos_photo_id ON photos USING btree (photo_id);
CREATE INDEX index_taxa_taxon_id ON taxa USING btree (taxon_id);
CREATE INDEX index_observers_observers_id ON observers USING btree (observer_id);
CREATE INDEX index_observations_observer_id ON observations USING btree (observer_id);
CREATE INDEX index_observations_quality ON observations USING btree (quality_grade);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);
CREATE INDEX index_taxa_active ON taxa USING btree (active);

COMMIT;
"

# Create conditional index for anomaly_score if it exists
execute_sql "
DO \$\$
BEGIN
    IF EXISTS (
        SELECT 1 
        FROM information_schema.columns 
        WHERE table_name = 'observations' 
        AND column_name = 'anomaly_score'
    ) THEN
        CREATE INDEX idx_observations_anomaly ON observations (anomaly_score);
    END IF;
END \$\$;"

# Add geom column
print_progress "Adding geom column"
execute_sql "ALTER TABLE observations ADD COLUMN geom public.geometry;"

# Run parallel geom calculations
print_progress "Running parallel geom calculations"
"${BASE_DIR}/common/geom.sh" "$DB_NAME" "observations" "$NUM_PROCESSES" "$BASE_DIR"

# Create geom index
print_progress "Creating geom index"
execute_sql "
BEGIN;
CREATE INDEX observations_geom ON observations USING GIST (geom);
COMMIT;
"

# Vacuum analyze
print_progress "Vacuum analyze"
execute_sql "VACUUM ANALYZE;"

# Add origin, version, and release columns in parallel
print_progress "Adding origin, version, and release columns"
execute_sql "
BEGIN;

ALTER TABLE taxa ADD COLUMN origin VARCHAR(255);
ALTER TABLE observers ADD COLUMN origin VARCHAR(255);
ALTER TABLE observations ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN version VARCHAR(255);
ALTER TABLE observations ADD COLUMN version VARCHAR(255);
ALTER TABLE observers ADD COLUMN version VARCHAR(255);
ALTER TABLE taxa ADD COLUMN version VARCHAR(255);
ALTER TABLE photos ADD COLUMN release VARCHAR(255);
ALTER TABLE observations ADD COLUMN release VARCHAR(255);
ALTER TABLE observers ADD COLUMN release VARCHAR(255);
ALTER TABLE taxa ADD COLUMN release VARCHAR(255);

COMMIT;
"

# Run parallel updates for origin, version, and release columns
print_progress "Running parallel updates for origin, version, and release columns"
"${BASE_DIR}/common/vers_origin.sh" "$DB_NAME" "$NUM_PROCESSES" "$ORIGIN_VALUE" "$VERSION_VALUE" "$RELEASE_VALUE"

# Create indexes for origin, version, and release columns
print_progress "Creating indexes for origin, version, and release columns"
execute_sql "
BEGIN;

CREATE INDEX index_taxa_origins ON taxa USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_taxa_name ON taxa USING GIN (to_tsvector('simple', name));
CREATE INDEX index_observers_origins ON observers USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_observations_origins ON observations USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_origins ON photos USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_version ON photos USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observations_version ON observations USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observers_version ON observers USING GIN (to_tsvector('simple', version));
CREATE INDEX index_taxa_version ON taxa USING GIN (to_tsvector('simple', version));
CREATE INDEX index_photos_release ON photos USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observations_release ON observations USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observers_release ON observers USING GIN (to_tsvector('simple', release));
CREATE INDEX index_taxa_release ON taxa USING GIN (to_tsvector('simple', release));

COMMIT;
"

print_progress "Database setup complete"

----
Full Path: ingest/v0/common/vers_origin.sh

#!/bin/bash

# COMMENT: populates origin, version, and release columns in parallel
# COMMENT: these are the only columns on the base tables that are not populated by the ingest process

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/vers_origin_$(date +%Y%m%d_%H%M%S).log"
echo "Starting version/origin/release updates at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "${LOG_FILE}"
}

# Function for error logging and exit
error_exit() {
    log_message "ERROR: $1"
    exit 1
}

# Function to run the update in parallel
run_update() {
    local TABLE_NAME=$1
    local COLUMN_NAME=$2
    local VALUE=$3
    local OFFSET=$4
    local LIMIT=$5
    local DB_NAME=$6
    local DB_CONTAINER=$7
    local PROCESS_NUM=$8

    log_message "Process $PROCESS_NUM: Updating $TABLE_NAME.$COLUMN_NAME (offset: $OFFSET, limit: $LIMIT)"
    
    UPDATE_RESULT=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "
    UPDATE ${TABLE_NAME}
    SET ${COLUMN_NAME} = '${VALUE}'
    WHERE ctid IN (
        SELECT ctid
        FROM ${TABLE_NAME}
        ORDER BY ctid
        OFFSET ${OFFSET}
        LIMIT ${LIMIT}
    );")
    
    if [ $? -ne 0 ]; then
        error_exit "Failed to update ${TABLE_NAME}.${COLUMN_NAME} in process $PROCESS_NUM"
    fi
    
    log_message "Process $PROCESS_NUM: Completed update of $TABLE_NAME.$COLUMN_NAME"
}

# Validate arguments
if [ "$#" -ne 5 ]; then
    error_exit "Usage: $0 <database_name> <num_workers> <origin_value> <version_value> <release_value>"
fi

# Define arguments
DB_NAME=$1
NUM_PROCESSES=$2
ORIGIN_VALUE=$3
VERSION_VALUE=$4
RELEASE_VALUE=$5

# Validate NUM_PROCESSES is a positive integer
if ! [[ "$NUM_PROCESSES" =~ ^[1-9][0-9]*$ ]]; then
    error_exit "Number of workers must be a positive integer"
fi

# Use container name from environment or default
DB_CONTAINER=${DB_CONTAINER:-"ibridaDB"}

# Verify database exists
if ! docker exec ${DB_CONTAINER} psql -U postgres -lqt | cut -d \| -f 1 | grep -qw "${DB_NAME}"; then
    error_exit "Database ${DB_NAME} does not exist"
fi

# Tables and their columns to update
declare -A TABLES_COLUMNS
TABLES_COLUMNS=(
    ["taxa"]="origin,version,release"
    ["observers"]="origin,version,release"
    ["observations"]="origin,version,release"
    ["photos"]="origin,version,release"
)

# Function to update columns in parallel
update_columns_in_parallel() {
    local TABLE_NAME=$1
    local COLUMN_NAME=$2
    local VALUE=$3
    local TOTAL_ROWS

    # Verify table exists
    if ! docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -c "\d ${TABLE_NAME}" &>/dev/null; then
        error_exit "Table ${TABLE_NAME} does not exist in database ${DB_NAME}"
    }

    # Get total rows with error handling
    TOTAL_ROWS=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};" | tr -d ' ')
    if [ $? -ne 0 ] || ! [[ "$TOTAL_ROWS" =~ ^[0-9]+$ ]]; then
        error_exit "Failed to get row count for ${TABLE_NAME}"
    }

    log_message "Starting parallel update of ${TABLE_NAME}.${COLUMN_NAME} (${TOTAL_ROWS} total rows)"
    
    local BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES + 1))
    local PIDS=()

    for ((i=0; i<NUM_PROCESSES; i++)); do
        local OFFSET=$((i * BATCH_SIZE))
        run_update ${TABLE_NAME} ${COLUMN_NAME} ${VALUE} ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${DB_CONTAINER} $i &
        PIDS+=($!)
    done

    # Wait for all processes and check their exit status
    for pid in "${PIDS[@]}"; do
        if ! wait $pid; then
            error_exit "One of the parallel update processes failed"
        fi
    done
    
    log_message "Completed update of ${TABLE_NAME}.${COLUMN_NAME}"
}

# Main update process
log_message "Starting updates with parameters:"
log_message "Database: ${DB_NAME}"
log_message "Number of processes: ${NUM_PROCESSES}"
log_message "Origin value: ${ORIGIN_VALUE}"
log_message "Version value: ${VERSION_VALUE}"
log_message "Release value: ${RELEASE_VALUE}"

for TABLE_NAME in "${!TABLES_COLUMNS[@]}"; do
    log_message "Processing table: ${TABLE_NAME}"
    IFS=',' read -ra COLUMNS <<< "${TABLES_COLUMNS[$TABLE_NAME]}"
    for COLUMN in "${COLUMNS[@]}"; do
        case "$COLUMN" in
            "origin")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$ORIGIN_VALUE"
                ;;
            "version")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$VERSION_VALUE"
                ;;
            "release")
                update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$RELEASE_VALUE"
                ;;
        esac
    done
done

log_message "All updates completed successfully"

----
Full Path: ingest/v0/common/functions.sh

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

----
Full Path: ingest/v0/common/geom.sh

#!/bin/bash

# Function to run the update in parallel
run_update() {
  local OFFSET=$1
  local LIMIT=$2
  local DB_NAME=$3
  local TABLE_NAME=$4
  local DB_CONTAINER=$5

  docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -c "
  UPDATE ${TABLE_NAME}
  SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)::public.geometry
  WHERE observation_uuid IN (
    SELECT observation_uuid
    FROM ${TABLE_NAME}
    ORDER BY observation_uuid
    OFFSET ${OFFSET}
    LIMIT ${LIMIT}
  );"
}

# Check if correct number of arguments are provided
if [ "$#" -ne 4 ]; then
  echo "Usage: $0 <database_name> <table_name> <num_workers> <base_dir>"
  exit 1
fi

# Define arguments
DB_NAME=$1
TABLE_NAME=$2
NUM_PROCESSES=$3
BASE_DIR=$4

# Use container name from environment or default
DB_CONTAINER=${DB_CONTAINER:-"ibridaDB"}

# Calculate total rows and batch size
TOTAL_ROWS=$(docker exec ${DB_CONTAINER} psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};")
BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES))

# Run updates in parallel
for ((i=0; i<NUM_PROCESSES; i++)); do
  OFFSET=$((i * BATCH_SIZE))
  run_update ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${TABLE_NAME} ${DB_CONTAINER} &
done

# Wait for all processes to finish
wait
echo "All updates completed."

----
Full Path: ingest/v0/r1/structure.sql

-- Structure for v0r1 (December 2024 release)
-- Note: anomaly_score column added in r1, not present in r0

CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date,
    anomaly_score numeric(15,6)  -- New column in r1
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

-- Note: The following columns are added by our ingestion process:
-- All tables:
--   origin VARCHAR(255)
--   version VARCHAR(255)
--   release VARCHAR(255)
-- Observations table:
--   geom public.geometry

----
Full Path: ingest/v0/r1/wrapper.sh

#!/bin/bash

### REVIEW: Previous run didn't populate version/origin columns. We applied a fix to vers_origin.sh (argument mismatch) but watch logs carefully next run.

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing ingest process with configuration:"

# Database and user variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"
log_message "Database User: ${DB_USER}"
log_message "Template DB: ${DB_TEMPLATE}"
log_message "Parallel Processes: ${NUM_PROCESSES}"

# Source variable
export SOURCE="Dec2024"
export METADATA_PATH="/metadata/${SOURCE}"
log_message "Source: ${SOURCE}"
log_message "Metadata Path: ${METADATA_PATH}"

# Version and origin values
export ORIGIN_VALUE="iNat-${SOURCE}"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r1/structure.sql"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"
log_message "Origin: ${ORIGIN_VALUE}"
log_message "Structure SQL: ${STRUCTURE_SQL}"

# Execute main script
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"

----
Full Path: ingest/v0/utils/add_release.sh

#!/bin/bash

# Database variables
DB_USER="postgres"
DB_NAME="ibrida-v0"  # The existing database name
DB_CONTAINER="ibridaDB"
RELEASE_VALUE="r0"

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

# Add release column to all tables
print_progress "Adding release column to tables"
execute_sql "
BEGIN;
ALTER TABLE taxa ADD COLUMN release VARCHAR(255);
ALTER TABLE observers ADD COLUMN release VARCHAR(255);
ALTER TABLE observations ADD COLUMN release VARCHAR(255);
ALTER TABLE photos ADD COLUMN release VARCHAR(255);
COMMIT;
"

# Set release values
print_progress "Setting release values"
execute_sql "
BEGIN;
UPDATE taxa SET release = '${RELEASE_VALUE}';
UPDATE observers SET release = '${RELEASE_VALUE}';
UPDATE observations SET release = '${RELEASE_VALUE}';
UPDATE photos SET release = '${RELEASE_VALUE}';
COMMIT;
"

# Create indexes for release column
print_progress "Creating indexes for release column"
execute_sql "
BEGIN;
CREATE INDEX index_taxa_release ON taxa USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observers_release ON observers USING GIN (to_tsvector('simple', release));
CREATE INDEX index_observations_release ON observations USING GIN (to_tsvector('simple', release));
CREATE INDEX index_photos_release ON photos USING GIN (to_tsvector('simple', release));
COMMIT;
"

print_progress "Release column added and populated successfully"

----
Full Path: export/v0/export.md

# ibridaDB Export Reference (v1)

This document describes how to configure and run an **ibridaDB** export job using our **v1** pipeline. The pipeline is driven by a set of **environment variables** that control which data are included, how they are filtered, and where the outputs are written. These variables are typically set in a **wrapper script** (e.g., `r1/wrapper.sh`).

Below is an overview of:

- [ibridaDB Export Reference (v1)](#ibridadb-export-reference-v1)
  - [NOTES/LIMITATIONS](#noteslimitations)
  - [Introduction \& Pipeline Overview](#introduction--pipeline-overview)
  - [Quick Start](#quick-start)
  - [Environment Variables](#environment-variables)
    - [Database Config](#database-config)
    - [Export Parameters](#export-parameters)
    - [Paths](#paths)
  - [Export Flow \& Scripts](#export-flow--scripts)

A **placeholder** for a mermaid diagram is provided below. You can generate or modify the diagram according to your team’s preferences and paste it in there.  

---
## NOTES/LIMITATIONS
- The upper boundary used for ancestor search, which is determine by the CLADE/METACLADE/MACROCLADE, is used for the regional-base tables ()"${REGION_TAG}_min${MIN_OBS}_all_sp_and_ancestors\"), not on one of the clade-export specific tables in cladistic.sh. So this is at odds with the previous design (before we added ancestor-aware logic), where previously the _all_sp table only varied by the REGION_TAG/MIN_OBS. This is probably OK, and might even be necessary for the purposes of determining the exact set of ancestral taxonIDs that need to be included in the base tables when looking to export more than just research-grade observations (i.e. observations with an uncontested species-level label) but it is a bit of a departure from the previous design. So we need to confirm what the set of taxa in the _all_sp_and_ancestors table depends upon (I think it is only the REGION_TAG/MIN_OBS/boundary ranks), and we can potentially mitigate by adjusting the generated base tables names to include the highest root rank (or highest root ranks, in the case of metaclades with multiple root ranks) used in the ancestor search; this will properly version the regional base tables and prevent reuse of base tables when the ancestor scopes differ.
  - So this means that the boundaries of the ancestor search for generating the regional _all_sp_and_ancestors is defined with respect to the configured clade/metaclade for a job, and so the regional base table might need to be recreated for successive job using clades/metaclades with different root ranks.
    - Really, the ancestor-aware logic should be implemented on the cladistic.sh tables.
    - The regional base table names do not fully capture the 'versining', so e.g. a NAfull_min50_all_sp_and_ancestors table generated from a PTA (metaclade) job would not be reusable for a successive job that used a MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)' macroclade, since the PTA root ranks are lower than the L60 rank-- so that regional base table would be missing L50 ancestors. 
      - This would actually be OK in theory but it might break some downstream assumptions, so it would be better to recreate the regional base table for each successive job if that job uses a different root rank.
      - TODO: Confirm that it is only the root rank, not the root taxonID, that is used to define the ancestor search for the regional base tables.
        - If the regional base table _all_sp_and_ancestors only varies by the REGION_TAG/MIN_OBS/boundary ranks, then we could mitigate by adjusting the generated base tables names to include the highest root rank used in the ancestor search.
        - Otherwise, we would need to include the CLADE/METACLADE/MACROCLADE in the regional base table name.
  - regional base table is an increasingly inappropraite name for this table. It was fine when the tables always just included the species in the region that passed the MIN_OBS threshold/the corresponding observations, but the contents of the table are now dependent on the CLADE/METACLADE/MACROCLADE.
    - This issue was averted for INCLUDE_OUT_OF_REGION_OBS, because the regional base observations table always include all observations for the species in the region that passed the MIN_OBS threshold (and now for all their ancestors in the scope of the ancestor search, too).
      - And then if INCLUDE_OUT_OF_REGION_OBS=false, then we re-applied the bounding box for the final table.
    - There might be a similar mitigation approach we could take for ancestor search here. A much more inclusive set of observations for, i.e. _all_sp_and_ancestors would include all species in the region that passed the MIN_OBS threshold and all the ancestors of those species up to but not including L100 (state of matter), i.e. unrestricted ancestor search. _sp_and_ancestors_obs would include all observations where taxon_id=[<a taxonID from _all_sp_and_ancestors].
      - By default, only search for the major-rank ancestors, i.e. L20, L30, L40, L50, L57, L60, L70. So INCLUDE_MINOR_RANKS_IN_ANCESTORS=false. If INCLUDE_MINOR_RANKS_IN_ANCESTORS=true, then include minor ranks in the unbounded ancestor search, and adjust the table names (_all_sp_and_ancestors_incl_minor_ranks, _sp_and_ancestors_obs_incl_minor_ranks).
      - 

## Introduction & Pipeline Overview

The **ibridaDB** export pipeline is designed to subset observations from a large PostgreSQL/PostGIS database based on:

- **Geographic region** (e.g., bounding box for North America).  
- **Minimum number of research-grade observations** required for each species (`MIN_OBS`).  
- **Taxonomic clade** (class, order, or custom “metaclade”).  
- **Export parameters** such as maximum number of photos per species, whether to include only the primary photo or all photos, etc.

The pipeline executes in the following broad stages:

1. **Wrapper Script** (e.g., `r1/wrapper.sh`) sets environment variables to configure the export.  
2. **Main Script** (`common/main.sh`) orchestrates creation of “regional base” tables if needed, then calls the **cladistic** filtering step.  
3. **Regional Base** (`common/regional_base.sh`): Creates two key tables:
   - `<REGION_TAG>_min${MIN_OBS}_all_taxa`, storing species that meet the threshold in the bounding box.  
   - `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs`, storing all observations for those species, possibly including out-of-region if configured.  
4. **Cladistic** (`common/cladistic.sh`): Filters by the chosen clade/metaclade, creating a final table `<EXPORT_GROUP>_observations`. It then exports a CSV using partition-based random sampling.  
5. **Summary & Logging**: `main.sh` writes a single `*_export_summary.txt` that includes relevant environment variables, final observation counts, and other metrics. It also optionally copies the wrapper script for reproducibility.

```mermaid

flowchart TB
    A["Wrapper Script<br/>(r1/wrapper.sh)"] --> B["Main Script<br/>(common/main.sh)"]
    B --> C{"Check<br/>SKIP_REGIONAL_BASE?"}
    C -- "true && table exists" --> E["Reuse existing tables"]
    C -- "false" --> D["regional_base.sh<br/>Create tables"]
    E --> F["cladistic.sh<br/>Filter by clade"]
    D --> F["cladistic.sh<br/>Filter by clade"]
    F --> G["Export CSV + final table"]
    G --> H["main.sh<br/>Write summary<br/>+ copy wrapper"]


```

---

## Quick Start

1. **Clone or navigate** to the `dbTools/export/v0` directory.  
2. **Create/modify** a wrapper script (e.g., `r1/wrapper.sh`) to set your parameters:
   - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`, etc.  
   - `CLADE` or `METACLADE` if focusing on a particular subset of taxa.  
   - Optionally `INCLUDE_OUT_OF_REGION_OBS` and `RG_FILTER_MODE`.  
3. **Run** the wrapper script. The pipeline will:
   - Create region-based tables (if not skipping).  
   - Join them to `expanded_taxa` for a final set of observations.  
   - Write a CSV with photo metadata.  
   - Dump a summary file describing the final dataset.

---

## Environment Variables

Below are the most commonly used variables. **All** variables are read in the wrapper, then passed to `main.sh` (and subsequently to `regional_base.sh` or `cladistic.sh`).

### Database Config

- **`DB_USER`**  
  - **Description**: PostgreSQL user for executing SQL.  
  - **Default**: `"postgres"`  

- **`VERSION_VALUE`**  
  - **Description**: Data version identifier (e.g. `"v0"`).  
  - **Usage**: Combined with `RELEASE_VALUE` to build `DB_NAME`. Also included in logs and table references.

- **`RELEASE_VALUE`**  
  - **Description**: Additional label for the data release (e.g., `"r1"`).  
  - **Usage**: Combined with `VERSION_VALUE` to create `DB_NAME`. Controls logic in some scripts (e.g., whether to include anomaly_score).

- **`ORIGIN_VALUE`**  
  - **Description**: Describes data provenance (e.g., `"iNat-Dec2024"`).  
  - **Usage**: Logged in summary contexts.

- **`DB_NAME`**  
  - **Description**: Full name of the database. Typically `"ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"`.

---

### Export Parameters

- **`REGION_TAG`**  
  - **Description**: Specifies a broad region whose bounding box is defined in `regional_base.sh`.  
  - **Examples**: `"NAfull"`, `"EURfull"`.  
  - **Usage**: `regional_base.sh` calls `set_region_coordinates()` to set `$XMIN,$YMIN,$XMAX,$YMAX`.

- **`MIN_OBS`**  
  - **Description**: Minimum number of **research-grade** observations required for a species to be included in the region-based tables.  
  - **Usage**: In `regional_base.sh`, we gather species with at least `MIN_OBS` research-grade observations inside the bounding box.  
  - **Default**: `50`.

- **`MAX_RN`**  
  - **Description**: Maximum number of research-grade observations to be sampled per species in the final CSV.  
  - **Usage**: In `cladistic.sh`, we do a partition-based random sampling. Observations at species rank beyond `MAX_RN` are excluded in the final CSV.  
  - **Default**: `4000`.

- **`PRIMARY_ONLY`**  
  - **Description**: If `true`, only export the primary (position=0) photo for each observation; if `false`, include all photos.  
  - **Usage**: In `cladistic.sh`, the final `COPY` statement filters `p.position=0` if `PRIMARY_ONLY=true`.  

- **`CLADE`** / **`METACLADE`** / **`MACROCLADE`**  
  - **Description**: Used to define the clade or group of interest.  
  - **Usage**: In `clade_defns.sh`, we have integer-based conditions for major taxonomic ranks (e.g., `L50_taxonID=3` for birds). If `METACLADE` is set, it overrides `CLADE` or `MACROCLADE`.  
  - **Example**: `METACLADE="terrestrial_arthropods"` or `CLADE="amphibia"`.

- **`EXPORT_GROUP`**  
  - **Description**: The final label for the exported dataset; used to name `<EXPORT_GROUP>_observations` and the CSV.  
  - **Usage**: Also appended to summary logs, e.g., `amphibia_export_summary.txt`.

- **`PROCESS_OTHER`**  
  - **Description**: Generic boolean-like flag (default `false`).  
  - **Usage**: Not heavily used but can gate extra steps if desired.

- **`SKIP_REGIONAL_BASE`**  
  - **Description**: If `true`, skip creating region-based tables if they already exist and are non-empty.  
  - **Usage**: `main.sh` checks for `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs`; if present and non-empty, it is reused.  
  - **Default**: `false`.

- **`INCLUDE_OUT_OF_REGION_OBS`**  
  - **Description**: If `true`, once a species is selected by `MIN_OBS` inside the bounding box, we include **all** observations of that species globally. If `false`, re-apply bounding box in final table.  
  - **Usage**: Set in `regional_base.sh`. Defaults to `true` in most wrappers to increase data coverage.

- **`RG_FILTER_MODE`**  
  - **Description**: Controls how research-grade vs. non-research observations are ultimately included in the final `<EXPORT_GROUP>_observations`.  
  - **Possible Values**:
    1. `ONLY_RESEARCH` — Only research-grade observations (`quality_grade='research'`).
    2. `ALL` — Include all observations, regardless of quality_grade.
    3. `ALL_EXCLUDE_SPECIES_NON_RESEARCH` — Include everything except non-research-grade at the species level (`L10_taxonID` non-null).
    4. `ONLY_NONRESEARCH` — Include only non-research-grade (`quality_grade!='research'`).
    5. `ONLY_NONRESEARCH_EXCLUDE_SPECIES` — Include only non-research-grade, but also exclude species-level assignments (`quality_grade!='research' AND L10_taxonID IS NULL`).
    6. `ONLY_NONRESEARCH_WIPE_SPECIES_LABEL` — Include only non-research-grade, but keep any records with a species-level guess. The `L10_taxonID` is forcibly set to `NULL` so the model does not see a species label.  
  - **Usage**: `cladistic.sh` uses a SQL CASE block to apply the correct filter logic. If the mode is unrecognized, it defaults to `ALL`. The final CSV export currently retains a separate step that filters photos by research-grade for sampling (e.g., `o.quality_grade='research'`), so you may wish to unify that logic if desired.

---

### Paths

- **`DB_CONTAINER`**  
  - **Description**: Docker container name running PostgreSQL (often `"ibridaDB"`).  
  - **Usage**: Scripts run `docker exec ${DB_CONTAINER} psql ...`.

- **`HOST_EXPORT_BASE_PATH`**  
  - **Description**: Host filesystem path to store exports.  
  - **Default**: `"/datasets/ibrida-data/exports"`.

- **`CONTAINER_EXPORT_BASE_PATH`**  
  - **Description**: Container path mapping to `HOST_EXPORT_BASE_PATH`.  
  - **Default**: `"/exports"`.

- **`EXPORT_SUBDIR`**  
  - **Description**: A subdirectory typically combining `VERSION_VALUE`, `RELEASE_VALUE`, and other parameters (e.g., `"v0/r1/primary_only_50min_4000max"`).  
  - **Usage**: `main.sh` assembles final output paths from `CONTAINER_EXPORT_BASE_PATH/$EXPORT_SUBDIR` and `HOST_EXPORT_BASE_PATH/$EXPORT_SUBDIR`.

- **`BASE_DIR`**  
  - **Description**: Path to the export scripts inside the container.  
  - **Usage**: Set in the wrapper to locate `common/functions.sh`, `common/regional_base.sh`, `common/cladistic.sh`, etc.

---

## Export Flow & Scripts

Below is the **script-by-script** overview:

1. **`wrapper.sh`**  
   - You define all environment variables needed for your particular export (see above).  
   - Sets `WRAPPER_PATH="$0"` so the pipeline can copy the wrapper into the output directory for reproducibility.  
   - Calls `main.sh`.

2. **`main.sh`**  
   - Validates environment variables.  
   - Creates the export directory (`EXPORT_SUBDIR`).  
   - **Optional**: If `SKIP_REGIONAL_BASE=true`, checks whether region-based tables already exist.  
   - Sources `regional_base.sh` if needed.  
   - Calls `cladistic.sh` to do the final clade-based filtering and CSV export.  
   - Gathers final stats (e.g., #observations, #taxa) from `<EXPORT_GROUP>_observations`.  
   - Writes a single summary file (`${EXPORT_GROUP}_export_summary.txt`).  
   - Optionally copies the wrapper script into the export folder.

3. **`regional_base.sh`**  
   - Sets bounding box coordinates for the region (`REGION_TAG`).  
   - Creates `<REGION_TAG>_min${MIN_OBS}_all_taxa`, selecting species with at least `MIN_OBS` research-grade obs.  
   - Creates `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs`, either global or region-limited based on `INCLUDE_OUT_OF_REGION_OBS`.

4. **`cladistic.sh`**  
   - Loads `clade_defns.sh` to interpret `CLADE`, `METACLADE`, or `MACROCLADE`.  
   - Joins `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs` to `expanded_taxa` and filters for active taxa matching the clade condition. Applies `RG_FILTER_MODE` to the final table. Produces `<EXPORT_GROUP>_observations`.  
   - Exports a CSV with photo metadata, applying `PRIMARY_ONLY` and random sampling of up to `MAX_RN` observations per species.

----
Full Path: export/v0/r0/wrapper.sh

#!/bin/bash

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r0"
export ORIGIN_VALUE="iNat-June2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"

# Base paths
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"

### Primary Terrestrial Arthropoda Export Parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true
export EXPORT_GROUP="primary_terrestrial_arthropoda"
export PROCESS_OTHER=false
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"

### Amphibia Export Parameters (commented out)
# export REGION_TAG="NAfull"
# export MIN_OBS=400
# export MAX_RN=1000
# export PRIMARY_ONLY=true
# export EXPORT_GROUP="amphibia"
# export PROCESS_OTHER=false
# export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"

# Execute main script
"${BASE_DIR}/common/main.sh"

----
Full Path: export/v0/common/main.sh

#!/bin/bash
#
# main.sh
#
# Orchestrates the export pipeline by:
#  1) Validating environment variables
#  2) Always calling regional_base.sh (which handles creating/reusing
#     the region/clade-specific ancestor tables as needed).
#  3) Calling cladistic.sh to produce the final <EXPORT_GROUP>_observations table
#  4) Writing a unified export summary (environment variables + final stats)
#  5) Optionally copying the wrapper script for reproducibility
#
# NOTE:
#  - We no longer do skip/existence checks here. Instead, regional_base.sh
#    performs partial skip logic for its tables (_all_sp, _all_sp_and_ancestors_*, etc.).
#  - We have removed references to ANCESTOR_ROOT_RANKLEVEL, since our new multi-root
#    approach does not require it.

source "${BASE_DIR}/common/functions.sh"

# ------------------------------------------------------------------------------
# 0) Validate Required Environment Variables
# ------------------------------------------------------------------------------
required_vars=(
    "DB_USER" "VERSION_VALUE" "RELEASE_VALUE" "ORIGIN_VALUE"
    "DB_NAME" "REGION_TAG" "MIN_OBS" "MAX_RN"
    "DB_CONTAINER" "HOST_EXPORT_BASE_PATH" "CONTAINER_EXPORT_BASE_PATH"
    "EXPORT_GROUP"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Some environment variables are optional but relevant, so let's note them.
# e.g. SKIP_REGIONAL_BASE, INCLUDE_OUT_OF_REGION_OBS, RG_FILTER_MODE, MIN_OCCURRENCES_PER_RANK,
# INCLUDE_MINOR_RANKS_IN_ANCESTORS, etc.
# We'll just rely on them if set, or let them default in the scripts.

# ------------------------------------------------------------------------------
# 1) Create Export Directory Structure
# ------------------------------------------------------------------------------
print_progress "Creating export directory structure"
EXPORT_DIR="${CONTAINER_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
HOST_EXPORT_DIR="${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
ensure_directory "${HOST_EXPORT_DIR}"

# ------------------------------------------------------------------------------
# 2) Create PostgreSQL Extension & Role if needed (once per container, but safe to run again)
# ------------------------------------------------------------------------------
execute_sql "
DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS dblink;
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'exportuser') THEN
        CREATE ROLE exportuser;
    END IF;
END \$\$;"

# ------------------------------------------------------------------------------
# Timing: We'll measure how long each major phase takes
# ------------------------------------------------------------------------------
overall_start=$(date +%s)
regional_start=$(date +%s)

# ------------------------------------------------------------------------------
# 3) Always Invoke regional_base.sh (which handles partial skip logic)
# ------------------------------------------------------------------------------
print_progress "Invoking ancestor-aware regional_base.sh"
source "${BASE_DIR}/common/regional_base.sh"
print_progress "regional_base.sh completed"
regional_end=$(date +%s)
regional_secs=$(( regional_end - regional_start ))

# ------------------------------------------------------------------------------
# 4) Apply Cladistic Filtering
# ------------------------------------------------------------------------------
cladistic_start=$(date +%s)
print_progress "Applying cladistic filters via cladistic.sh"
source "${BASE_DIR}/common/cladistic.sh"
print_progress "Cladistic filtering complete"
cladistic_end=$(date +%s)
cladistic_secs=$(( cladistic_end - cladistic_start ))

# ------------------------------------------------------------------------------
# 5) Single Unified Export Summary
# ------------------------------------------------------------------------------
stats_start=$(date +%s)
print_progress "Creating unified export summary"

STATS=$(execute_sql "
WITH export_stats AS (
    SELECT 
        COUNT(DISTINCT observation_uuid) AS num_observations,
        COUNT(DISTINCT taxon_id) AS num_taxa,
        COUNT(DISTINCT observer_id) AS num_observers
    FROM \"${EXPORT_GROUP}_observations\"
)
SELECT format(
    'Observations: %s\nUnique Taxa: %s\nUnique Observers: %s',
    num_observations, num_taxa, num_observers
)
FROM export_stats;")

SUMMARY_FILE="${HOST_EXPORT_DIR}/${EXPORT_GROUP}_export_summary.txt"
{
  echo "Export Summary"
  echo "Version: ${VERSION_VALUE}"
  echo "Release: ${RELEASE_VALUE}"
  echo "Region: ${REGION_TAG}"
  echo "Minimum Observations (species): ${MIN_OBS}"
  echo "Maximum Random Number (MAX_RN): ${MAX_RN}"
  echo "Export Group: ${EXPORT_GROUP}"
  echo "Date: $(date)"
  echo "SKIP_REGIONAL_BASE: ${SKIP_REGIONAL_BASE}"
  echo "INCLUDE_OUT_OF_REGION_OBS: ${INCLUDE_OUT_OF_REGION_OBS}"
  echo "INCLUDE_MINOR_RANKS_IN_ANCESTORS: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
  echo "RG_FILTER_MODE: ${RG_FILTER_MODE}"
  echo "MIN_OCCURRENCES_PER_RANK (L20, L30, L40): ${MIN_OCCURRENCES_PER_RANK}"
  echo ""
  echo "Final Table Stats:"
  echo "${STATS}"
  echo ""
  echo "Timing:"
  echo " - Regional Base: ${regional_secs} seconds"
} > "${SUMMARY_FILE}"

stats_end=$(date +%s)
stats_secs=$(( stats_end - stats_start ))
print_progress "Stats/summary step took ${stats_secs} seconds"

# ------------------------------------------------------------------------------
# 6) Optionally Copy the Wrapper Script for Reproducibility
# ------------------------------------------------------------------------------
if [ -n "${WRAPPER_PATH}" ] && [ -f "${WRAPPER_PATH}" ]; then
    cp "${WRAPPER_PATH}" "${HOST_EXPORT_DIR}/"
fi

# ------------------------------------------------------------------------------
# 7) Wrap Up
# ------------------------------------------------------------------------------
overall_end=$(date +%s)
overall_secs=$(( overall_end - overall_start ))
print_progress "Export process complete (total time: ${overall_secs} seconds)"

{
  echo " - Cladistic: ${cladistic_secs} seconds"
  echo " - Summary/Stats Step: ${stats_secs} seconds"
  echo " - Overall: ${overall_secs} seconds"
} >> "${SUMMARY_FILE}"

send_notification "Export for ${EXPORT_GROUP} complete. Summary at ${SUMMARY_FILE}"


----
Full Path: export/v0/common/regional_base.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# regional_base.sh
# ------------------------------------------------------------------------------
# Generates region-specific species tables and associated ancestor sets,
# factoring in the user's clade/metaclade and the major/minor rank mode.
#
# Steps:
#   1) Parse environment variables and region coordinates.
#   2) Build or reuse the <REGION_TAG>_min<MIN_OBS>_all_sp table (region + MIN_OBS only).
#   3) Parse clade condition (single or multi-root). If multi-root, check overlap.
#   4) Build or reuse <REGION_TAG>_min<MIN_OBS>_all_sp_and_ancestors_<cladeID>_<mode>
#   5) Build or reuse <REGION_TAG>_min<MIN_OBS>_sp_and_ancestors_obs_<cladeID>_<mode>
#   6) Output final info/summary
#
# Requires:
#   - environment variables: DB_NAME, DB_CONTAINER, DB_USER, ...
#   - script variables: REGION_TAG, MIN_OBS, SKIP_REGIONAL_BASE,
#     INCLUDE_OUT_OF_REGION_OBS, INCLUDE_MINOR_RANKS_IN_ANCESTORS,
#     etc.
#
# ------------------------------------------------------------------------------

source "${BASE_DIR}/common/functions.sh"
source "${BASE_DIR}/common/clade_defns.sh"
source "${BASE_DIR}/common/clade_helpers.sh"
source "${BASE_DIR}/common/region_defns.sh"

# ---------------------------------------------------------------------------
# 0) Validate Environment + Setup
# ---------------------------------------------------------------------------
: "${REGION_TAG:?Error: REGION_TAG is not set}"
: "${MIN_OBS:?Error: MIN_OBS is not set}"
: "${SKIP_REGIONAL_BASE:?Error: SKIP_REGIONAL_BASE is not set}"
: "${INCLUDE_OUT_OF_REGION_OBS:?Error: INCLUDE_OUT_OF_REGION_OBS is not set}"
: "${INCLUDE_MINOR_RANKS_IN_ANCESTORS:?Error: INCLUDE_MINOR_RANKS_IN_ANCESTORS is not set}"

print_progress "=== regional_base.sh: Starting Ancestor-Aware Regional Base Generation ==="

# Retrieve bounding box for the region
get_region_coordinates || {
  echo "Failed to retrieve bounding box for REGION_TAG=${REGION_TAG}" >&2
  exit 1
}

print_progress "Using bounding box => XMIN=${XMIN}, YMIN=${YMIN}, XMAX=${XMAX}, YMAX=${YMAX}"

# ---------------------------------------------------------------------------
# 1) Build or Reuse <REGION_TAG>_min<MIN_OBS>_all_sp
# ---------------------------------------------------------------------------
ALL_SP_TABLE="${REGION_TAG}_min${MIN_OBS}_all_sp"

check_and_build_all_sp() {
  # Check existence
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ALL_SP_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    # If table exists, check row count
    local row_count
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ALL_SP_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ALL_SP_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_REGIONAL_BASE}" = "true" ]; then
        print_progress "SKIP_REGIONAL_BASE=true => reusing existing _all_sp table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating (or recreating) table \"${ALL_SP_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ALL_SP_TABLE}\" CASCADE;"

  # Build the table with bounding box + rank_level=10 + MIN_OBS filter
  execute_sql "
  CREATE TABLE \"${ALL_SP_TABLE}\" AS
  SELECT s.taxon_id
  FROM observations s
  JOIN taxa t ON t.taxon_id = s.taxon_id
  WHERE t.rank_level = 10
    AND s.quality_grade = 'research'
    AND s.geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
  GROUP BY s.taxon_id
  HAVING COUNT(s.observation_uuid) >= ${MIN_OBS};
  "
}

check_and_build_all_sp

# ---------------------------------------------------------------------------
# 2) Parse Clade Condition & Check Overlap if Multi-root
# ---------------------------------------------------------------------------
CLADE_CONDITION="$(get_clade_condition)"
print_progress "Clade Condition: ${CLADE_CONDITION}"

root_list=( $(parse_clade_expression "${CLADE_CONDITION}") )
root_count="${#root_list[@]}"
print_progress "Found ${root_count} root(s) from the clade condition"

# Decide on a short ID for the clade/metaclade
# (if you want to embed actual environment variables: e.g. $CLADE or $METACLADE
#  or parse the user-supplied string from the condition. We'll do a naive approach.)
if [ -n "${METACLADE}" ]; then
  CLADE_ID="${METACLADE}"
elif [ -n "${CLADE}" ]; then
  CLADE_ID="${CLADE}"
elif [ -n "${MACROCLADE}" ]; then
  CLADE_ID="${MACROCLADE}"
else
  # fallback if user didn't set anything
  CLADE_ID="universal"
fi

# Clean up the clade_id so it doesn't contain spaces or special chars
CLADE_ID="${CLADE_ID// /_}"

# If multi-root => check overlap
if [ "${root_count}" -gt 1 ]; then
  print_progress "Multiple roots => checking independence"
  check_root_independence "${DB_NAME}" "${root_list[@]}"
  if [ $? -ne 0 ]; then
    echo "ERROR: Overlap detected among metaclade roots. Aborting."
    exit 1
  fi
  print_progress "All roots are mutually independent"
fi

# Decide majorOrMinor string
if [ "${INCLUDE_MINOR_RANKS_IN_ANCESTORS}" = "true" ]; then
  RANK_MODE="inclMinor"
else
  RANK_MODE="majorOnly"
fi

# Build final table names
ANCESTORS_TABLE="${REGION_TAG}_min${MIN_OBS}_all_sp_and_ancestors_${CLADE_ID}_${RANK_MODE}"
ANCESTORS_OBS_TABLE="${REGION_TAG}_min${MIN_OBS}_sp_and_ancestors_obs_${CLADE_ID}_${RANK_MODE}"

# ---------------------------------------------------------------------------
# 3) Build or Reuse <REGION_TAG>_min<MIN_OBS>_all_sp_and_ancestors_<cladeID>_<mode>
# ---------------------------------------------------------------------------
check_and_build_ancestors() {
  # Check existence
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ANCESTORS_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    local row_count
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ANCESTORS_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ANCESTORS_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_REGIONAL_BASE}" = "true" ]; then
        print_progress "Skipping creation of ancestors table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating table \"${ANCESTORS_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ANCESTORS_TABLE}\" CASCADE;"
  execute_sql "
  CREATE TABLE \"${ANCESTORS_TABLE}\" (
    taxon_id integer PRIMARY KEY
  );
  "

  # local function to insert ancestors for one root
  local insert_ancestors_for_root
  insert_ancestors_for_root() {
    local root_pair="$1"  # e.g. "50=47158"
    local rank_part="${root_pair%%=*}"
    local root_taxid="${root_pair##*=}"

    local col_name="L${rank_part}_taxonID"

    local boundary_rank="$rank_part"
    if [ "${INCLUDE_MINOR_RANKS_IN_ANCESTORS}" = "false" ]; then
      boundary_rank="$(get_major_rank_floor "${rank_part}")"
    fi

    execute_sql "
    ----------------------------------------------------------------
    -- 1) get all species that are part of the root in the region --
    ----------------------------------------------------------------
    DROP TABLE IF EXISTS temp_${root_taxid}_sp_list CASCADE;
    CREATE TEMP TABLE temp_${root_taxid}_sp_list AS
    SELECT s.taxon_id
    FROM \"${ALL_SP_TABLE}\" s
    JOIN expanded_taxa e ON e.\"taxonID\" = s.taxon_id
    WHERE e.\"${col_name}\" = ${root_taxid};

    ----------------------------------------------------------------
    -- 2) unravel each species up to boundary_rank
    ----------------------------------------------------------------
    DROP TABLE IF EXISTS temp_${root_taxid}_all_ancestors CASCADE;

    WITH unravel AS (
      SELECT
        e.\"taxonID\" as sp_id,
        e.\"L5_taxonID\", e.\"L10_taxonID\", e.\"L11_taxonID\", e.\"L12_taxonID\",
        e.\"L13_taxonID\", e.\"L15_taxonID\", e.\"L20_taxonID\", e.\"L24_taxonID\",
        e.\"L25_taxonID\", e.\"L26_taxonID\", e.\"L27_taxonID\", e.\"L30_taxonID\",
        e.\"L32_taxonID\", e.\"L33_taxonID\", e.\"L33_5_taxonID\", e.\"L34_taxonID\",
        e.\"L34_5_taxonID\", e.\"L35_taxonID\", e.\"L37_taxonID\", e.\"L40_taxonID\",
        e.\"L43_taxonID\", e.\"L44_taxonID\", e.\"L45_taxonID\", e.\"L47_taxonID\",
        e.\"L50_taxonID\", e.\"L53_taxonID\", e.\"L57_taxonID\", e.\"L60_taxonID\",
        e.\"L67_taxonID\", e.\"L70_taxonID\"
      FROM expanded_taxa e
      JOIN temp_${root_taxid}_sp_list sp
         ON e.\"taxonID\" = sp.taxon_id
    )
    SELECT DISTINCT UNNEST(array[
      unravel.sp_id,
      -- For each column, we conditionally include it if its rank < boundary_rank
      -- e.g. rankLevel(...) < boundary_rank
      -- This is conceptual: you need a function or a join to retrieve rank levels.

      CASE WHEN \"rankLevel\"(unravel.\"L5_taxonID\")  < ${boundary_rank} THEN unravel.\"L5_taxonID\"  ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L10_taxonID\") < ${boundary_rank} THEN unravel.\"L10_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L11_taxonID\") < ${boundary_rank} THEN unravel.\"L11_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L12_taxonID\") < ${boundary_rank} THEN unravel.\"L12_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L13_taxonID\") < ${boundary_rank} THEN unravel.\"L13_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L15_taxonID\") < ${boundary_rank} THEN unravel.\"L15_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L20_taxonID\") < ${boundary_rank} THEN unravel.\"L20_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L24_taxonID\") < ${boundary_rank} THEN unravel.\"L24_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L25_taxonID\") < ${boundary_rank} THEN unravel.\"L25_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L26_taxonID\") < ${boundary_rank} THEN unravel.\"L26_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L27_taxonID\") < ${boundary_rank} THEN unravel.\"L27_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L30_taxonID\") < ${boundary_rank} THEN unravel.\"L30_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L32_taxonID\") < ${boundary_rank} THEN unravel.\"L32_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L33_taxonID\") < ${boundary_rank} THEN unravel.\"L33_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L33_5_taxonID\") < ${boundary_rank} THEN unravel.\"L33_5_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L34_taxonID\") < ${boundary_rank} THEN unravel.\"L34_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L34_5_taxonID\") < ${boundary_rank} THEN unravel.\"L34_5_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L35_taxonID\") < ${boundary_rank} THEN unravel.\"L35_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L37_taxonID\") < ${boundary_rank} THEN unravel.\"L37_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L40_taxonID\") < ${boundary_rank} THEN unravel.\"L40_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L43_taxonID\") < ${boundary_rank} THEN unravel.\"L43_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L44_taxonID\") < ${boundary_rank} THEN unravel.\"L44_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L45_taxonID\") < ${boundary_rank} THEN unravel.\"L45_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L47_taxonID\") < ${boundary_rank} THEN unravel.\"L47_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L50_taxonID\") < ${boundary_rank} THEN unravel.\"L50_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L53_taxonID\") < ${boundary_rank} THEN unravel.\"L53_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L57_taxonID\") < ${boundary_rank} THEN unravel.\"L57_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L60_taxonID\") < ${boundary_rank} THEN unravel.\"L60_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L67_taxonID\") < ${boundary_rank} THEN unravel.\"L67_taxonID\" ELSE NULL END,
      CASE WHEN \"rankLevel\"(unravel.\"L70_taxonID\") < ${boundary_rank} THEN unravel.\"L70_taxonID\" ELSE NULL END
    ]) AS taxon_id
    INTO TEMP temp_${root_taxid}_all_ancestors
    FROM unravel
    WHERE true; -- placeholder in case we need conditions

    INSERT INTO \"${ANCESTORS_TABLE}\"(taxon_id)
    SELECT DISTINCT taxon_id
    FROM temp_${root_taxid}_all_ancestors
    WHERE taxon_id IS NOT NULL;
    "
  }

  # single vs multi-root
  if [ "${root_count}" -eq 0 ]; then
    print_progress "No recognized root => no ancestors inserted. (Might be 'TRUE' clade?)"
  elif [ "${root_count}" -eq 1 ]; then
    print_progress "Single root => straightforward insertion"
    insert_ancestors_for_root "${root_list[0]}"
  else
    print_progress "Multi-root => union each root's ancestor set"
    for root_entry in "${root_list[@]}"; do
      insert_ancestors_for_root "${root_entry}"
    done
  fi
}

check_and_build_ancestors

# ---------------------------------------------------------------------------
# 4) Build or Reuse <REGION_TAG>_min<MIN_OBS>_sp_and_ancestors_obs_<cladeID>_<mode>
# ---------------------------------------------------------------------------
check_and_build_ancestors_obs() {
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ANCESTORS_OBS_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    local row_count
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ANCESTORS_OBS_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ANCESTORS_OBS_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_REGIONAL_BASE}" = "true" ]; then
        print_progress "Skipping creation of ancestors_obs table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating table \"${ANCESTORS_OBS_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ANCESTORS_OBS_TABLE}\" CASCADE;"

  local OBS_COLUMNS
  OBS_COLUMNS="$(get_obs_columns)"

  if [ "${INCLUDE_OUT_OF_REGION_OBS}" = "true" ]; then
    execute_sql "
    CREATE TABLE \"${ANCESTORS_OBS_TABLE}\" AS
    SELECT ${OBS_COLUMNS}
    FROM observations
    WHERE taxon_id IN (
      SELECT taxon_id
      FROM \"${ANCESTORS_TABLE}\"
    );
    "
  else
    execute_sql "
    CREATE TABLE \"${ANCESTORS_OBS_TABLE}\" AS
    SELECT ${OBS_COLUMNS}
    FROM observations
    WHERE taxon_id IN (
      SELECT taxon_id
      FROM \"${ANCESTORS_TABLE}\"
    )
    AND geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326);
    "
  fi
}

check_and_build_ancestors_obs

export ANCESTORS_OBS_TABLE="${ANCESTORS_OBS_TABLE}" # for cladistic.sh

print_progress "=== regional_base.sh: Completed building base tables for ${REGION_TAG}, minObs=${MIN_OBS}, clade=${CLADE_ID}, mode=${RANK_MODE} ==="


----
Full Path: export/v0/common/functions.sh

#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

get_obs_columns() {
    # Start with standard columns
    local cols="observation_uuid, observer_id, latitude, longitude, positional_accuracy, taxon_id, quality_grade, observed_on"
    
    # TEMPORARY HOTFIX: Commenting out version tracking columns until bulk update is complete
    # Add version tracking columns
    # cols="${cols}, origin, version, release"
    
    # Check if anomaly_score exists in this release
    if [[ "${RELEASE_VALUE}" == "r1" ]]; then
        cols="${cols}, anomaly_score"
    fi
    
    echo "$cols"
}

# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    mkdir -p "${dir}"
    chmod -R 777 "${dir}"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f get_obs_columns
export -f ensure_directory
export -f send_notification

----
Full Path: export/v0/common/region_defns.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# region_defns.sh
# ------------------------------------------------------------------------------
# This file defines the bounding box coordinates for each supported region.
#
# Usage:
#   source region_defns.sh
#   Then set REGION_TAG in your environment, and use get_region_coordinates()
#   to populate XMIN, XMAX, YMIN, YMAX environment variables.
# ------------------------------------------------------------------------------

declare -A REGION_COORDINATES

# North America
REGION_COORDINATES["NAfull"]="(-169.453125 12.211180 -23.554688 84.897147)"

# Europe
REGION_COORDINATES["EURwest"]="(-12.128906 40.245992 12.480469 60.586967)"
REGION_COORDINATES["EURnorth"]="(-25.927734 54.673831 45.966797 71.357067)"
REGION_COORDINATES["EUReast"]="(10.722656 41.771312 39.550781 59.977005)"
REGION_COORDINATES["EURfull"]="(-30.761719 33.284620 43.593750 72.262310)"

# Mediterranean
REGION_COORDINATES["MED"]="(-16.259766 29.916852 36.474609 46.316584)"

# Australia
REGION_COORDINATES["AUSfull"]="(111.269531 -47.989922 181.230469 -9.622414)"

# Asia
REGION_COORDINATES["ASIAse"]="(82.441406 -11.523088 153.457031 28.613459)"
REGION_COORDINATES["ASIAeast"]="(462.304688 23.241346 550.195313 78.630006)"
REGION_COORDINATES["ASIAcentral"]="(408.515625 36.031332 467.753906 76.142958)"
REGION_COORDINATES["ASIAsouth"]="(420.468750 1.581830 455.097656 39.232253)"
REGION_COORDINATES["ASIAsw"]="(386.718750 12.897489 423.281250 48.922499)"
REGION_COORDINATES["ASIA_nw"]="(393.046875 46.800059 473.203125 81.621352)"

# South America
REGION_COORDINATES["SAfull"]="(271.230469 -57.040730 330.644531 15.114553)"

# Africa
REGION_COORDINATES["AFRfull"]="(339.082031 -37.718590 421.699219 39.232253)"

# ------------------------------------------------------------------------------
# get_region_coordinates()
# ------------------------------------------------------------------------------
# Sets XMIN, YMIN, XMAX, YMAX variables from the region definition for REGION_TAG.
# If REGION_TAG is not recognized, prints an error and returns 1.
#
# Usage:
#   export REGION_TAG="XYZ"
#   source region_defns.sh
#   get_region_coordinates  # => sets XMIN, YMIN, XMAX, YMAX
# ------------------------------------------------------------------------------
function get_region_coordinates() {
    local coords="${REGION_COORDINATES[$REGION_TAG]}"
    if [ -z "$coords" ]; then
        echo "ERROR: Unknown REGION_TAG: $REGION_TAG" >&2
        return 1
    fi
    
    # Parse the coordinate quadruple from parentheses
    read XMIN YMIN XMAX YMAX <<< "${coords//[()]/}"

    # Export them for use by the caller
    export XMIN YMIN XMAX YMAX
}

export -f get_region_coordinates


----
Full Path: export/v0/common/clade_defns.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# clade_defns.sh
# ------------------------------------------------------------------------------
# This file defines the integer-based filtering expressions for macroclades,
# clades, and metaclades, referencing columns in "expanded_taxa".
#
# Usage:
#   source clade_defns.sh
#   Then pick a macroclade (MACROCLADE="..."), or a clade (CLADE="..."),
#   or a metaclade (METACLADE="...") in your environment, and the
#   cladistic.sh script will build a condition from one of the arrays below.
#
# Example:
#   MACROCLADES["arthropoda"]='("L60_taxonID" = 47119)'
#   CLADES["insecta"]='("L50_taxonID" = 47120)'
#   METACLADES["primary_terrestrial_arthropoda"]='("L50_taxonID" = 47120 OR "L50_taxonID" = 101885)'
#
# Be sure to substitute the correct taxonIDs for your local database!
# ------------------------------------------------------------------------------
#
# Sections in this file:
#   1) Macroclade Definitions
#   2) Clade Definitions
#   3) Metaclade Definitions
#   4) get_clade_condition() helper
#
# NOTE: We do NOT remove any existing definitions or comments.

# ---[ 1) Macroclade Definitions ]---------------------------------------------
# Typically for kingdom-level (L70) or phylum-level (L60) anchors.

declare -A MACROCLADES

# 1) Arthropoda => phylum at L60 = 47120
MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'

# 2) Chordata => phylum at L60 = 2
MACROCLADES["chordata"]='("L60_taxonID" = 2)'

# 3) Plantae => kingdom at L70 = 47126
MACROCLADES["plantae"]='("L70_taxonID" = 47126)'

# 4) Fungi => kingdom at L70 = 47170
MACROCLADES["fungi"]='("L70_taxonID" = 47170)'

# (Optional) If you consider Actinopterygii, Mammalia, Reptilia, etc.
# to be "macroclades," you may define them here instead of in CLADES.
# For instance:
#   MACROCLADES["mammalia"]='("L50_taxonID" = 40151)'


# ---[ 2) Clade Definitions ]--------------------------------------------------
# Typically for class-level (L50), order-level (L40), or narrower taxonomic groups.
# single-root, so functionally equivalent to METACLADES.

declare -A CLADES

# -- Plant Clades (Subphylum and Class levels) --
# -- Plant Subphylum (L57) --
CLADES["angiospermae"]='("L57_taxonID" = 47125)' # flowering plants

# -- Plant Classes (L50) --
CLADES["liliopsida"]='("L50_taxonID" = 47163)'    # monocots
CLADES["magnoliopsida"]='("L50_taxonID" = 47124)' # dicots

# -- Class-level (L50) Examples --
CLADES["actinopterygii"]='("L50_taxonID" = 47178)'
CLADES["amphibia"]='("L50_taxonID" = 20978)'
CLADES["arachnida"]='("L50_taxonID" = 47119)'
CLADES["aves"]='("L50_taxonID" = 3)'
CLADES["insecta"]='("L50_taxonID" = 47158)'
CLADES["mammalia"]='("L50_taxonID" = 40151)'
CLADES["reptilia"]='("L50_taxonID" = 26036)'

# -- Order-level (L40) Examples --
CLADES["testudines"]='("L40_taxonID" = 39532)'
CLADES["crocodylia"]='("L40_taxonID" = 26039)'
CLADES["coleoptera"]='("L40_taxonID" = 47208)'
CLADES["lepidoptera"]='("L40_taxonID" = 47157)'
CLADES["hymenoptera"]='("L40_taxonID" = 47201)'
CLADES["hemiptera"]='("L40_taxonID" = 47744)'
CLADES["orthoptera"]='("L40_taxonID" = 47651)'
CLADES["odonata"]='("L40_taxonID" = 47792)'
CLADES["diptera"]='("L40_taxonID" = 47822)'

# -- Additional Named Groups (Suborders, Clade Subsets, etc.) --
# Pterygota => The DB shows two taxonIDs (184884, 418641) plus
# another entry with L40_taxonID=48796. We combine them with OR:
CLADES["pterygota"]='("taxonID" = 184884 OR "taxonID" = 418641 OR "L40_taxonID" = 48796)'

# Phasmatodea => Not found in your query results. If/when you know its ID,
# you can fill it in here:
# CLADES["phasmatodea"]='("L40_taxonID" = ???)'

# Subclades within Hymenoptera (all share L40_taxonID=47201).
# Typically, referencing the top-level order is "hymenoptera"
# while these might be more specific anchor taxa:
CLADES["aculeata"]='("taxonID" = 326777)'
CLADES["apoidea"]='("taxonID" = 47222)'
CLADES["formicidae"]='("taxonID" = 47336)'
CLADES["vespoidea"]='("taxonID" = 48740)'
CLADES["vespidae"]='("taxonID" = 52747)'


# ---[ 3) Metaclade Definitions ]----------------------------------------------
# Multi-root or cross-macroclade definitions. Compose bigger groups using OR.

declare -A METACLADES

# Example 1: terrestrial_arthropods => Insecta OR Arachnida OR others.
METACLADES["terrestrial_arthropods"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'

# Example 2: flying_vertebrates => Birds (aves) OR Bats (chiroptera)
METACLADES["flying_vertebrates"]='("L50_taxonID" = 3 OR "L40_taxonID" = 7721)'

# Example 3: nonavian_reptiles => reptilia minus birds.
# METACLADES["nonavian_reptiles"]='("L50_taxonID" = 26036 AND "L50_taxonID" != 3)'


# ---[ 4) get_clade_condition() Helper ]-----------------------------------------
# Picks the correct expression given environment variables (METACLADE, CLADE,
# MACROCLADE). This is used by cladistic.sh to filter rows.

function get_clade_condition() {
  local condition

  # 1) If METACLADE is set (and found in METACLADES), return that
  if [[ -n "${METACLADE}" && -n "${METACLADES[${METACLADE}]}" ]]; then
    condition="${METACLADES[${METACLADE}]}"
    echo "${condition}"
    return
  fi

  # 2) Else if CLADE is set
  if [[ -n "${CLADE}" && -n "${CLADES[${CLADE}]}" ]]; then
    condition="${CLADES[${CLADE}]}"
    echo "${condition}"
    return
  fi

  # 3) Else if MACROCLADE is set
  if [[ -n "${MACROCLADE}" && -n "${MACROCLADES[${MACROCLADE}]}" ]]; then
    condition="${MACROCLADES[${MACROCLADE}]}"
    echo "${condition}"
    return
  fi

  # 4) Fallback: no recognized key => no filter
  echo "TRUE"
}

export -f get_clade_condition

----
Full Path: export/v0/common/cladistic.sh

#!/bin/bash
# -------------------------------------------------------------------------------
# cladistic.sh
# -------------------------------------------------------------------------------
# Creates a final observation subset for the user-specified clade/metaclade,
# referencing the "expanded_taxa" table. The input table for this script is
# typically provided in ANCESTORS_OBS_TABLE, which contains:
#
#   - All observations of species that passed the MIN_OBS threshold in the
#     specified region (REGION_TAG bounding box) plus all their ancestral
#     taxonIDs, up to the root rank(s) of the chosen CLADE/METACLADE.
#
#   - If INCLUDE_OUT_OF_REGION_OBS=true, that table may also include
#     observations that fall outside the bounding box but belong to those
#     same species or ancestor taxonIDs. Otherwise, the bounding box is
#     re-applied to keep only in-bounds data.
#
#   - If INCLUDE_MINOR_RANKS_IN_ANCESTORS=false, the table only includes
#     major (decade) ranks up to the boundary. If =true, minor ranks are
#     included.
#
#   - This table is named like:
#       ${REGION_TAG}_min${MIN_OBS}_sp_and_ancestors_obs_${CLADE_ID}_${RANK_MODE}
#     but we do NOT compute that name here. Instead, we read the environment
#     variable $ANCESTORS_OBS_TABLE, which is set by regional_base.sh.
#
# Once we have that ancestor-based observation set, we:
#   1) Create an export-specific table named <EXPORT_GROUP>_observations.
#   2) Possibly filter out or rewrite certain rows based on RG_FILTER_MODE
#      (e.g. wiping species-level IDs if not research grade).
#   3) (Optional) Wipe partial ranks (L20, L30, L40) if they have fewer than
#      MIN_OCCURRENCES_PER_RANK occurrences.
#   4) Export the final dataset to CSV, applying a maximum row limit per
#      species (MAX_RN) for research-grade rows, and unioning that with
#      everything else.
#
# Environment Variables Used:
#   - ANCESTORS_OBS_TABLE: The table containing the region/clade-specific
#                          ancestor-based observations. (Set by regional_base.sh)
#   - EXPORT_GROUP:        Used as a prefix for the final table name
#   - RG_FILTER_MODE:      Determines how we handle non-research vs. research rows
#   - MIN_OCCURRENCES_PER_RANK: If set >= 1, triggers partial-rank wiping for L20/L30/L40
#   - MAX_RN:              The maximum number of random research-grade rows to keep
#   - PRIMARY_ONLY:        If true, we only keep photo records where position=0
#   - EXPORT_DIR:          Destination for the CSV export
#   - DB_CONTAINER, DB_USER, DB_NAME, etc. for database connections
#
# -------------------------------------------------------------------------------
set -e

source "${BASE_DIR}/common/functions.sh"
source "${BASE_DIR}/common/clade_defns.sh"  # only needed if we reference get_clade_condition, etc.

# -------------------------------------------------------------------------------
# 0) Validate that ANCESTORS_OBS_TABLE is set
# -------------------------------------------------------------------------------
if [ -z "${ANCESTORS_OBS_TABLE}" ]; then
  echo "ERROR: cladistic.sh requires ANCESTORS_OBS_TABLE to be set (exported by regional_base.sh)."
  exit 1
fi

print_progress "cladistic.sh: Using ancestor-based table = ${ANCESTORS_OBS_TABLE}"

# We'll build a final table named <EXPORT_GROUP>_observations
TABLE_NAME="${EXPORT_GROUP}_observations"
OBS_COLUMNS="$(get_obs_columns)"

# -------------------------------------------------------------------------------
# Step A) Drop any old final table
# -------------------------------------------------------------------------------
execute_sql "
DROP TABLE IF EXISTS \"${TABLE_NAME}\" CASCADE;
"

# -------------------------------------------------------------------------------
# Step B) Construct a WHERE clause & rewriting logic based on RG_FILTER_MODE
# -------------------------------------------------------------------------------
# Typically, we interpret RG_FILTER_MODE to decide how to handle research vs. non-research
# observations. Possibly we wipe the L10_taxonID for non-research, or exclude them, etc.

rg_where_condition="TRUE"
rg_l10_col="e.\"L10_taxonID\""

case "${RG_FILTER_MODE}" in
  "ONLY_RESEARCH")
    rg_where_condition="o.quality_grade='research'"
    ;;
  "ALL")
    rg_where_condition="TRUE"
    ;;
  "ALL_EXCLUDE_SPECIES_NON_RESEARCH")
    rg_where_condition="NOT (o.quality_grade!='research' AND e.\"L10_taxonID\" IS NOT NULL)"
    ;;
  "ONLY_NONRESEARCH")
    rg_where_condition="o.quality_grade!='research'"
    ;;
  "ONLY_NONRESEARCH_EXCLUDE_SPECIES")
    rg_where_condition="(o.quality_grade!='research' AND e.\"L10_taxonID\" IS NULL)"
    ;;
  "ONLY_NONRESEARCH_WIPE_SPECIES_LABEL")
    rg_where_condition="o.quality_grade!='research'"
    rg_l10_col="NULL::integer"
    ;;
  *)
    rg_where_condition="TRUE"
    ;;
esac

print_progress "Building final table \"${TABLE_NAME}\" from ${ANCESTORS_OBS_TABLE}"

# -------------------------------------------------------------------------------
# Step C) Create <EXPORT_GROUP>_observations table by joining to expanded_taxa
# -------------------------------------------------------------------------------
# In theory, ${ANCESTORS_OBS_TABLE} has taxon_id referencing the desired region/clade
# observations. We join with expanded_taxa for additional columns. Then we apply
# RG_FILTER_MODE logic.

execute_sql "
CREATE TABLE \"${TABLE_NAME}\" AS
SELECT
    o.${OBS_COLUMNS},
    e.\"taxonID\"       AS expanded_taxonID,
    e.\"rankLevel\"     AS expanded_rankLevel,
    e.\"name\"          AS expanded_name,

    e.\"L5_taxonID\",
    ${rg_l10_col}       AS \"L10_taxonID\",
    e.\"L11_taxonID\",
    e.\"L12_taxonID\",
    e.\"L13_taxonID\",
    e.\"L15_taxonID\",
    e.\"L20_taxonID\",
    e.\"L24_taxonID\",
    e.\"L25_taxonID\",
    e.\"L26_taxonID\",
    e.\"L27_taxonID\",
    e.\"L30_taxonID\",
    e.\"L32_taxonID\",
    e.\"L33_taxonID\",
    e.\"L33_5_taxonID\",
    e.\"L34_taxonID\",
    e.\"L34_5_taxonID\",
    e.\"L35_taxonID\",
    e.\"L37_taxonID\",
    e.\"L40_taxonID\",
    e.\"L43_taxonID\",
    e.\"L44_taxonID\",
    e.\"L45_taxonID\",
    e.\"L47_taxonID\",
    e.\"L50_taxonID\",
    e.\"L53_taxonID\",
    e.\"L57_taxonID\",
    e.\"L60_taxonID\",
    e.\"L67_taxonID\",
    e.\"L70_taxonID\"
FROM \"${ANCESTORS_OBS_TABLE}\" o
JOIN expanded_taxa e ON e.\"taxonID\" = o.taxon_id
WHERE e.\"taxonActive\" = TRUE
  AND (${rg_where_condition});
"

# -------------------------------------------------------------------------------
# Step D) Optional Partial-Rank Wiping (L20, L30, L40) if MIN_OCCURRENCES_PER_RANK >= 1
# -------------------------------------------------------------------------------
if [ -z "${MIN_OCCURRENCES_PER_RANK}" ] || [ "${MIN_OCCURRENCES_PER_RANK}" = "-1" ]; then
  print_progress "Skipping partial-rank wipe (MIN_OCCURRENCES_PER_RANK not set or == -1)."
else
  print_progress "Applying partial-rank wipe with threshold = ${MIN_OCCURRENCES_PER_RANK}"

  RANK_COLS=("L20_taxonID" "L30_taxonID" "L40_taxonID")
  for rc in "${RANK_COLS[@]}"; do
    print_progress "Wiping low-occurrence ${rc} if usage < ${MIN_OCCURRENCES_PER_RANK}"
    execute_sql "
    WITH usage_ct AS (
      SELECT \"${rc}\" as tid, COUNT(*) as c
      FROM \"${TABLE_NAME}\"
      WHERE \"${rc}\" IS NOT NULL
      GROUP BY 1
    )
    UPDATE \"${TABLE_NAME}\"
    SET \"${rc}\" = NULL
    FROM usage_ct
    WHERE usage_ct.tid = \"${TABLE_NAME}\".\"${rc}\"
      AND usage_ct.c < ${MIN_OCCURRENCES_PER_RANK};
    "
  done
fi

# -------------------------------------------------------------------------------
# Step E) Export Final CSV with a max row limit per species for research-grade
# -------------------------------------------------------------------------------
send_notification "cladistic.sh: Exporting filtered observations"
print_progress "cladistic.sh: Exporting filtered observations"

pos_condition="TRUE"
if [ "${PRIMARY_ONLY}" = true ]; then
    pos_condition="p.position=0"
fi

execute_sql "
COPY (
  WITH
  capped_research_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY random()
      ) AS rn
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE
      ${pos_condition}
      AND o.quality_grade='research'
      AND o.\"L10_taxonID\" IS NOT NULL
  ),
  everything_else AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE
      ${pos_condition}
      AND NOT (o.quality_grade='research' AND o.\"L10_taxonID\" IS NOT NULL)
  )
  SELECT * FROM capped_research_species
  WHERE rn <= ${MAX_RN}
  UNION ALL
  SELECT * FROM everything_else
) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"

print_progress "cladistic.sh: Filtering & export complete"


----
Full Path: export/v0/common/clade_helpers.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# clade_helpers.sh
# ------------------------------------------------------------------------------
# This file contains helper functions for multi-root/metaclade logic,
# rank boundary calculations, and advanced taxon-ancestry checks.
#
# Proposed usage:
#   1) "parse_clade_expression()" to parse user-provided condition strings
#      (e.g. "L50_taxonID=123 OR L40_taxonID=9999") into structured data.
#   2) "check_root_independence()" to verify that each root is truly disjoint
#      (none is an ancestor of another).
#   3) "get_major_rank_floor()" to compute the next-lower major-rank boundary
#      if user does not want to include minor ranks. Typically used if the root
#      is e.g. 57 => 50. If user includes minor ranks, we skip the rounding.
#
# NOTE: We do not forcibly integrate with existing "get_clade_condition()"
# in clade_defns.sh. Instead, you can call parse_clade_expression() if you
# want to do deeper multi-root logic.
#
# Implementation details:
#   - We store a reference map from "L<number>_taxonID" to the numeric rank
#     (e.g. "L50_taxonID" => 50). If the user requests minor ranks, we do not
#     round them down to the multiple of 10.
#   - We rely on "expanded_taxa" for ancestry checks. The "check_root_independence()"
#     function is conceptual: it gathers each root's entire ancestry (e.g. ~30
#     columns from L5..L70) and ensures no overlap among root sets.
#
# ------------------------------------------------------------------------------
#
# Exports:
#   - parse_clade_expression()
#   - check_root_independence()
#   - get_major_rank_floor()
#

# -------------------------------------------------------------
# A) Internal reference: Maps "L50_taxonID" => 50, "L40_taxonID" => 40, etc.
# -------------------------------------------------------------
declare -A RANKLEVEL_MAP=(
  ["L5_taxonID"]="5"
  ["L10_taxonID"]="10"
  ["L11_taxonID"]="11"
  ["L12_taxonID"]="12"
  ["L13_taxonID"]="13"
  ["L15_taxonID"]="15"
  ["L20_taxonID"]="20"
  ["L24_taxonID"]="24"
  ["L25_taxonID"]="25"
  ["L26_taxonID"]="26"
  ["L27_taxonID"]="27"
  ["L30_taxonID"]="30"
  ["L32_taxonID"]="32"
  ["L33_taxonID"]="33"
  ["L33_5_taxonID"]="33.5"
  ["L34_taxonID"]="34"
  ["L34_5_taxonID"]="34.5"
  ["L35_taxonID"]="35"
  ["L37_taxonID"]="37"
  ["L40_taxonID"]="40"
  ["L43_taxonID"]="43"
  ["L44_taxonID"]="44"
  ["L45_taxonID"]="45"
  ["L47_taxonID"]="47"
  ["L50_taxonID"]="50"
  ["L53_taxonID"]="53"
  ["L57_taxonID"]="57"
  ["L60_taxonID"]="60"
  ["L67_taxonID"]="67"
  ["L70_taxonID"]="70"
  # stateofmatter => 100, if we had that in expanded_taxa
)

# -------------------------------------------------------------
# B) parse_clade_expression()
# -------------------------------------------------------------
# This function attempts to parse a string such as:
#   "L50_taxonID = 47158 OR L60_taxonID=123"
# and return an array-like structure describing each root:
#   e.g. ( (50,47158), (60,123) )
#
# The method is naive: we split on "OR", then parse each sub-expression
# for "Lxx_taxonID" and a numeric taxonID. We ignore parentheses or any
# advanced logic. If the user uses AND or more complicated logic, we skip
# or partially parse. This is suitable for typical "Lxx_taxonID=yyyy" patterns.
#
# Return format:
#   We'll echo lines in the form "50=47158" "60=123".
#   The caller can read them into an array for further usage.
#
# Example usage:
#   roots=( $(parse_clade_expression "L50_taxonID=47158 OR L40_taxonID=7721") )
#   # roots -> ["50=47158" "40=7721"]
#
function parse_clade_expression() {
  local expr="$1"
  # We'll do a simple split on "OR" (case-insensitive).
  local or_parts
  # Lowercase for naive approach
  local lower_expr=$(echo "$expr" | tr '[:upper:]' '[:lower:]')
  IFS=' or ' read -ra or_parts <<< "$lower_expr"

  local results=()

  for part in "${or_parts[@]}"; do
    # Trim spaces
    local trimmed=$(echo "$part" | sed 's/^[ \t]*//;s/[ \t]*$//')
    # Now we expect something like "l50_taxonid=47158" or "l40_taxonid=7721"
    # We'll parse the left side (e.g. "l50_taxonid") and the numeric on the right side.

    # Attempt a naive parse:
    # separate by '='
    IFS='=' read -r lhs rhs <<< "$trimmed"
    lhs=$(echo "$lhs" | sed 's/ //g')  # remove spaces
    rhs=$(echo "$rhs" | sed 's/ //g')

    # e.g. lhs might be "l50_taxonid"
    # We attempt to find the rank int from RANKLEVEL_MAP
    # first uppercase the "lhs" to standardize: "L50_TAXONID"
    local upper_lhs=$(echo "$lhs" | tr '[:lower:]' '[:upper:]')
    # but we have RANKLEVEL_MAP in normal "L50_taxonID" form, so let's revert the underscore:
    local normal_lhs=$(echo "$upper_lhs" | sed 's/_taxonid$/_taxonID/')

    # Now we revert the uppercase "L50_taxonID" => "L50_taxonID" literally.
    # Actually, let's do:
    normal_lhs=$(echo "$normal_lhs" | tr '[:upper:]' '[:lower:]' | sed 's/_taxonid$/_taxonID/')

    # The user might have "l50_taxonid" or "L50_taxonID" in the expression.
    # We'll forcibly ensure "L50_taxonID" if possible:
    # We'll do a small fix:
    # e.g. normal_lhs might be "l50_taxonID".
    local final_lhs="L${normal_lhs:1}" # hack: replace leading "l" with uppercase "L"

    # Now we see if final_lhs is in RANKLEVEL_MAP
    local rank_level="${RANKLEVEL_MAP[$final_lhs]}"

    if [ -z "$rank_level" ]; then
      # If we fail, we can skip. In real code we'd warn.
      continue
    fi
    # Now "rhs" should be numeric
    # We'll assume user typed an integer taxonID.
    # We'll just store "rank=taxonID" in results.
    results+=( "${rank_level}=${rhs}" )
  done

  echo "${results[@]}"
}

function check_root_independence() {
  # --------------------------------------------------------------------------
  # check_root_independence()
  #
  # Ensures that each root in a multi-root scenario is truly independent;
  # i.e., no root is an ancestor or descendant of another. We do so by:
  #   1) For each root taxonID, fetching its row from expanded_taxa,
  #      collecting (taxonID, L5_taxonID, L10_taxonID, ..., L70_taxonID).
  #   2) Storing them in a set (space-separated IDs).
  #   3) Checking every pair of sets for intersection. Any overlap indicates
  #      that one root’s ancestry includes another root, violating independence.
  #
  # Usage:
  #   check_root_independence <db_name> <rootArray...>
  # where rootArray are strings of the form "rankLevel=taxonID" (e.g. "50=47119").
  #
  # Returns:
  #   0 if no overlap, 1 if overlap is detected or if a root is not found.
  # --------------------------------------------------------------------------

  local dbName="$1"
  shift
  local roots=("$@")  # e.g. ("50=47158" "60=47119")

  # If there's 0 or 1 root, there's nothing to compare => trivially independent
  if [ "${#roots[@]}" -le 1 ]; then
    return 0
  fi

  declare -A rootSets  # will map index => "list of ancestor taxonIDs"

  for i in "${!roots[@]}"; do
    local pair="${roots[$i]}"
    local rank="${pair%%=*}"
    local tid="${pair##*=}"

    # -- 1) Query expanded_taxa for the single row of ancestor IDs --
    # We use a COPY ... TO STDOUT with CSV approach to produce a single CSV line,
    # which we then parse. This row includes the root's own taxonID plus L5..L70.
    local sql="
COPY (
  SELECT
    e.taxonID,
    e.\"L5_taxonID\", e.\"L10_taxonID\", e.\"L11_taxonID\", e.\"L12_taxonID\",
    e.\"L13_taxonID\", e.\"L15_taxonID\", e.\"L20_taxonID\", e.\"L24_taxonID\",
    e.\"L25_taxonID\", e.\"L26_taxonID\", e.\"L27_taxonID\", e.\"L30_taxonID\",
    e.\"L32_taxonID\", e.\"L33_taxonID\", e.\"L33_5_taxonID\", e.\"L34_taxonID\",
    e.\"L34_5_taxonID\", e.\"L35_taxonID\", e.\"L37_taxonID\", e.\"L40_taxonID\",
    e.\"L43_taxonID\", e.\"L44_taxonID\", e.\"L45_taxonID\", e.\"L47_taxonID\",
    e.\"L50_taxonID\", e.\"L53_taxonID\", e.\"L57_taxonID\", e.\"L60_taxonID\",
    e.\"L67_taxonID\", e.\"L70_taxonID\"
  FROM expanded_taxa e
  WHERE e.taxonID = ${tid}
) TO STDOUT WITH CSV HEADER;
"

    local query_result
    query_result="$(execute_sql "$sql")"

    # If the query has no data row (other than a header), it might indicate
    # that taxonID was not found in expanded_taxa.
    local data_line
    data_line="$(echo "$query_result" | tail -n1)"

    # If there's only a header line, tail -n1 might be identical to the header if no data row.
    # We'll do a small check to see if that line contains "taxonID" => meaning it's the header.
    if [[ "$data_line" == *"taxonID"* ]]; then
      echo "ERROR: check_root_independence: No row found for taxonID=${tid}. Aborting." >&2
      return 1
    fi

    # -- 2) Parse CSV line into an array of ancestor IDs (including the root itself). --
    local ancestors=()
    IFS=',' read -ra fields <<< "$data_line"
    for f in "${fields[@]}"; do
      # strip whitespace
      local trimmed="$(echo "$f" | xargs)"
      # only keep if non-empty
      if [[ -n "$trimmed" ]]; then
        ancestors+=( "$trimmed" )
      fi
    done

    # Store them in a space-separated list in rootSets[i].
    # Example: "47158 47157 541 ... 1"
    rootSets["$i"]="${ancestors[*]}"
  done

  # -- 3) Compare each pair of sets for intersection. --
  for ((i=0; i<${#roots[@]}; i++)); do
    for ((j=i+1; j<${#roots[@]}; j++)); do
      local set1=" ${rootSets[$i]} "
      for t2 in ${rootSets[$j]}; do
        # Match on a token boundary, to avoid partial string hits
        if [[ "$set1" =~ " $t2 " ]]; then
          echo "ERROR: Overlap detected between root #$i (${roots[$i]}) \
and root #$j (${roots[$j]}) on taxonID=${t2}" >&2
          return 1
        fi
      done
    done
  done

  # If we reached here, no overlap was found
  return 0
}

# -------------------------------------------------------------
# D) get_major_rank_floor()
# -------------------------------------------------------------
# This function returns the next-lower major rank multiple of 10 if we want
# to exclude minor ranks. For instance:
#   if input=57 => output=50
#   if input=50 => output=40
#   if input=70 => output=60
#
# If the user wants minor ranks, we might skip or do partial rounding logic.
# For now, we do a straightforward approach:
#
function get_major_rank_floor() {
  local input_rank="$1"
  # We'll do a naive loop:
  # possible major ranks = [70,60,50,40,30,20,10,5]
  # or we can do math: floor((input_rank/10))*10 => but that fails for e.g. 57 => 50 is fine
  # Actually that might be enough, but let's handle if it's exactly a multiple of 10 => we subtract 10 again
  # e.g. 50 => 40, because we want "strictly less than the root rank".
  # If input=57 => floor(57/10)*10=50 => good
  # If input=50 => floor(50/10)*10=50 => but we want 40 => so let's do -10 if exactly multiple

  local base=$(( input_rank/10*10 ))
  if (( $(echo "$input_rank == $base" | bc) == 1 )); then
    # means input is multiple of 10
    base=$(( base-10 ))
  fi
  echo "$base"
}

export -f parse_clade_expression
export -f check_root_independence
export -f get_major_rank_floor

----
Full Path: export/v0/r1/wrapper_mammalia.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=4000
export PRIMARY_ONLY=true
export CLADE="mammalia"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: export/v0/r1/wrapper_amphibia.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=4000
export PRIMARY_ONLY=true
export CLADE="amphibia"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: export/v0/r1/wrapper_pta_non_rg.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

# We’ll use a metaclade here as an example
export METACLADE="primary_terrestrial_arthropoda"
export EXPORT_GROUP="${METACLADE}_non_rg"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=false

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ONLY_NONRESEARCH_WIPE_SPECIES_LABEL"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"


----
Full Path: export/v0/r1/wrapper_angiospermae.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true
export CLADE="angiospermae"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: export/v0/r1/wrapper_dicots.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true
export CLADE="magnoliopsida"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: export/v0/r1/wrapper_pta.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

# We’ll use a metaclade here as an example
export METACLADE="primary_terrestrial_arthropoda"
export EXPORT_GROUP="${METACLADE}"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=false

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ONLY_RESEARCH"

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"


----
Full Path: export/v0/r1/wrapper_aves.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=4000
export PRIMARY_ONLY=true
export CLADE="aves"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: export/v0/r1/wrapper_reptilia.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=4000
export PRIMARY_ONLY=true
export CLADE="reptilia"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: export/v0/r1/wrapper.sh

#!/bin/bash
#
# wrapper.sh
#
# A typical user-facing script that sets environment variables and then calls main.sh.
# We define WRAPPER_PATH="$0" so that main.sh can copy this file for reproducibility.

SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"

echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Provide path to wrapper for reproducibility
export WRAPPER_PATH="$0"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=200
export PRIMARY_ONLY=true

# We could set CLADE or METACLADE here; let's pick something as an example:
export CLADE="amphibia"
export EXPORT_GROUP="${CLADE}"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false

# NEW ENV VARS
export INCLUDE_OUT_OF_REGION_OBS=true
export RG_FILTER_MODE="ALL"

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"

"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: export/v0/scrap/export_per_species_snippet.sh

#!/bin/bash
#
# export_per_species_snippet.sh
#
# This script performs a per-species random sampling export from an
# already-existing table. It does NOT drop or re-create the table.
# It's intended for quick usage to avoid re-running all upstream steps.
#
# Usage: set environment variables before calling, e.g.:
#   export DB_CONTAINER="ibridaDB"
#   export DB_USER="postgres"
#   export DB_NAME="ibrida-v0-r1"
#   export EXPORT_GROUP="primary_terrestrial_arthropoda"
#   export EXPORT_DIR="/exports/v0/r1/primary_only_50min_4000max"
#   export MAX_RN=4000
#   export PRIMARY_ONLY=true
#   Then run:
#   ./export_per_species_snippet.sh
#

# CLARIFY: We assume that the user already has a table named ${EXPORT_GROUP}_observations
#          that includes all columns needed, and a 'photos' table too.
# ASSUMPTION: The container user has write access to $EXPORT_DIR.

SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
source "${SCRIPT_DIR}/functions.sh"

TABLE_NAME="${EXPORT_GROUP}_observations"

print_progress "Starting quick per-species export from existing table: ${TABLE_NAME}"

# The actual COPY logic is adapted from cladistic.sh, focusing on partition-based random sampling:
if [ "${PRIMARY_ONLY}" = true ]; then
    # Photos with position=0, quality_grade='research'
    execute_sql "
COPY (
  WITH per_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY random()
      ) AS species_rand_idx
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE p.position = 0
      AND o.quality_grade = 'research'
  )
  SELECT *
  FROM per_species
  WHERE
    (\"L10_taxonID\" IS NULL)
    OR (\"L10_taxonID\" IS NOT NULL AND species_rand_idx <= ${MAX_RN})
) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"
else
    # All photos for the final set, restricted to quality_grade='research'
    execute_sql "
COPY (
  WITH per_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY random()
      ) AS species_rand_idx
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE o.quality_grade = 'research'
  )
  SELECT *
  FROM per_species
  WHERE
    (\"L10_taxonID\" IS NULL)
    OR (\"L10_taxonID\" IS NOT NULL AND species_rand_idx <= ${MAX_RN})
) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"
fi

print_progress "Quick per-species export complete."

----
Full Path: ../docker/stausee/docker-compose.yml

services:
  ibrida:
    image: postgis/postgis:15-3.3
    user: "998:998"
    shm_size: '16g'
    environment:
      POSTGRES_PASSWORD: ooglyboogly69
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_SHARED_BUFFERS: 8GB
      POSTGRES_WORK_MEM: 2048MB
      POSTGRES_MAINTENANCE_WORK_MEM: 4GB
    volumes:
      - ~/repo/ibridaDB/dbTools:/tool
      - ~/repo/ibridaDB/dbQueries:/query
      - /database/ibridaDB:/var/lib/postgresql/data
      - /datasets/ibrida-data/exports:/exports
      - /datasets/ibrida-data/intake:/metadata
    ports:
      - "5432:5432"
    container_name: ibridaDB

----
Full Path: ../docker/stausee/entrypoint.sh

#!/bin/bash
set -e

# Just log and exit - let Docker's default entrypoint handle PostgreSQL
echo "Entrypoint script executed at $(date)"

----
Full Path: README.md

# ibrida Database Reproduction Guide

## Overview
This guide documents the end-to-end process for **reproducing** and **exporting** from the ibrida database, which is derived from iNaturalist open data dumps. The database uses a versioning system with two components:
- **Version (v#)**: Indicates structural changes (schema revisions) to the database.
- **Release (r#)**: Indicates distinct data releases from iNaturalist under the same schema version.

For example:
- **v0r0**: June 2024 iNat data release
- **v0r1**: December 2024 iNat data release (adds `anomaly_score` column to `observations`)

## System Architecture
The pipeline is split into two phases:
1. **Database Initialization** (`ingest/`)
2. **Data Export** (`export/`)

Each phase has:
- Common scripts for shared logic
- Release- or job-specific *wrapper scripts* that set environment variables for that particular run

## 1. Database Initialization (ingest/)
### Directory Structure

```
dbTools/ingest/v0/
├── common/
│   ├── geom.sh         # Geometry calculations
│   ├── vers_origin.sh  # Version/origin updates
│   └── main.sh         # Core ingestion logic
├── r0/
│   ├── wrapper.sh      # June 2024 release
│   └── structure.sql   # schema for r0
└── r1/
    ├── wrapper.sh      # December 2024 release
    └── structure.sql   # schema for r1 (adds anomaly_score)
```

### Running the Ingestion Process
1. **Make scripts executable**:
    ```bash
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/*.sh
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r0/wrapper.sh
    chmod +x /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
    ```
2. **Run**:
    ```bash
    # For June 2024 (r0)
    /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r0/wrapper.sh

    # For December 2024 (r1)
    /home/caleb/repo/ibridaDB/dbTools/ingest/v0/r1/wrapper.sh
    ```

## 2. Data Export (export/)
The export pipeline allows flexible subsetting of the DB by region, minimum threshold, clade, etc. For additional detail, see [export.md](export.md).

### Directory Structure
```
dbTools/export/v0/
├── common/
│   ├── main.sh            # Orchestrates creation or skipping of base tables; final summary
│   ├── regional_base.sh   # Region-based table creation, ancestor-aware logic
│   ├── cladistic.sh       # Taxonomic filtering, partial-rank wiping, CSV export
│   └── functions.sh       # Shared shell functions
├── r0/
│   └── wrapper.sh         # Example job wrapper for June 2024 export
├── r1/
│   └── wrapper.sh         # Example job wrapper for December 2024 export
└── export.md              # Detailed usage documentation (v1)
```

### Export Workflow
1. **User creates/edits a wrapper script** (e.g., `r1/my_special_wrapper.sh`) to set:
   - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`
   - Optional toggles like `INCLUDE_OUT_OF_REGION_OBS`, `RG_FILTER_MODE`, `ANCESTOR_ROOT_RANKLEVEL`, `MIN_OCCURRENCES_PER_RANK`
   - A unique `EXPORT_GROUP` name
2. **Run** that wrapper. The pipeline will:
   1. **(regional_base.sh)** Build base tables of in-threshold species + ancestors, optionally bounding to region or not, depending on `INCLUDE_OUT_OF_REGION_OBS`.
   2. **(cladistic.sh)** Filter final observations by clade or metaclade, optionally wipe partial ranks, and do a random-sample CSV export.
   3. **(main.sh)** Write a summary file enumerating environment variables, row counts, timing, etc.

3. **Check** `/datasets/ibrida-data/exports` for final CSV output (organized by `VERSION_VALUE` / `RELEASE_VALUE` / any job-specific subdirectory).

### Drafting a New Wrapper
It is **recommended** to create a separate wrapper script for each new export job. For instance:
```bash
#!/bin/bash

export WRAPPER_PATH="$0"

export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_test"

export INCLUDE_OUT_OF_REGION_OBS=false
export RG_FILTER_MODE="ALL"
export ANCESTOR_ROOT_RANKLEVEL=40
export MIN_OCCURRENCES_PER_RANK=30

# other optional vars, e.g. PROCESS_OTHER, SKIP_REGIONAL_BASE, etc.

export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/myamphibia_job"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

source "${BASE_DIR}/common/functions.sh"

/home/caleb/repo/ibridaDB/dbTools/export/v0/common/main.sh
```
Then `chmod +x` this file and run it to generate a new job.

### Example Outputs
The final CSV and summary are placed in a subdirectory (e.g. `v0/r1/myamphibia_job`). A typical summary file `amphibia_test_export_summary.txt` includes:
- Region: NAfull
- MIN_OBS: 50
- RG_FILTER_MODE: ALL
- Observations: 10,402
- Unique Taxa: 927
- Timings for each step

### Further Reading
- **[export.md](export/v0/export.md)** for a deeper parameter reference (v1).
- **clade_defns.sh** for built-in definitions of macroclades, clades, and metaclades.

## Overall Flow
Below is a schematic of the entire ingest→export pipeline. For details on the ingest side, see [Ingestion docs](#database-initialization-ingest):
```
Ingest (ingest/v0/) --> Database --> Export (export/v0/)
```
In the export sub-phase, each new wrapper script can define a distinct job. Summaries and CSVs are stored in `HOST_EXPORT_BASE_PATH` for easy retrieval and analysis.

## Notes on Schema
- **v0r1** adds the `anomaly_score numeric(15,6)` column to `observations`.
- The export scripts automatically check if that column is present based on `RELEASE_VALUE`.
- If partial-labeled data is desired (coarse ranks for rare species), see the advanced features in `regional_base.sh` (ancestor logic) and `cladistic.sh` (partial-rank wiping logic).

**Notes**:
- The ingest side is unchanged for v0→v0r1 except for adding columns and data updates.
- The export side is significantly more flexible now, supporting ancestor‐aware logic and partial-labeled data.  
- Each new export job typically has its own wrapper script referencing the relevant `VERSION_VALUE`, `RELEASE_VALUE`, region, and clade parameters.

----
Full Path: FLOW.md

```mermaid
flowchart TB

    subgraph Ingest["Database Initialization (ingest/)"]
        i_wrap["Ingest Wrapper<br/>(e.g. r0/wrapper.sh)"]
        i_main["Ingest Main<br/>(common/main.sh)"]
        i_other["Other Common Scripts"]
        db["(ibridaDB PostgreSQL)"]
        i_wrap --> i_main
        i_main --> i_other
        i_other --> db
    end

    subgraph Export["Data Export (export/)"]
        e_wrap["Export Wrapper<br/>(e.g. r1/my_wrapper.sh)"]
        e_main["Export Main<br/>(common/main.sh)"]
        rbase["regional_base.sh<br/>Species + Ancestors"]
        clad["cladistic.sh<br/>RG_FILTER_MODE + partial-labeled"]
        csv_out["CSV + Summary Files"]
        e_wrap --> e_main
        e_main --> rbase
        rbase --> clad
        clad --> csv_out
    end

    i_other --> db
    db --> e_wrap

    style Ingest fill:#f9f,stroke:#333,stroke-width:2px
    style Export fill:#bbf,stroke:#333,stroke-width:2px

```

----
Full Path: schema.md

```markdown
### Observations
Column | Description
-------|------------
observation_uuid | A unique identifier associated with each observation also available at iNaturalist.org via URLs constructed like this https://www.inaturalist.org/observations/c075c500-b566-44aa-847c-95da8fb8b3c9
observer_id | The identifier of the associated iNaturalist user who recorded the observation
latitude | The latitude where the organism was encountered
longitude | The longitude where the organism was encountered
positional_accuracy | The uncertainty in meters around the latitude and longitude
taxon_id | The identifier of the associated axon the observation has been identified as
quality_grade | `Casual` observations are missing certain data components (e.g. latitude) or may have flags associated with them not shown here (e.g. `location appears incorrect`). Observations flagged as not wild are also considered Casual. All other observations are either `Needs ID` or `Research Grade`. Generally, Research Grade observations have more than one agreeing identifications at the species level, or if there are disagreements at least ⅔ of the identifications are in agreement a the species level
observed_on | The date at which the observation took place
<NOTE> New column added in v0/r1 'anomaly_score' </NOTE>

### Observers
Column | Description
-------|------------
observer_id | A unique identifier associated with each observer also available on https://www.inaturalist.org via URLs constructed like this: https://www.inaturalist.org/users/1
login | A unique login associated with each observer
name | Personal name of the observer, if provided

### Photos
Column | Description
-------|------------
photo_uuid | A unique identifier associated with each photo. Note that photo_uuid can be non-unique across different observations.
photo_id | A photo identifier used on iNaturalist and available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/photos/113756411
observation_uuid | The identifier of the associated observation
observer_id | The identifier of the associated observer who took the photo
extension | The image file format, e.g. `jpeg`
license | All photos in the dataset have open licenses (e.g. Creative Commons) and unlicensed (CC0 / public domain)
width | The width of the photo in pixels
height | The height of the photo in pixels
position | When observations have multiple photos the user can set the position in which the photos should appear. Lower numbers are meant to appear first
>The issue is that some observations include more than one photo, and photos associated with observations that have >1 photo share a photo_id and photo_uuid, which I did not expect. These additional photos (which have their own rows in the 'photos' table) are denoted by the 'position' field, where position ==0 indicates that the photo is the primary photo for the record. If an observation only has one photo, then the associated 'photos' record will have position == 0. Therefore. I'm pretty sure that a composite key of photo_id ++ photo_uuid ++ position will function as a primary key. 

### Taxa
Column | Description
-------|------------
taxon_id | A unique identifier associated with each node in the iNaturalist taxonomy hierarchy. Also available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/taxa/47219
ancestry | The taxon_ids of ancestry of the taxon ordered from the root of the tree to the taxon concatenated together with `\`
rank_level | A number associated with the rank. Taxon rank_levels must be less than the rank level of their parent. For example, a taxon with rank genus and rank_level 20 cannot descend from a taxon of rank species and rank_level 10
rank | A constrained set of labels associated with nodes on the hierarchy. These include the standard Linnaean ranks: Kingdom, Phylum, Class, Order, Family, Genus, Species, and a number of internodes such as Subfamily
name | The scientific name for the taxon
active | When the taxonomy changes, generally taxa aren’t deleted on iNaturalist to avoid breaking links. Instead taxa are made inactive and observations are moved to new active nodes. Occasionally, observations linger on inactive taxa which are no longer active parts of the iNaturalist taxonomy
```

