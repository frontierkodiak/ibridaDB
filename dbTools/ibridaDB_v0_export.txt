Directory tree, stemming from root "/home/caleb/repo/ibridaDB/dbTools":
├── __init__.py
├── export
│   └── v0
│       │   ├── README.md
│       │   ├── regional_base.sh
│       │   ├── wrapper_r0.sh
│       │   └── wrapper_r1.sh
├── ingest
│   ├── v0
│   │   ├── r0
│   │   │   ├── geom.sh
│   │   │   ├── ingest.sh
│   │   │   ├── init.md
│   │   │   └── vers_origin.sh
│   │   └── r1
│   │       │   ├── geom.sh
│   │       │   ├── ingest.sh
│   │       │   ├── init.md
│   │       │   └── vers_origin.sh
│   ├── v0x
│   │   ├── geom.sh
│   │   ├── init.md
│   │   ├── init.sh
│   │   └── init2.bak.md
│   ├── xMerge
│   │   ├── step1_observations.sql
│   │   ├── step1_observers.sql
│   │   ├── step1_photos.sql
│   │   ├── step2_observations.sh
│   │   ├── step3_observations.sql
│   │   ├── step3_observers.sql
│   │   ├── step3_photos.sql
│   │   ├── xMerge.md
│   │   └── xMerge.sh
│   └── xMergeQuick
│       │   ├── progress.txt
│       │   ├── step1_observations.sql
│       │   ├── step1_observers.sql
│       │   ├── step1_pt1_photos.sql
│       │   ├── step1_pt2_photos.sh
│       │   ├── step1_pt3_photos.sql
│       │   ├── step2_observations.sh
│       │   ├── step3_observations.sh
│       │   ├── step3_observers.sql
│       │   ├── step3_photos.sh
│       │   ├── xMergeQuick.sh
│       │   └── xMergeQuick_master.sh
├── parent
│   └── regions
│       │   ├── region
│       │   └── wrapper.sh
├── schema.md
├── schema.py
├── taxa
│   ├── __init__.py
│   ├── analysis_utils.py
│   ├── analyze_diff.py
│   ├── diffs
│   │   └── May2024
│   │       │   ├── L40_analysis.txt
│   │       │   ├── L50_analysis.txt
│   │       │   ├── L60_analysis.txt
│   │       │   ├── active_status_changes.csv
│   │       │   ├── changed_attributes.csv
│   │       │   ├── deprecated_taxon_ids.csv
│   │       │   ├── inactive_observations_count.csv
│   │       │   ├── name_changes.csv
│   │       │   ├── new_taxon_count.csv
│   │       │   └── new_taxon_ids.csv
│   ├── mappings
│   └── reference.md
└── taxa_expanded
    │   ├── make.py
    │   ├── model.py
    │   └── taxa_expanded.md

----
Full Path: schema.md

```markdown
### Observations
Column | Description
-------|------------
observation_uuid | A unique identifier associated with each observation also available at iNaturalist.org via URLs constructed like this https://www.inaturalist.org/observations/c075c500-b566-44aa-847c-95da8fb8b3c9
observer_id | The identifier of the associated iNaturalist user who recorded the observation
latitude | The latitude where the organism was encountered
longitude | The longitude where the organism was encountered
positional_accuracy | The uncertainty in meters around the latitude and longitude
taxon_id | The identifier of the associated axon the observation has been identified as
quality_grade | `Casual` observations are missing certain data components (e.g. latitude) or may have flags associated with them not shown here (e.g. `location appears incorrect`). Observations flagged as not wild are also considered Casual. All other observations are either `Needs ID` or `Research Grade`. Generally, Research Grade observations have more than one agreeing identifications at the species level, or if there are disagreements at least ⅔ of the identifications are in agreement a the species level
observed_on | The date at which the observation took place

### Observers
Column | Description
-------|------------
observer_id | A unique identifier associated with each observer also available on https://www.inaturalist.org via URLs constructed like this: https://www.inaturalist.org/users/1
login | A unique login associated with each observer
name | Personal name of the observer, if provided

### Photos
Column | Description
-------|------------
photo_uuid | A unique identifier associated with each photo. Note that photo_uuid can be non-unique across different observations.
photo_id | A photo identifier used on iNaturalist and available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/photos/113756411
observation_uuid | The identifier of the associated observation
observer_id | The identifier of the associated observer who took the photo
extension | The image file format, e.g. `jpeg`
license | All photos in the dataset have open licenses (e.g. Creative Commons) and unlicensed (CC0 / public domain)
width | The width of the photo in pixels
height | The height of the photo in pixels
position | When observations have multiple photos the user can set the position in which the photos should appear. Lower numbers are meant to appear first
>The issue is that some observations include more than one photo, and photos associated with observations that have >1 photo share a photo_id and photo_uuid, which I did not expect. These additional photos (which have their own rows in the 'photos' table) are denoted by the 'position' field, where position ==0 indicates that the photo is the primary photo for the record. If an observation only has one photo, then the associated 'photos' record will have position == 0. Therefore. I'm pretty sure that a composite key of photo_id ++ photo_uuid ++ position will function as a primary key. 

### Taxa
Column | Description
-------|------------
taxon_id | A unique identifier associated with each node in the iNaturalist taxonomy hierarchy. Also available on iNaturalist.org via URLs constructed like this https://www.inaturalist.org/taxa/47219
ancestry | The taxon_ids of ancestry of the taxon ordered from the root of the tree to the taxon concatenated together with `\`
rank_level | A number associated with the rank. Taxon rank_levels must be less than the rank level of their parent. For example, a taxon with rank genus and rank_level 20 cannot descend from a taxon of rank species and rank_level 10
rank | A constrained set of labels associated with nodes on the hierarchy. These include the standard Linnaean ranks: Kingdom, Phylum, Class, Order, Family, Genus, Species, and a number of internodes such as Subfamily
name | The scientific name for the taxon
active | When the taxonomy changes, generally taxa aren’t deleted on iNaturalist to avoid breaking links. Instead taxa are made inactive and observations are moved to new active nodes. Occasionally, observations linger on inactive taxa which are no longer active parts of the iNaturalist taxonomy
```

----
Full Path: ingest/v0/r0/vers_origin.sh

#!/bin/bash

# Function to run the update in parallel
run_update() {
  local TABLE_NAME=$1
  local COLUMN_NAME=$2
  local VALUE=$3
  local OFFSET=$4
  local LIMIT=$5
  local DB_NAME=$6

  docker exec ibrida psql -U postgres -d "${DB_NAME}" -c "
  UPDATE ${TABLE_NAME}
  SET ${COLUMN_NAME} = '${VALUE}'
  WHERE ctid IN (
    SELECT ctid
    FROM ${TABLE_NAME}
    ORDER BY ctid
    OFFSET ${OFFSET}
    LIMIT ${LIMIT}
  );"
}

# Check if correct number of arguments are provided
if [ "$#" -ne 5 ]; then
  echo "Usage: $0 <database_name> <num_workers> <origin_value> <version_value>"
  exit 1
fi

# Define arguments
DB_NAME=$1
NUM_PROCESSES=$2
ORIGIN_VALUE=$3
VERSION_VALUE=$4

# Tables and their columns to update
declare -A TABLES_COLUMNS
TABLES_COLUMNS=(
  ["taxa"]="origin,version"
  ["observers"]="origin,version"
  ["observations"]="origin,version"
  ["photos"]="origin,version"
)

# Function to update columns in parallel
update_columns_in_parallel() {
  local TABLE_NAME=$1
  local COLUMN_NAME=$2
  local VALUE=$3
  local TOTAL_ROWS
  TOTAL_ROWS=$(docker exec ibrida psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};")
  local BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES))

  for ((i=0; i<NUM_PROCESSES; i++)); do
    local OFFSET=$((i * BATCH_SIZE))
    run_update ${TABLE_NAME} ${COLUMN_NAME} ${VALUE} ${OFFSET} ${BATCH_SIZE} ${DB_NAME} &
  done
}

# Update columns in parallel
for TABLE_NAME in "${!TABLES_COLUMNS[@]}"; do
  IFS=',' read -ra COLUMNS <<< "${TABLES_COLUMNS[$TABLE_NAME]}"
  for COLUMN in "${COLUMNS[@]}"; do
    if [ "$COLUMN" == "origin" ]; then
      update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$ORIGIN_VALUE"
    elif [ "$COLUMN" == "version" ]; then
      update_columns_in_parallel "$TABLE_NAME" "$COLUMN" "$VERSION_VALUE"
    fi
  done
done

# Wait for all processes to finish
wait
echo "All updates completed."


----
Full Path: ingest/v0/r0/ingest.sh

#!/bin/bash

# Database and user variables
DB_USER="postgres"
DB_TEMPLATE="template_postgis"
NUM_PROCESSES=16
BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

# Source variable
SOURCE="Dec2024"

# Construct origin value based on source
ORIGIN_VALUE="iNat-${SOURCE}"

# Version variable
VERSION_VALUE="v0"

# Construct database name
DB_NAME="ibrida-${VERSION_VALUE}"

# Function to execute SQL commands
execute_sql() {
  local sql="$1"
  docker exec ibrida psql -U "$DB_USER" -d "$DB_NAME" -c "$sql"
}

# Function to print progress
print_progress() {
  local message="$1"
  echo "======================================"
  echo "$message"
  echo "======================================"
}

# Create database, drop if exists
print_progress "Creating database"
docker exec ibrida psql -U "$DB_USER" -c "DROP DATABASE IF EXISTS \"$DB_NAME\";"
docker exec ibrida psql -U "$DB_USER" -c "CREATE DATABASE \"$DB_NAME\" WITH TEMPLATE $DB_TEMPLATE OWNER $DB_USER;"

# Connect to the database and create tables
print_progress "Creating tables"
execute_sql "
BEGIN;

CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

COMMIT;
"

# Import data
print_progress "Importing data"
execute_sql "
BEGIN;

COPY observations FROM '/metadata/${SOURCE}/observations.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY photos FROM '/metadata/${SOURCE}/photos.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY taxa FROM '/metadata/${SOURCE}/taxa.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY observers FROM '/metadata/${SOURCE}/observers.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;

COMMIT;
"

# Create indexes
print_progress "Creating indexes"
execute_sql "
BEGIN;

CREATE INDEX index_photos_photo_uuid ON photos USING btree (photo_uuid);
CREATE INDEX index_photos_observation_uuid ON photos USING btree (observation_uuid);
CREATE INDEX index_photos_position ON photos USING btree (position);
CREATE INDEX index_photos_photo_id ON photos USING btree (photo_id);
CREATE INDEX index_taxa_taxon_id ON taxa USING btree (taxon_id);
CREATE INDEX index_observers_observers_id ON observers USING btree (observer_id);
CREATE INDEX index_observations_observer_id ON observations USING btree (observer_id);
CREATE INDEX index_observations_quality ON observations USING btree (quality_grade);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);
CREATE INDEX index_taxa_active ON taxa USING btree (active);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);

COMMIT;
"

# Add geom column (parallelized calculation using geom.sh)
print_progress "Adding geom column"
execute_sql "ALTER TABLE observations ADD COLUMN geom public.geometry;"

# Run parallel geom calculations
print_progress "Running parallel geom calculations"
"${BASE_DIR}/geom.sh" "$DB_NAME" "observations" "$NUM_PROCESSES" "$BASE_DIR"

# Create geom index
print_progress "Creating geom index"
execute_sql "
BEGIN;

CREATE INDEX observations_geom ON observations USING GIST (geom);

COMMIT;
"

# Vacuum analyze
print_progress "Vacuum analyze"
execute_sql "VACUUM ANALYZE;"

# Add origin and version columns in parallel
print_progress "Adding origin and version columns"
execute_sql "
BEGIN;

ALTER TABLE taxa ADD COLUMN origin VARCHAR(255);
ALTER TABLE observers ADD COLUMN origin VARCHAR(255);
ALTER TABLE observations ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN version VARCHAR(255);
ALTER TABLE observations ADD COLUMN version VARCHAR(255);
ALTER TABLE observers ADD COLUMN version VARCHAR(255);
ALTER TABLE taxa ADD COLUMN version VARCHAR(255);

COMMIT;
"

# Run parallel updates for origin and version columns
print_progress "Running parallel updates for origin and version columns"
"${BASE_DIR}/vers_origin.sh" "$DB_NAME" "$NUM_PROCESSES" "$ORIGIN_VALUE" "$VERSION_VALUE"

# Create indexes for origin and version columns
print_progress "Creating indexes for origin and version columns"
execute_sql "
BEGIN;

CREATE INDEX index_taxa_origins ON taxa USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_taxa_name ON taxa USING GIN (to_tsvector('simple', name));
CREATE INDEX index_observers_origins ON observers USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_observations_origins ON observations USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_origins ON photos USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_version ON photos USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observations_version ON observations USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observers_version ON observers USING GIN (to_tsvector('simple', version));
CREATE INDEX index_taxa_version ON taxa USING GIN (to_tsvector('simple', version));

COMMIT;
"

print_progress "Database setup complete"


----
Full Path: ingest/v0/r0/init.md

## First-time ingest 
*First-time ingest, using iNat open data dump from June2023 (iNatJune2023).*

Import iNat metadata into Ibrida:
```sql
-- bash into psql:
docker exec -ti ibrida psql -U postgres

-- Create new database
### NOTE #### The real database, for whatever reason, is actually named 'postgres'!!
CREATE DATABASE "inaturalist-open-data" WITH TEMPLATE template_postgis OWNER postgres;
-- apply structure
--- Just open structure.sql and run these commands manually..
CREATE TABLE observations (
    observation_uuid uuid NOT NULL,
    observer_id integer,
    latitude numeric(15,10),
    longitude numeric(15,10),
    positional_accuracy integer,
    taxon_id integer,
    quality_grade character varying(255),
    observed_on date
);

CREATE TABLE photos (
    photo_uuid uuid NOT NULL,
    photo_id integer NOT NULL,
    observation_uuid uuid NOT NULL,
    observer_id integer,
    extension character varying(5),
    license character varying(255),
    width smallint,
    height smallint,
    position smallint
);

CREATE TABLE taxa (
    taxon_id integer NOT NULL,
    ancestry character varying(255),
    rank_level double precision,
    rank character varying(255),
    name character varying(255),
    active boolean
);

CREATE TABLE observers (
    observer_id integer NOT NULL,
    login character varying(255),
    name character varying(255)
);

-- Import:
COPY observations FROM '/metadata/inaturalist-open-data-20230627/observations.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY photos FROM '/metadata/inaturalist-open-data-20230627/photos.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY taxa FROM '/metadata/inaturalist-open-data-20230627/taxa.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
COPY observers FROM '/metadata/inaturalist-open-data-20230627/observers.csv' DELIMITER E'\t' QUOTE E'\b' CSV HEADER;
```

Make indices:
```sql
CREATE INDEX index_photos_photo_uuid ON photos USING btree (photo_uuid);
CREATE INDEX index_photos_observation_uuid ON photos USING btree (observation_uuid);
CREATE INDEX index_photos_position ON photos USING btree (position);
CREATE INDEX index_photos_photo_id ON photos USING btree (photo_id);
CREATE INDEX index_taxa_taxon_id ON taxa USING btree (taxon_id);
CREATE INDEX index_observers_observers_id ON observers USING btree (observer_id);
CREATE INDEX index_observations_observer_id ON observations USING btree (observer_id);
CREATE INDEX index_observations_quality ON observations USING btree (quality_grade);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);
CREATE INDEX index_taxa_active ON taxa USING btree (active);
CREATE INDEX index_observations_taxon_id ON observations USING btree (taxon_id);
```
Add geometry:
```sql
ALTER TABLE observations ADD COLUMN geom public.geometry;
UPDATE observations SET geom = ST_GeomFromText('POINT(' || longitude || ' ' || latitude || ')', 4326);
CREATE INDEX observations_geom ON observations USING GIST (geom);
VACUUM ANALYZE;
```
Adding 'origins' tag for versioning the pull date from iNat
```sql
ALTER TABLE taxa ADD COLUMN origin VARCHAR(255);
ALTER TABLE observers ADD COLUMN origin VARCHAR(255);
ALTER TABLE observations ADD COLUMN origin VARCHAR(255);
ALTER TABLE photos ADD COLUMN origin VARCHAR(255);

UPDATE taxa SET origin = 'iNat-June2023';
UPDATE observers SET origin = 'iNat-June2023';
UPDATE observations SET origin = 'iNat-June2023';
UPDATE photos SET origin = 'iNat-June2023';

CREATE INDEX index_taxa_origins ON taxa USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_taxa_name ON taxa USING GIN (to_tsvector('simple', name));
CREATE INDEX index_observers_origins ON observers USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_observations_origins ON observations USING GIN (to_tsvector('simple', origin));
CREATE INDEX index_photos_origins ON photos USING GIN (to_tsvector('simple', origin));
```

Add 'version' tag for version with the ibrida-pulls schema (used downstream).
```sql
ALTER TABLE photos ADD COLUMN version VARCHAR(255);
ALTER TABLE observations ADD COLUMN version VARCHAR(255);
ALTER TABLE observers ADD COLUMN version VARCHAR(255);
ALTER TABLE taxa ADD COLUMN version VARCHAR(255);


UPDATE photos SET version = 'v1';
UPDATE observations SET version = 'v1';
UPDATE observers SET version = 'v1';
UPDATE taxa SET version = 'v1';

CREATE INDEX index_photos_version ON photos USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observations_version ON observations USING GIN (to_tsvector('simple', version));
CREATE INDEX index_observers_version ON observers USING GIN (to_tsvector('simple', version));
CREATE INDEX index_taxa_version ON taxa USING GIN (to_tsvector('simple', version));

-- Verify
SELECT DISTINCT version FROM photos;
SELECT DISTINCT version FROM observations;
```

Explicitly set primary keys on master tables:
```sql
-- Add primary key to observations
ALTER TABLE observations
ADD CONSTRAINT observations_pkey PRIMARY KEY (observation_uuid);

-- Add primary key to photos
ALTER TABLE photos
ADD CONSTRAINT photos_pkey PRIMARY KEY (photo_uuid, photo_id, position, observation_uuid);

-- Add primary key to observers
ALTER TABLE observers
ADD CONSTRAINT observers_pkey PRIMARY KEY (observer_id);

-- Add primary key to taxa
ALTER TABLE taxa
ADD CONSTRAINT taxa_pkey PRIMARY KEY (taxon_id);


--- Inspect
----- See primary key definition
SELECT conname AS constraint_name, 
       pg_get_constraintdef(c.oid) AS constraint_definition
FROM   pg_constraint c
JOIN   pg_namespace n ON n.oid = c.connamespace
JOIN   pg_class cl ON cl.oid = c.conrelid
WHERE  cl.relname = 'photos' AND c.contype = 'p';
------- Why photos table "position" col surrounded by quotes?

----
Full Path: ingest/v0/r0/geom.sh

#!/bin/bash

# Function to run the update in parallel
run_update() {
  local OFFSET=$1
  local LIMIT=$2
  local DB_NAME=$3
  local TABLE_NAME=$4

  docker exec ibrida psql -U postgres -d "${DB_NAME}" -c "
  UPDATE ${TABLE_NAME}
  SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)::public.geometry
  WHERE observation_uuid IN (
    SELECT observation_uuid
    FROM ${TABLE_NAME}
    ORDER BY observation_uuid
    OFFSET ${OFFSET}
    LIMIT ${LIMIT}
  );"
}

# Check if correct number of arguments are provided
if [ "$#" -ne 4 ]; then
  echo "Usage: $0 <database_name> <table_name> <num_workers> <base_dir>"
  exit 1
fi

# Define arguments
DB_NAME=$1
TABLE_NAME=$2
NUM_PROCESSES=$3
BASE_DIR=$4

# Calculate total rows and batch size
TOTAL_ROWS=$(docker exec ibrida psql -U postgres -d "${DB_NAME}" -t -c "SELECT COUNT(*) FROM ${TABLE_NAME};")
BATCH_SIZE=$((TOTAL_ROWS / NUM_PROCESSES))

# Run updates in parallel
for ((i=0; i<NUM_PROCESSES; i++)); do
  OFFSET=$((i * BATCH_SIZE))
  run_update ${OFFSET} ${BATCH_SIZE} ${DB_NAME} ${TABLE_NAME} &
done

# Wait for all processes to finish
wait
echo "All updates completed."


----
Full Path: ingest/v0/r1/vers_origin.sh



----
Full Path: ingest/v0/r1/ingest.sh



----
Full Path: ingest/v0/r1/init.md



----
Full Path: ingest/v0/r1/geom.sh



----
Full Path: export/v0/regional_base.sh

#!/bin/bash

# Use the variables passed from the wrapper script
DB_USER="${DB_USER}"
DB_NAME="${DB_NAME}"
REGION_TAG="${REGION_TAG}"
MIN_OBS="${MIN_OBS}"
DB_CONTAINER="${DB_CONTAINER}"

# Function to set region-specific coordinates
set_region_coordinates() {
  case "$REGION_TAG" in
    "NAfull")
      XMIN=-169.453125
      YMIN=12.211180
      XMAX=-23.554688
      YMAX=84.897147
      ;;
    "EURwest")
      XMIN=-12.128906
      YMIN=40.245992
      XMAX=12.480469
      YMAX=60.586967
      ;;
    "EURnorth")
      XMIN=-25.927734
      YMIN=54.673831
      XMAX=45.966797
      YMAX=71.357067
      ;;
    "EUReast")
      XMIN=10.722656
      YMIN=41.771312
      XMAX=39.550781
      YMAX=59.977005
      ;;
    "EURfull")
      XMIN=-30.761719
      YMIN=33.284620
      XMAX=43.593750
      YMAX=72.262310
      ;;
    "MED")
      XMIN=-16.259766
      YMIN=29.916852
      XMAX=36.474609
      YMAX=46.316584
      ;;
    "AUSfull")
      XMIN=111.269531
      YMIN=-47.989922
      XMAX=181.230469
      YMAX=-9.622414
      ;;
    "ASIAse")
      XMIN=82.441406
      YMIN=-11.523088
      XMAX=153.457031
      YMAX=28.613459
      ;;
    "ASIAeast")
      XMIN=462.304688
      YMIN=23.241346
      XMAX=550.195313
      YMAX=78.630006
      ;;
    "ASIAcentral")
      XMIN=408.515625
      YMIN=36.031332
      XMAX=467.753906
      YMAX=76.142958
      ;;
    "ASIAsouth")
      XMIN=420.468750
      YMIN=1.581830
      XMAX=455.097656
      YMAX=39.232253
      ;;
    "ASIAsw")
      XMIN=386.718750
      YMIN=12.897489
      XMAX=423.281250
      YMAX=48.922499
      ;;
    "ASIA_nw")
      XMIN=393.046875
      YMIN=46.800059
      XMAX=473.203125
      YMAX=81.621352
      ;;
    "SAfull")
      XMIN=271.230469
      YMIN=-57.040730
      XMAX=330.644531
      YMAX=15.114553
      ;;
    "AFRfull")
      XMIN=339.082031
      YMIN=-37.718590
      XMAX=421.699219
      YMAX=39.232253
      ;;
    *)
      echo "Unknown REGION_TAG: $REGION_TAG"
      exit 1
      ;;
  esac
}

# Set region coordinates
set_region_coordinates

# Function to execute SQL commands
execute_sql() {
  local sql="$1"
  docker exec "$DB_CONTAINER" psql -U "$DB_USER" -d "$DB_NAME" -c "$sql"
}

# Function to print progress
print_progress() {
  local message="$1"
  echo "======================================"
  echo "$message"
  echo "======================================"
}

# Drop existing tables if they exist
drop_existing_tables() {
  execute_sql "DROP TABLE IF EXISTS ${REGION_TAG}_min${MIN_OBS}_all_taxa;"
  execute_sql "DROP TABLE IF EXISTS ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs;"
}

# Drop tables if they exist
drop_existing_tables

# Create table <REGION_TAG>_min<MIN_OBS>_all_taxa
print_progress "Creating table ${REGION_TAG}_min${MIN_OBS}_all_taxa"
execute_sql "
BEGIN;

CREATE TABLE ${REGION_TAG}_min${MIN_OBS}_all_taxa AS
SELECT  
    DISTINCT observations.taxon_id  
FROM  
    observations 
WHERE 
    observations.observation_uuid = ANY (
        SELECT observations.observation_uuid
        FROM observations
        JOIN taxa ON observations.taxon_id = taxa.taxon_id
        WHERE 
            NOT (taxa.rank_level = 10 AND observations.quality_grade != 'research')
            AND observations.latitude BETWEEN ${YMIN} AND ${YMAX}
            AND observations.longitude BETWEEN ${XMIN} AND ${XMAX}
    )
    AND observations.taxon_id IN (
        SELECT observations.taxon_id
        FROM observations
        GROUP BY observations.taxon_id
        HAVING COUNT(observation_uuid) >= ${MIN_OBS}
    );

COMMIT;
"

# Create table <REGION_TAG>_min<MIN_OBS>_all_taxa_obs (NO VERSION, ORIGIN)
print_progress "Creating table ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs"
execute_sql "
BEGIN;

CREATE TABLE ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs AS
SELECT  
    observation_uuid, observer_id, latitude, longitude, positional_accuracy, taxon_id, quality_grade,  
    observed_on
FROM
    observations
WHERE
    taxon_id IN (
        SELECT taxon_id
        FROM ${REGION_TAG}_min${MIN_OBS}_all_taxa
    );

COMMIT;
"

print_progress "Regional base tables created"

# NOTE: Below is the original query that included version and origin.
# # Create table <REGION_TAG>_min<MIN_OBS>_all_taxa_obs
# print_progress "Creating table ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs"
# execute_sql "
# BEGIN;

# CREATE TABLE ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs AS
# SELECT  
#     observation_uuid, observer_id, latitude, longitude, positional_accuracy, taxon_id, quality_grade,  
#     observed_on, origin, version
# FROM
#     observations
# WHERE
#     taxon_id IN (
#         SELECT taxon_id
#         FROM ${REGION_TAG}_min${MIN_OBS}_all_taxa
#     );

# COMMIT;
# "

# print_progress "Regional base tables created"


----
Full Path: export/v0/README.md

Documentation for the new three-tier clade system:

Macroclade: High-level groupings anchored by a single root taxa node. Examples: arthropoda, aves.
Clade: Subsets within macroclades, still joined at a single root taxa node. Examples: insecta, arachnidae (within arthropoda).
Metaclade: Groupings of one or more clades, potentially crossing macroclade boundaries. Example: primary_terrestrial_arthropoda (includes insecta and arachnidae).

Preference hierarchy for table naming:

1. If a metaclade is specified, use the metaclade name.
2. If no metaclade but a clade is specified, use the clade name.
3. If neither metaclade nor clade is specified, use the macroclade name.

This system allows for flexible and precise control over taxa groupings while maintaining backward compatibility with the existing macroclade-level exports.

----
Full Path: export/v0/wrapper_r1.sh



----
Full Path: export/v0/wrapper_r0.sh

#!/bin/bash

# Define variables
### primary_terrestrial_arthropoda, v0, primary_only_50min_3000max, iNat-June2024, NAfull
# DB_USER="postgres"
# VERSION_VALUE="v0"
# ORIGIN_VALUE="iNat-June2024"
# DB_NAME="ibrida-${VERSION_VALUE}"
# REGION_TAG="NAfull"
# MIN_OBS=50
# MAX_RN=3000
# PRIMARY_ONLY=true  # Set this to true to select only primary photos (position == 0)
# EXPORT_GROUP="primary_terrestrial_arthropoda"  # Metaclade to export
# PROCESS_OTHER=false  # Set to true if you want to process the 'other' group
# EXPORT_SUBDIR="${ORIGIN_VALUE}/${VERSION_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"  # Subdirectory for CSV exports
# DB_CONTAINER="fast-ibrida-1"  # Update this to your container name
# HOST_EXPORT_BASE_PATH="/pond/Polli/ibridaExports"
# CONTAINER_EXPORT_BASE_PATH="/exports"

### amphibia, v0, primary_only_400min_1000max, iNat-June2024, NAfull
# DB_USER="postgres"
# VERSION_VALUE="v0"
# ORIGIN_VALUE="iNat-June2024"
# DB_NAME="ibrida-${VERSION_VALUE}"
# REGION_TAG="NAfull"
# MIN_OBS=400
# MAX_RN=1000
# PRIMARY_ONLY=true  # Set this to true to select only primary photos (position == 0)
# EXPORT_GROUP="amphibia"  # Metaclade to export
# PROCESS_OTHER=false  # Set to true if you want to process the 'other' group
# EXPORT_SUBDIR="${ORIGIN_VALUE}/${VERSION_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"  # Subdirectory for CSV exports
# DB_CONTAINER="fast-ibrida-1"  # Update this to your container name
# HOST_EXPORT_BASE_PATH="/pond/Polli/ibridaExports"
# CONTAINER_EXPORT_BASE_PATH="/exports"

## exporting to banana for fast testing (espec while pond is offline)
DB_USER="postgres"
VERSION_VALUE="v0"
ORIGIN_VALUE="iNat-June2024"
DB_NAME="ibrida-${VERSION_VALUE}"
REGION_TAG="NAfull"
MIN_OBS=1000
MAX_RN=1500
PRIMARY_ONLY=true  # Set this to true to select only primary photos (position == 0)
EXPORT_GROUP="primary_terrestrial_arthropoda"  # Metaclade to export
PROCESS_OTHER=false  # Set to true if you want to process the 'other' group
EXPORT_SUBDIR="${ORIGIN_VALUE}/${VERSION_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"  # Subdirectory for CSV exports
DB_CONTAINER="ibrida-noPond"  # Update this to your container name
HOST_EXPORT_BASE_PATH="/banana/miniH5/ibrida_exports"
CONTAINER_EXPORT_BASE_PATH="/exports"

# Paths to the scripts
REGIONAL_BASE_SCRIPT="/home/caleb/repo/ibridaDB/dbTools/export/v0/regional_base.sh"
CLADISTIC_SCRIPT="/home/caleb/repo/ibridaDB/dbTools/export/v0/cladistic.sh"

# Function to execute a script and check its success
execute_script() {
  local script="$1"
  if ! bash "$script"; then
    echo "Error: Script $script failed."
    exit 1
  fi
}

# Set permissions before running scripts
docker exec "$DB_CONTAINER" bash -c "chmod -R 777 $CONTAINER_EXPORT_BASE_PATH && chown -R postgres:postgres $CONTAINER_EXPORT_BASE_PATH"
echo "Permissions set for $CONTAINER_EXPORT_BASE_PATH"

# Export variables to be used by the child scripts
export DB_USER VERSION_VALUE ORIGIN_VALUE DB_NAME REGION_TAG MIN_OBS MAX_RN PRIMARY_ONLY EXPORT_SUBDIR DB_CONTAINER HOST_EXPORT_BASE_PATH CONTAINER_EXPORT_BASE_PATH EXPORT_GROUP PROCESS_OTHER

# Execute the regional_base.sh script
# NOTE: Commented as we already ran this successfully.
# execute_script "$REGIONAL_BASE_SCRIPT"

# Execute the cladistic.sh script
execute_script "$CLADISTIC_SCRIPT"

echo "All scripts executed successfully."

# NOTE: Version and origin temporarily removed from export tables.
# NOTE: Version still used for export path.

# Display summary information
echo "Export Summary:"
echo "---------------"
echo "Database: $DB_NAME"
echo "Export Group: $EXPORT_GROUP"
echo "Region: $REGION_TAG"
echo "Minimum Observations: $MIN_OBS"
echo "Maximum Random Number: $MAX_RN"
echo "Primary Photos Only: $PRIMARY_ONLY"
echo "Process 'Other' Group: $PROCESS_OTHER"
echo "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Check if export was successful
if [ -f "${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}/${EXPORT_GROUP}_photos.csv" ]; then
    echo "Export successful. CSV file created: ${EXPORT_GROUP}_photos.csv"
else
    echo "Warning: CSV file not found. Export may have failed."
fi

echo "For detailed export information, please check the export_summary.txt file in the export directory."

----
Full Path: /home/caleb/repo/ibridaDB/docker/stausee/docker-compose.yml

services:
  ibrida:
    image: postgis/postgis:15-3.3
    user: "998:998"
    shm_size: '16g'
    environment:
      POSTGRES_PASSWORD: ooglyboogly69
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_SHARED_BUFFERS: 8GB
      POSTGRES_WORK_MEM: 2048MB
      POSTGRES_MAINTENANCE_WORK_MEM: 4GB
    volumes:
      - ~/repo/ibridaDB/dbTools:/tool
      - ~/repo/ibridaDB/dbQueries:/query
      - /database/ibridaDB:/var/lib/postgresql/data
      - /datasets/ibrida-data/exports:/exports
      - /datasets/ibrida-data/intake:/metadata
    ports:
      - "5432:5432"
    container_name: ibridaDB

----
Full Path: /home/caleb/repo/ibridaDB/docker/stausee/entrypoint.sh

#!/bin/bash
set -e

# Just log and exit - let Docker's default entrypoint handle PostgreSQL
echo "Entrypoint script executed at $(date)"

