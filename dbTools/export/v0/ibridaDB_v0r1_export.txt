Directory tree, stemming from root "/home/caleb/repo/ibridaDB/dbTools/export/v0":
├── common (618 lines)
│   ├── clade_defns.sh (171)
│   ├── cladistic.sh (134)
│   ├── functions.sh (52)
│   ├── main.sh (62)
│   └── regional_base.sh (199)
└── r1 (59)
    │   └── wrapper.sh (59)
----
----
Full Path: common/main.sh

#!/bin/bash

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Validate required variables
required_vars=(
    "DB_USER" "VERSION_VALUE" "RELEASE_VALUE" "ORIGIN_VALUE" 
    "DB_NAME" "REGION_TAG" "MIN_OBS" "MAX_RN" 
    "DB_CONTAINER" "HOST_EXPORT_BASE_PATH" "CONTAINER_EXPORT_BASE_PATH"
    "EXPORT_GROUP"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Create export directory structure
print_progress "Creating export directory structure"
EXPORT_DIR="${CONTAINER_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
HOST_EXPORT_DIR="${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Create host directory with proper permissions
ensure_directory "${HOST_EXPORT_DIR}"

# Create PostgreSQL extension and role if needed
execute_sql "
DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS dblink;
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'exportuser') THEN
        CREATE ROLE exportuser;
    END IF;
END \$\$;"

# Run regional base creation (source functions first)
print_progress "Creating regional base tables"
source "${BASE_DIR}/common/regional_base.sh"
send_notification "${REGION_TAG} regional base tables created"

# Run cladistic filtering
print_progress "Applying cladistic filters"
source "${BASE_DIR}/common/cladistic.sh"
send_notification "${EXPORT_GROUP} cladistic filtering complete"

# Export summary
print_progress "Creating export summary"
cat > "${HOST_EXPORT_DIR}/export_summary.txt" << EOL
Export Summary
Version: ${VERSION_VALUE}
Release: ${RELEASE_VALUE}
Region: ${REGION_TAG}
Minimum Observations: ${MIN_OBS}
Maximum Random Number: ${MAX_RN}
Export Group: ${EXPORT_GROUP}
Date: $(date)
EOL

print_progress "Export process complete"

----
Full Path: common/regional_base.sh

#!/bin/bash

# Note: functions are sourced from main.sh

# Function to set region-specific coordinates
set_region_coordinates() {
  case "$REGION_TAG" in
    "NAfull")
      XMIN=-169.453125
      YMIN=12.211180
      XMAX=-23.554688
      YMAX=84.897147
      ;;
    "EURwest")
      XMIN=-12.128906
      YMIN=40.245992
      XMAX=12.480469
      YMAX=60.586967
      ;;
    "EURnorth")
      XMIN=-25.927734
      YMIN=54.673831
      XMAX=45.966797
      YMAX=71.357067
      ;;
    "EUReast")
      XMIN=10.722656
      YMIN=41.771312
      XMAX=39.550781
      YMAX=59.977005
      ;;
    "EURfull")
      XMIN=-30.761719
      YMIN=33.284620
      XMAX=43.593750
      YMAX=72.262310
      ;;
    "MED")
      XMIN=-16.259766
      YMIN=29.916852
      XMAX=36.474609
      YMAX=46.316584
      ;;
    "AUSfull")
      XMIN=111.269531
      YMIN=-47.989922
      XMAX=181.230469
      YMAX=-9.622414
      ;;
    "ASIAse")
      XMIN=82.441406
      YMIN=-11.523088
      XMAX=153.457031
      YMAX=28.613459
      ;;
    "ASIAeast")
      XMIN=462.304688
      YMIN=23.241346
      XMAX=550.195313
      YMAX=78.630006
      ;;
    "ASIAcentral")
      XMIN=408.515625
      YMIN=36.031332
      XMAX=467.753906
      YMAX=76.142958
      ;;
    "ASIAsouth")
      XMIN=420.468750
      YMIN=1.581830
      XMAX=455.097656
      YMAX=39.232253
      ;;
    "ASIAsw")
      XMIN=386.718750
      YMIN=12.897489
      XMAX=423.281250
      YMAX=48.922499
      ;;
    "ASIA_nw")
      XMIN=393.046875
      YMIN=46.800059
      XMAX=473.203125
      YMAX=81.621352
      ;;
    "SAfull")
      XMIN=271.230469
      YMIN=-57.040730
      XMAX=330.644531
      YMAX=15.114553
      ;;
    "AFRfull")
      XMIN=339.082031
      YMIN=-37.718590
      XMAX=421.699219
      YMAX=39.232253
      ;;
    *)
      echo "Unknown REGION_TAG: $REGION_TAG"
      exit 1
      ;;
  esac
}


# Set region coordinates
set_region_coordinates

# # Debug: Check version and release values
# print_progress "Debugging database parameters"
# execute_sql "
# SELECT DISTINCT version, release, count(*)
# FROM observations
# GROUP BY version, release;"

# # Debug: Check coordinate bounds
# print_progress "Checking observations within coordinate bounds"
# execute_sql "
# SELECT COUNT(*)
# FROM observations
# WHERE latitude BETWEEN ${YMIN} AND ${YMAX}
# AND longitude BETWEEN ${XMIN} AND ${XMAX};"

# # Debug: Check quality grade distribution
# print_progress "Checking quality grade distribution"
# execute_sql "
# SELECT quality_grade, COUNT(*)
# FROM observations
# WHERE version = '${VERSION_VALUE}'
# AND release = '${RELEASE_VALUE}'
# GROUP BY quality_grade;"

# Drop existing tables if they exist
print_progress "Dropping existing tables"
execute_sql "DROP TABLE IF EXISTS ${REGION_TAG}_min${MIN_OBS}_all_taxa CASCADE;"
execute_sql "DROP TABLE IF EXISTS ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs CASCADE;"

# Create table with debug output
print_progress "Creating table ${REGION_TAG}_min${MIN_OBS}_all_taxa with debug"
execute_sql "
CREATE TABLE ${REGION_TAG}_min${MIN_OBS}_all_taxa AS
WITH debug_counts AS (
    SELECT COUNT(*) as total_obs,
           COUNT(DISTINCT taxon_id) as unique_taxa
    FROM observations
    WHERE
    -- TEMPORARY HOTFIX: Commenting out version filters until bulk update is complete
    -- version = '${VERSION_VALUE}'
    -- AND release = '${RELEASE_VALUE}'
    -- AND 
    geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
)
SELECT * FROM debug_counts;

SELECT DISTINCT observations.taxon_id
FROM observations
JOIN taxa ON observations.taxon_id = taxa.taxon_id
WHERE 
    -- TEMPORARY HOTFIX: Commenting out version filters until bulk update is complete
    -- observations.version = '${VERSION_VALUE}'
    -- AND observations.release = '${RELEASE_VALUE}'
    -- AND 
    NOT (taxa.rank_level = 10 AND observations.quality_grade != 'research')
    AND geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
    AND observations.taxon_id IN (
        SELECT observations.taxon_id
        FROM observations
        WHERE 
        -- TEMPORARY HOTFIX: Commenting out version filters until bulk update is complete
        -- version = '${VERSION_VALUE}'
        -- AND release = '${RELEASE_VALUE}'
        -- AND 
        1=1
        GROUP BY observations.taxon_id
        HAVING COUNT(observation_uuid) >= ${MIN_OBS}
    );

# Create table <REGION_TAG>_min<MIN_OBS>_all_taxa_obs with dynamic columns
print_progress "Creating table ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs"
OBS_COLUMNS=$(get_obs_columns)

# Debug: show the columns being used
echo "Using columns: ${OBS_COLUMNS}"

execute_sql "
CREATE TABLE ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs AS
SELECT ${OBS_COLUMNS}
FROM observations
WHERE 
    -- TEMPORARY HOTFIX: Commenting out version filters until bulk update is complete
    -- version = '${VERSION_VALUE}'
    -- AND release = '${RELEASE_VALUE}'
    -- AND 
    taxon_id IN (
        SELECT taxon_id
        FROM ${REGION_TAG}_min${MIN_OBS}_all_taxa
    );"

print_progress "Regional base tables created"

----
Full Path: common/functions.sh

#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

get_obs_columns() {
    # Start with standard columns
    local cols="observation_uuid, observer_id, latitude, longitude, positional_accuracy, taxon_id, quality_grade, observed_on"
    
    # TEMPORARY HOTFIX: Commenting out version tracking columns until bulk update is complete
    # Add version tracking columns
    # cols="${cols}, origin, version, release"
    
    # Check if anomaly_score exists in this release
    if [[ "${RELEASE_VALUE}" == "r1" ]]; then
        cols="${cols}, anomaly_score"
    fi
    
    echo "$cols"
}

# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    mkdir -p "${dir}"
    chmod -R 777 "${dir}"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    curl -d "$message" polliserve:8089/ibridaDB
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f get_obs_columns
export -f ensure_directory
export -f send_notification

----
Full Path: common/clade_defns.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# clade_defns.sh
# ------------------------------------------------------------------------------
# This file defines the integer-based filtering expressions for macroclades,
# clades, and metaclades, referencing columns in "expanded_taxa".
#
# Usage:
#   source clade_defns.sh
#   Then pick a macroclade (MACROCLADE="..."), or a clade (CLADE="..."),
#   or a metaclade (METACLADE="...") in your environment, and the
#   cladistic.sh script will build a condition from one of the arrays below.
#
# Example:
#   MACROCLADES["arthropoda"]='("L60_taxonID" = 47119)'
#   CLADES["insecta"]='("L50_taxonID" = 47120)'
#   METACLADES["primary_terrestrial_arthropoda"]='("L50_taxonID" = 47120 OR "L50_taxonID" = 101885)'
#
# Be sure to substitute the correct taxonIDs for your local database!
# ------------------------------------------------------------------------------

# ---[ Macroclades ]-----------------------------------------------------------
# Typically for kingdom-level (L70) or phylum-level (L60) anchors.

declare -A MACROCLADES

# 1) Arthropoda => phylum at L60 = 47120
MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'

# 2) Chordata => phylum at L60 = 2
MACROCLADES["chordata"]='("L60_taxonID" = 2)'

# 3) Plantae => kingdom at L70 = 47126
MACROCLADES["plantae"]='("L70_taxonID" = 47126)'

# 4) Fungi => kingdom at L70 = 47170
MACROCLADES["fungi"]='("L70_taxonID" = 47170)'

# (Optional) If you consider Actinopterygii, Mammalia, Reptilia, etc.
# to be "macroclades," you may define them here instead of in CLADES.
# For instance:
#   MACROCLADES["mammalia"]='("L50_taxonID" = 40151)'


# ---[ Clades ]----------------------------------------------------------------
# Typically for class-level (L50), order-level (L40), or narrower taxonomic groups.
declare -A CLADES

# -- Class-level (L50) Examples --

# 1) Actinopterygii => L50 = 47178
CLADES["actinopterygii"]='("L50_taxonID" = 47178)'

# 2) Amphibia => L50 = 20978
CLADES["amphibia"]='("L50_taxonID" = 20978)'

# 3) Arachnida => L50 = 47119
CLADES["arachnida"]='("L50_taxonID" = 47119)'

# 4) Aves => L50 = 3
CLADES["aves"]='("L50_taxonID" = 3)'

# 5) Insecta => L50 = 47158
CLADES["insecta"]='("L50_taxonID" = 47158)'

# 6) Mammalia => L50 = 40151
CLADES["mammalia"]='("L50_taxonID" = 40151)'

# 7) Reptilia => L50 = 26036
CLADES["reptilia"]='("L50_taxonID" = 26036)'


# -- Order-level (L40) Examples --

# 1) Testudines => L40 = 39532
CLADES["testudines"]='("L40_taxonID" = 39532)'

# 2) Crocodylia => L40 = 26039
CLADES["crocodylia"]='("L40_taxonID" = 26039)'

# 3) Coleoptera => L40 = 47208
CLADES["coleoptera"]='("L40_taxonID" = 47208)'

# 4) Lepidoptera => L40 = 47157
CLADES["lepidoptera"]='("L40_taxonID" = 47157)'

# 5) Hymenoptera => L40 = 47201
CLADES["hymenoptera"]='("L40_taxonID" = 47201)'

# 6) Hemiptera => L40 = 47744
CLADES["hemiptera"]='("L40_taxonID" = 47744)'

# 7) Orthoptera => L40 = 47651
CLADES["orthoptera"]='("L40_taxonID" = 47651)'

# 8) Odonata => L40 = 47792
CLADES["odonata"]='("L40_taxonID" = 47792)'

# 9) Diptera => L40 = 47822
CLADES["diptera"]='("L40_taxonID" = 47822)'


# -- Additional Named Groups (Suborders, Clade Subsets, etc.) --

# Pterygota => The DB shows two taxonIDs (184884, 418641) plus
# another entry with L40_taxonID=48796. We combine them with OR:
CLADES["pterygota"]='("taxonID" = 184884 OR "taxonID" = 418641 OR "L40_taxonID" = 48796)'

# Phasmatodea => Not found in your query results. If/when you know its ID,
# you can fill it in here:
# CLADES["phasmatodea"]='("L40_taxonID" = ???)'

# Subclades within Hymenoptera (all share L40_taxonID=47201).
# Typically, referencing the top-level order is "hymenoptera"
# while these might be more specific anchor taxa:
CLADES["aculeata"]='("taxonID" = 326777)'
CLADES["apoidea"]='("taxonID" = 47222)'
CLADES["formicidae"]='("taxonID" = 47336)'
CLADES["vespoidea"]='("taxonID" = 48740)'
CLADES["vespidae"]='("taxonID" = 52747)'


# ---[ Metaclades ]------------------------------------------------------------
# Multi-root or cross-macroclade definitions. Compose bigger groups using OR/AND.

declare -A METACLADES

# Example 1: terrestrial_arthropods => Insecta OR Arachnida OR others.
# (Using the taxonIDs from the CLADES above.)
METACLADES["terrestrial_arthropods"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'

# Example 2: flying_vertebrates => Birds (aves) OR Bats (chiroptera)
# Suppose chiroptera => L40=7721 (if that’s valid in your DB).
METACLADES["flying_vertebrates"]='("L50_taxonID" = 3 OR "L40_taxonID" = 7721)'

# Example 3: nonavian_reptiles => reptilia minus birds. You might do:
# METACLADES["nonavian_reptiles"]='("L50_taxonID" = 26036 AND "L50_taxonID" != 3)'


# ---[ Helper Function ]-------------------------------------------------------
# Picks the correct expression given environment variables.
function get_clade_condition() {
  local condition

  # 1) If METACLADE is set (and found in METACLADES), return that
  if [[ -n "${METACLADE}" && -n "${METACLADES[${METACLADE}]}" ]]; then
    condition="${METACLADES[${METACLADE}]}"
    echo "${condition}"
    return
  fi

  # 2) Else if CLADE is set
  if [[ -n "${CLADE}" && -n "${CLADES[${CLADE}]}" ]]; then
    condition="${CLADES[${CLADE}]}"
    echo "${condition}"
    return
  fi

  # 3) Else if MACROCLADE is set
  if [[ -n "${MACROCLADE}" && -n "${MACROCLADES[${MACROCLADE}]}" ]]; then
    condition="${MACROCLADES[${MACROCLADE}]}"
    echo "${condition}"
    return
  fi

  # 4) Fallback: no recognized key => no filter
  echo "TRUE"
}

export -f get_clade_condition


----
Full Path: common/cladistic.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# cladistic.sh
# ------------------------------------------------------------------------------
# Creates filtered observation subsets for a specified clade/metaclade,
# referencing the "expanded_taxa" table instead of the old recursive string method.
#
# Usage pattern:
#   1) Environment variables set by an upstream wrapper or main script:
#      - METACLADE=...
#      - CLADE=...
#      - MACROCLADE=...
#      - DB_CONTAINER, DB_USER, DB_NAME
#      - REGION_TAG, MIN_OBS, ...
#   2) We load "clade_defns.sh" to define which integer columns to match
#      (e.g. "L50_taxonID" = 47120).
#   3) We create a final table <group>_observations joined to expanded_taxa
#      so only active, recognized taxonIDs appear.
#   4) We then do an export to CSV with a random subset of photos, etc.
#
# The environment variable "EXPORT_GROUP" is used to name the final table
# and output CSV. This is typically set to your clade/metaclade name
# (e.g. "primary_terrestrial_arthropoda").
#
# ------------------------------------------------------------------------------

# Load shared functions
# (execute_sql, print_progress, get_obs_columns, etc.)
source "${BASE_DIR}/common/functions.sh"

# Load the new clade definitions
source "${BASE_DIR}/common/clade_defns.sh"

# Construct the final condition from environment variables
CLADE_CONDITION="$(get_clade_condition)"

print_progress "Creating filtered tables for ${EXPORT_GROUP}"

# We'll build the final table name, e.g. "primary_terrestrial_arthropoda_observations"
TABLE_NAME="${EXPORT_GROUP}_observations"

# We rely on the region-based table built in regional_base.sh:
#   <REGION_TAG>_min${MIN_OBS}_all_taxa_obs
# which is typically "NAfull_min50_all_taxa_obs", etc.
REGIONAL_TABLE="${REGION_TAG}_min${MIN_OBS}_all_taxa_obs"

# The columns we'll select from the region table
OBS_COLUMNS="$(get_obs_columns)"

# 1) Drop old table if it exists
execute_sql "
DROP TABLE IF EXISTS \"${TABLE_NAME}\" CASCADE;
"

# 2) Create new table by joining to expanded_taxa
#    so that we skip any taxon_id not found in expanded_taxa (i.e., inactive or missing).
send_notification "Joining regional table ${REGIONAL_TABLE} to expanded_taxa"
print_progress "Joining regional table ${REGIONAL_TABLE} to expanded_taxa"
execute_sql "
CREATE TABLE \"${TABLE_NAME}\" AS
SELECT ${OBS_COLUMNS}
FROM \"${REGIONAL_TABLE}\" obs
JOIN \"expanded_taxa\" e
   ON e.\"taxonID\" = obs.taxon_id
WHERE e.\"taxonActive\" = TRUE
  AND ${CLADE_CONDITION};
"

# 3) Export to CSV (random subset w/ photos)
send_notification "Exporting filtered observations"
print_progress "Exporting filtered observations"

if [ "${PRIMARY_ONLY}" = true ]; then
    # Photos with position=0, quality_grade='research'
    execute_sql "\COPY (
        SELECT o.*,
               p.photo_uuid,
               p.photo_id,
               p.extension,
               p.license,
               p.width,
               p.height,
               p.position
        FROM \"${TABLE_NAME}\" o
        JOIN photos p ON o.observation_uuid = p.observation_uuid
        WHERE p.position = 0
          AND o.quality_grade = 'research'
        ORDER BY random()
        LIMIT ${MAX_RN}
    ) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
      WITH CSV HEADER DELIMITER E'\t';"
else
    # All photos for the final set, restricted to quality_grade='research'
    execute_sql "\COPY (
        SELECT o.*,
               p.photo_uuid,
               p.photo_id,
               p.extension,
               p.license,
               p.width,
               p.height,
               p.position
        FROM \"${TABLE_NAME}\" o
        JOIN photos p ON o.observation_uuid = p.observation_uuid
        WHERE o.quality_grade = 'research'
        ORDER BY random()
        LIMIT ${MAX_RN}
    ) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
      WITH CSV HEADER DELIMITER E'\t';"
fi

# 4) Summarize exported data
print_progress "Creating export statistics"
STATS=$(execute_sql "
WITH export_stats AS (
    SELECT 
        COUNT(DISTINCT observation_uuid) as num_observations,
        COUNT(DISTINCT taxon_id) as num_taxa,
        COUNT(DISTINCT observer_id) as num_observers
    FROM \"${TABLE_NAME}\"
)
SELECT format(
    'Exported Data Statistics:
    Observations: %s
    Unique Taxa: %s
    Unique Observers: %s',
    num_observations, num_taxa, num_observers
)
FROM export_stats;")

echo "${STATS}" >> "${HOST_EXPORT_DIR}/export_summary.txt"

print_progress "Cladistic filtering complete"


----
Full Path: r1/wrapper.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=4000
export PRIMARY_ONLY=true
export METACLADE="primary_terrestrial_arthropoda"
export EXPORT_GROUP="${METACLADE}"
export PROCESS_OTHER=false
log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: ../../../docker/stausee/docker-compose.yml

services:
  ibrida:
    image: postgis/postgis:15-3.3
    user: "998:998"
    shm_size: '16g'
    environment:
      POSTGRES_PASSWORD: ooglyboogly69
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_SHARED_BUFFERS: 8GB
      POSTGRES_WORK_MEM: 2048MB
      POSTGRES_MAINTENANCE_WORK_MEM: 4GB
    volumes:
      - ~/repo/ibridaDB/dbTools:/tool
      - ~/repo/ibridaDB/dbQueries:/query
      - /database/ibridaDB:/var/lib/postgresql/data
      - /datasets/ibrida-data/exports:/exports
      - /datasets/ibrida-data/intake:/metadata
    ports:
      - "5432:5432"
    container_name: ibridaDB

----
Full Path: ../../../docker/stausee/entrypoint.sh

#!/bin/bash
set -e

# Just log and exit - let Docker's default entrypoint handle PostgreSQL
echo "Entrypoint script executed at $(date)"

