Directory tree, stemming from root "/home/caleb/repo/ibridaDB/dbTools/export/v0":
├── common (729 lines)
│   ├── clade_defns.sh (179)
│   ├── cladistic.sh (217)
│   ├── functions.sh (52)
│   ├── main.sh (111)
│   └── regional_base.sh (170)
├── export.md (142)
└── r1 (248)
    │   ├── wrapper.sh (62)
    │   ├── wrapper_amphibia.sh (62)
    │   ├── wrapper_angiospermae.sh (62)
    │   └── wrapper_aves.sh (62)
----
----
Full Path: export.md

# ibridaDB Export Reference

This document describes the key parameters that drive the **v1** export process in our `ibridaDB` pipeline. The parameters are split into three logical groups, reflecting their usage and declaration in `wrapper.sh`: **Database Config**, **Export Parameters**, and **Paths**.

## 1. Database Config

These environment variables specify how the export scripts connect to the database and handle versioning.

### `DB_USER`
- **Description**: The database user that executes SQL commands.
- **Default**: `"postgres"`

### `VERSION_VALUE`
- **Description**: The string identifier for the data version being exported (e.g., `"v0"`).  
- **Usage**: Combined with `RELEASE_VALUE` to construct the `DB_NAME`, and also embedded in logs or summary files.

### `RELEASE_VALUE`
- **Description**: Further disambiguates the release within the version (e.g., `"r1"`).
- **Usage**: Combined with `VERSION_VALUE` to construct the `DB_NAME`; also used for conditional logic in `functions.sh` (e.g., deciding whether to append `anomaly_score` columns).

### `ORIGIN_VALUE`
- **Description**: Documents the source of the data (e.g., `"iNat-Dec2024"`).  
- **Usage**: Potentially used to label data, but currently only logged or stored in summary contexts.

### `DB_NAME`
- **Description**: The actual database name to which SQL commands are directed.  
- **Constructed**: Typically `"ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"` (e.g., `"ibrida-v0-r1"`).

---

## 2. Export Parameters

These variables control which data are included in the export and how they are sampled or filtered.

### `REGION_TAG`
- **Description**: Identifies a particular broad region (e.g., `"NAfull"`, `"EURfull"`) that will define bounding box coordinates.  
- **Usage**: Affected by the `set_region_coordinates()` function in `regional_base.sh`, which sets `XMIN`, `YMIN`, `XMAX`, `YMAX`.  
- **Example**: `"NAfull"` for North America bounding box.

### `MIN_OBS`
- **Description**: The minimum number of observations required for a species to be included in the regional tables.  
- **Intended Behavior**: 
  - In `regional_base.sh`, species (rank_level == 10) must have at least `MIN_OBS` **research-grade** observations to be included in `<REGION_TAG>_min${MIN_OBS}_all_taxa` and subsequent tables.  
  - Practically, the code checks for `HAVING COUNT(o2.observation_uuid) >= ${MIN_OBS}` and also filters out non-research-grade observations if `t.rank_level = 10`.  
- **Default**: `50`  
- **Note**: If a taxon is not strictly species rank, the code uses a looser condition, but for true species (rank_level=10), it must meet the threshold. 

### `MAX_RN`
- **Description**: The maximum number of **research-grade** observations that will be **sampled per species** in the final CSV export.  
- **Usage**:
  - In `cladistic.sh`, a partition-based random sampling ensures up to `MAX_RN` observations per species (`L10_taxonID`).  
  - Observations with `L10_taxonID IS NULL` are exempt from this cap (they are not strictly species-level).  
- **Default**: `4000`

### `PRIMARY_ONLY`
- **Description**: Controls whether to only export `position=0` (the “primary” or first) photo per observation, or *all* photos.  
- **Usage**:
  - In `cladistic.sh`, the `COPY` statements either require `p.position = 0` or allow all positions, based on `PRIMARY_ONLY=true/false`.  
- **Typical Values**: `true` (only primary photo) or `false` (all photos).

### `METACLADE`
- **Description**: A specialized grouping of clades, combining multiple lineages. Often something like `"primary_terrestrial_arthropoda"`.  
- **Usage**:
  - In `clade_defns.sh`, a `METACLADES["primary_terrestrial_arthropoda"] = ...` expression defines which taxonIDs to include.  
  - If `METACLADE` is set, it overrides `CLADE` or `MACROCLADE` when building the final condition.

### `EXPORT_GROUP`
- **Description**: The name or label for the final exported dataset (e.g., `"primary_terrestrial_arthropoda"`).  
- **Usage**:
  - Used to build the final table name (`<EXPORT_GROUP>_observations`) in `cladistic.sh`.  
  - Also appended to output CSV filenames and summary messages.

### `PROCESS_OTHER`
- **Description**: A boolean-like flag (default `false`) indicating whether to run additional extra steps.  
- **Usage**:
  - Not deeply integrated in the current code, but can be used to skip or include extra logic if set to `true`.

### `SKIP_REGIONAL_BASE`
- **Description**: Allows the user to **skip** dropping and recreating the regional base tables if they **already exist** and are **non-empty**.  
- **Usage**:
  - In `main.sh`, if `SKIP_REGIONAL_BASE=true`, the script checks if `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs` exists and has at least one row. If so, it skips calling `regional_base.sh`. Otherwise, it recreates the table as normal.  
  - Saves time when multiple clade exports reference the same region-based table.  
- **Default**: `false`

---

## 3. Paths

These variables handle filesystem or container paths for storing exports and controlling Docker context.

### `DB_CONTAINER`
- **Description**: The name of the Docker container running PostgreSQL (e.g., `"ibridaDB"`).  
- **Usage**:
  - `functions.sh` uses it in `execute_sql()` to run `docker exec ${DB_CONTAINER} ...`.

### `HOST_EXPORT_BASE_PATH`
- **Description**: The base path on the **host** for storing final exports (mapped into the container).  
- **Default**: `"/datasets/ibrida-data/exports"`.

### `CONTAINER_EXPORT_BASE_PATH`
- **Description**: The base path **inside** the Docker container that maps to `HOST_EXPORT_BASE_PATH`.  
- **Default**: `"/exports"`.

### `EXPORT_SUBDIR`
- **Description**: A dynamic subdirectory that includes the version, release, and other parameters (e.g., `"v0/r1/primary_only_50min_4000max"`).  
- **Usage**:
  - Combined with `HOST_EXPORT_BASE_PATH` for writing to disk from the host perspective, and with `CONTAINER_EXPORT_BASE_PATH` for writing to disk from within the container.  
  - E.g., final container path is `"$CONTAINER_EXPORT_BASE_PATH/$EXPORT_SUBDIR"`, final host path is `"$HOST_EXPORT_BASE_PATH/$EXPORT_SUBDIR"`.

### `BASE_DIR`
- **Description**: The top-level directory for the export scripts inside the container (e.g., `"/home/caleb/repo/ibridaDB/dbTools/export/v0"`).  
- **Usage**:  
  - Scripts in `wrapper.sh` or `main.sh` reference subdirectories within `BASE_DIR`, e.g., `BASE_DIR/common/functions.sh` or `BASE_DIR/common/regional_base.sh`.  

---

## Flow of Parameter Usage

1. **Wrapper Script** (`wrapper.sh`):  
   - Defines the environment variables documented above.  
   - Logs them and calls `main.sh`.

2. **Main Script** (`main.sh`):  
   - Validates required variables.  
   - Creates the export directory.  
   - **If `SKIP_REGIONAL_BASE=true`, checks if the existing region-based table is present and non-empty. Otherwise, or if the table is missing or empty, runs `regional_base.sh`.**  
   - Calls `cladistic.sh` to filter and export the final CSV.  
   - Generates a final `export_summary.txt`.

3. **regional_base.sh**:  
   - Uses `REGION_TAG` to set bounding box coordinates.  
   - Applies `MIN_OBS` to exclude species with fewer than the required number of research-grade observations.  
   - Produces `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs`.

4. **cladistic.sh**:  
   - Applies `METACLADE`, `CLADE`, or `MACROCLADE` logic to define taxonomic filters.  
   - Joins the regional table to `expanded_taxa`, producing `<EXPORT_GROUP>_observations`.  
   - Randomly samples up to `MAX_RN` observations **per species** (defined by `L10_taxonID`) in the final CSV, with `PRIMARY_ONLY` restricting photos if desired.

5. **export_per_species_snippet.sh** (Optional):  
   - A shortcut script to export a final CSV from an already-existing `<EXPORT_GROUP>_observations` table without re-creating upstream tables or re-running the entire flow.


----
Full Path: common/main.sh

#!/bin/bash

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Validate required variables
required_vars=(
    "DB_USER" "VERSION_VALUE" "RELEASE_VALUE" "ORIGIN_VALUE" 
    "DB_NAME" "REGION_TAG" "MIN_OBS" "MAX_RN" 
    "DB_CONTAINER" "HOST_EXPORT_BASE_PATH" "CONTAINER_EXPORT_BASE_PATH"
    "EXPORT_GROUP"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Create export directory structure
print_progress "Creating export directory structure"
EXPORT_DIR="${CONTAINER_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
HOST_EXPORT_DIR="${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Create host directory with proper permissions
ensure_directory "${HOST_EXPORT_DIR}"

# Create PostgreSQL extension and role if needed
execute_sql "
DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS dblink;
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'exportuser') THEN
        CREATE ROLE exportuser;
    END IF;
END \$\$;"

# ASSUMPTION: We assume the user won't skip if the table name is inconsistent with
# the desired region or MIN_OBS. We only do a basic check that the table exists
# and has rows. 
# ASSUMPTION: The user is responsible for ensuring that the existing table 
# matches the correct region and MIN_OBS setting if SKIP_REGIONAL_BASE=true.

if [ "${SKIP_REGIONAL_BASE}" = "true" ]; then
    print_progress "SKIP_REGIONAL_BASE=true: Checking existing tables..."

    # 1) Check if the table actually exists
    table_check=$(execute_sql "SELECT 1
      FROM pg_catalog.pg_tables
      WHERE schemaname = 'public'
        AND tablename = '${REGION_TAG}_min${MIN_OBS}_all_taxa_obs'
      LIMIT 1;")

    # Convert the output to something we can parse easily:
    # psql typically returns row headers, so we check if it contains "(1 row)" or "1"
    # For simplicity, we do a naive grep or check:
    if [[ "$table_check" =~ "1" ]]; then
        print_progress "Table ${REGION_TAG}_min${MIN_OBS}_all_taxa_obs found, checking row count..."

        row_count=$(execute_sql "SELECT count(*) as cnt FROM \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\";")
        
        # We'll parse the integer from row_count with a simple approach:
        # row_count might look like:
        #  cnt
        # ----
        #  452201
        # (1 row)
        # We can do a quick grep or awk:
        numeric=$(echo "$row_count" | awk '/[0-9]/{print $1}' | head -1)

        if [[ -n "$numeric" && "$numeric" -gt 0 ]]; then
            print_progress "Table has $numeric rows; skipping creation of regional base."
            send_notification "Skipped creating regional base; table is non-empty."
        else
            print_progress "Table is empty or row count could not be determined; re-creating..."
            source "${BASE_DIR}/common/regional_base.sh"
            send_notification "${REGION_TAG} regional base tables created"
        fi
    else
        print_progress "Table not found; re-creating the regional base."
        source "${BASE_DIR}/common/regional_base.sh"
        send_notification "${REGION_TAG} regional base tables created"
    fi
else
    # Run regional base creation (source functions first)
    print_progress "Creating regional base tables"
    source "${BASE_DIR}/common/regional_base.sh"
    send_notification "${REGION_TAG} regional base tables created"
fi

# Run cladistic filtering
print_progress "Applying cladistic filters"
source "${BASE_DIR}/common/cladistic.sh"
send_notification "${EXPORT_GROUP} cladistic filtering complete"

# Export summary
print_progress "Creating export summary"
cat > "${HOST_EXPORT_DIR}/export_summary.txt" << EOL
Export Summary
Version: ${VERSION_VALUE}
Release: ${RELEASE_VALUE}
Region: ${REGION_TAG}
Minimum Observations: ${MIN_OBS}
Maximum Random Number: ${MAX_RN}
Export Group: ${EXPORT_GROUP}
Date: $(date)
SKIP_REGIONAL_BASE: ${SKIP_REGIONAL_BASE}
EOL

print_progress "Export process complete"

----
Full Path: common/regional_base.sh

#!/bin/bash

# Note: functions are sourced from main.sh

# ------------------[ 1) Region Coordinates ]-------------------
set_region_coordinates() {
  case "$REGION_TAG" in
    "NAfull")
      XMIN=-169.453125
      YMIN=12.211180
      XMAX=-23.554688
      YMAX=84.897147
      ;;
    "EURwest")
      XMIN=-12.128906
      YMIN=40.245992
      XMAX=12.480469
      YMAX=60.586967
      ;;
    "EURnorth")
      XMIN=-25.927734
      YMIN=54.673831
      XMAX=45.966797
      YMAX=71.357067
      ;;
    "EUReast")
      XMIN=10.722656
      YMIN=41.771312
      XMAX=39.550781
      YMAX=59.977005
      ;;
    "EURfull")
      XMIN=-30.761719
      YMIN=33.284620
      XMAX=43.593750
      YMAX=72.262310
      ;;
    "MED")
      XMIN=-16.259766
      YMIN=29.916852
      XMAX=36.474609
      YMAX=46.316584
      ;;
    "AUSfull")
      XMIN=111.269531
      YMIN=-47.989922
      XMAX=181.230469
      YMAX=-9.622414
      ;;
    "ASIAse")
      XMIN=82.441406
      YMIN=-11.523088
      XMAX=153.457031
      YMAX=28.613459
      ;;
    "ASIAeast")
      XMIN=462.304688
      YMIN=23.241346
      XMAX=550.195313
      YMAX=78.630006
      ;;
    "ASIAcentral")
      XMIN=408.515625
      YMIN=36.031332
      XMAX=467.753906
      YMAX=76.142958
      ;;
    "ASIAsouth")
      XMIN=420.468750
      YMIN=1.581830
      XMAX=455.097656
      YMAX=39.232253
      ;;
    "ASIAsw")
      XMIN=386.718750
      YMIN=12.897489
      XMAX=423.281250
      YMAX=48.922499
      ;;
    "ASIA_nw")
      XMIN=393.046875
      YMIN=46.800059
      XMAX=473.203125
      YMAX=81.621352
      ;;
    "SAfull")
      XMIN=271.230469
      YMIN=-57.040730
      XMAX=330.644531
      YMAX=15.114553
      ;;
    "AFRfull")
      XMIN=339.082031
      YMIN=-37.718590
      XMAX=421.699219
      YMAX=39.232253
      ;;
    *)
      echo "Unknown REGION_TAG: $REGION_TAG"
      exit 1
      ;;
  esac
}

# Set region coordinates based on $REGION_TAG
set_region_coordinates

# ------------------[ 2) Drop Old Tables ]----------------------
print_progress "Dropping existing tables"

execute_sql "
DROP TABLE IF EXISTS \"${REGION_TAG}_min${MIN_OBS}_all_taxa\" CASCADE;
DROP TABLE IF EXISTS \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\" CASCADE;
"

# ------------------[ 3) Optional Debug Counts ]----------------
# If you want the debug counts as a separate table:
print_progress "Creating debug_counts for region ${REGION_TAG}"
execute_sql "
DROP TABLE IF EXISTS \"${REGION_TAG}_debug_counts\" CASCADE;

CREATE TABLE \"${REGION_TAG}_debug_counts\" AS
WITH debug_counts AS (
    SELECT
      COUNT(*) AS total_obs,
      COUNT(DISTINCT taxon_id) AS unique_taxa
    FROM observations
    WHERE
      -- version='${VERSION_VALUE}' AND ...
      geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
)
SELECT * FROM debug_counts;
"

# ------------------[ 4) Create _all_taxa Table ]---------------
print_progress "Creating table \"${REGION_TAG}_min${MIN_OBS}_all_taxa\""
execute_sql "
CREATE TABLE \"${REGION_TAG}_min${MIN_OBS}_all_taxa\" AS
SELECT DISTINCT o.taxon_id
FROM observations o
JOIN taxa t ON o.taxon_id = t.taxon_id
WHERE
  NOT (t.rank_level = 10 AND o.quality_grade != 'research')
  AND o.geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
  AND o.taxon_id IN (
      SELECT o2.taxon_id
      FROM observations o2
      GROUP BY o2.taxon_id
      HAVING COUNT(o2.observation_uuid) >= ${MIN_OBS}
  );
"

# ------------------[ 5) Create _all_taxa_obs Table ]-----------
print_progress "Creating table \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\""
OBS_COLUMNS=$(get_obs_columns)

echo "Using columns: ${OBS_COLUMNS}"

execute_sql "
CREATE TABLE \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\" AS
SELECT ${OBS_COLUMNS}
FROM observations
WHERE taxon_id IN (
    SELECT taxon_id
    FROM \"${REGION_TAG}_min${MIN_OBS}_all_taxa\"
);
"

print_progress "Regional base tables created"


----
Full Path: common/functions.sh

#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

get_obs_columns() {
    # Start with standard columns
    local cols="observation_uuid, observer_id, latitude, longitude, positional_accuracy, taxon_id, quality_grade, observed_on"
    
    # TEMPORARY HOTFIX: Commenting out version tracking columns until bulk update is complete
    # Add version tracking columns
    # cols="${cols}, origin, version, release"
    
    # Check if anomaly_score exists in this release
    if [[ "${RELEASE_VALUE}" == "r1" ]]; then
        cols="${cols}, anomaly_score"
    fi
    
    echo "$cols"
}

# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    mkdir -p "${dir}"
    chmod -R 777 "${dir}"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    curl -d "$message" polliserve:8089/ibridaDB
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f get_obs_columns
export -f ensure_directory
export -f send_notification

----
Full Path: common/clade_defns.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# clade_defns.sh
# ------------------------------------------------------------------------------
# This file defines the integer-based filtering expressions for macroclades,
# clades, and metaclades, referencing columns in "expanded_taxa".
#
# Usage:
#   source clade_defns.sh
#   Then pick a macroclade (MACROCLADE="..."), or a clade (CLADE="..."),
#   or a metaclade (METACLADE="...") in your environment, and the
#   cladistic.sh script will build a condition from one of the arrays below.
#
# Example:
#   MACROCLADES["arthropoda"]='("L60_taxonID" = 47119)'
#   CLADES["insecta"]='("L50_taxonID" = 47120)'
#   METACLADES["primary_terrestrial_arthropoda"]='("L50_taxonID" = 47120 OR "L50_taxonID" = 101885)'
#
# Be sure to substitute the correct taxonIDs for your local database!
# ------------------------------------------------------------------------------

# ---[ Macroclades ]-----------------------------------------------------------
# Typically for kingdom-level (L70) or phylum-level (L60) anchors.

declare -A MACROCLADES

# 1) Arthropoda => phylum at L60 = 47120
MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'

# 2) Chordata => phylum at L60 = 2
MACROCLADES["chordata"]='("L60_taxonID" = 2)'

# 3) Plantae => kingdom at L70 = 47126
MACROCLADES["plantae"]='("L70_taxonID" = 47126)'

# 4) Fungi => kingdom at L70 = 47170
MACROCLADES["fungi"]='("L70_taxonID" = 47170)'

# (Optional) If you consider Actinopterygii, Mammalia, Reptilia, etc.
# to be "macroclades," you may define them here instead of in CLADES.
# For instance:
#   MACROCLADES["mammalia"]='("L50_taxonID" = 40151)'


# ---[ Clades ]----------------------------------------------------------------
# Typically for class-level (L50), order-level (L40), or narrower taxonomic groups.
declare -A CLADES

# -- Plant Clades (Subphylum and Class levels) --
# -- Plant Subphylum (L57) --
CLADES["angiospermae"]='("L57_taxonID" = 47125)' # flowering plants

# -- Plant Classes (L50) --
CLADES["liliopsida"]='("L50_taxonID" = 47163)'    # monocots
CLADES["magnoliopsida"]='("L50_taxonID" = 47124)'  # dicots

# -- Class-level (L50) Examples --

# 1) Actinopterygii => L50 = 47178
CLADES["actinopterygii"]='("L50_taxonID" = 47178)'

# 2) Amphibia => L50 = 20978
CLADES["amphibia"]='("L50_taxonID" = 20978)'

# 3) Arachnida => L50 = 47119
CLADES["arachnida"]='("L50_taxonID" = 47119)'

# 4) Aves => L50 = 3
CLADES["aves"]='("L50_taxonID" = 3)'

# 5) Insecta => L50 = 47158
CLADES["insecta"]='("L50_taxonID" = 47158)'

# 6) Mammalia => L50 = 40151
CLADES["mammalia"]='("L50_taxonID" = 40151)'

# 7) Reptilia => L50 = 26036
CLADES["reptilia"]='("L50_taxonID" = 26036)'


# -- Order-level (L40) Examples --

# 1) Testudines => L40 = 39532
CLADES["testudines"]='("L40_taxonID" = 39532)'

# 2) Crocodylia => L40 = 26039
CLADES["crocodylia"]='("L40_taxonID" = 26039)'

# 3) Coleoptera => L40 = 47208
CLADES["coleoptera"]='("L40_taxonID" = 47208)'

# 4) Lepidoptera => L40 = 47157
CLADES["lepidoptera"]='("L40_taxonID" = 47157)'

# 5) Hymenoptera => L40 = 47201
CLADES["hymenoptera"]='("L40_taxonID" = 47201)'

# 6) Hemiptera => L40 = 47744
CLADES["hemiptera"]='("L40_taxonID" = 47744)'

# 7) Orthoptera => L40 = 47651
CLADES["orthoptera"]='("L40_taxonID" = 47651)'

# 8) Odonata => L40 = 47792
CLADES["odonata"]='("L40_taxonID" = 47792)'

# 9) Diptera => L40 = 47822
CLADES["diptera"]='("L40_taxonID" = 47822)'


# -- Additional Named Groups (Suborders, Clade Subsets, etc.) --

# Pterygota => The DB shows two taxonIDs (184884, 418641) plus
# another entry with L40_taxonID=48796. We combine them with OR:
CLADES["pterygota"]='("taxonID" = 184884 OR "taxonID" = 418641 OR "L40_taxonID" = 48796)'

# Phasmatodea => Not found in your query results. If/when you know its ID,
# you can fill it in here:
# CLADES["phasmatodea"]='("L40_taxonID" = ???)'

# Subclades within Hymenoptera (all share L40_taxonID=47201).
# Typically, referencing the top-level order is "hymenoptera"
# while these might be more specific anchor taxa:
CLADES["aculeata"]='("taxonID" = 326777)'
CLADES["apoidea"]='("taxonID" = 47222)'
CLADES["formicidae"]='("taxonID" = 47336)'
CLADES["vespoidea"]='("taxonID" = 48740)'
CLADES["vespidae"]='("taxonID" = 52747)'


# ---[ Metaclades ]------------------------------------------------------------
# Multi-root or cross-macroclade definitions. Compose bigger groups using OR/AND.

declare -A METACLADES

# Example 1: terrestrial_arthropods => Insecta OR Arachnida OR others.
# (Using the taxonIDs from the CLADES above.)
METACLADES["terrestrial_arthropods"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'

# Example 2: flying_vertebrates => Birds (aves) OR Bats (chiroptera)
# Suppose chiroptera => L40=7721 (if that’s valid in your DB).
METACLADES["flying_vertebrates"]='("L50_taxonID" = 3 OR "L40_taxonID" = 7721)'

# Example 3: nonavian_reptiles => reptilia minus birds. You might do:
# METACLADES["nonavian_reptiles"]='("L50_taxonID" = 26036 AND "L50_taxonID" != 3)'


# ---[ Helper Function ]-------------------------------------------------------
# Picks the correct expression given environment variables.
function get_clade_condition() {
  local condition

  # 1) If METACLADE is set (and found in METACLADES), return that
  if [[ -n "${METACLADE}" && -n "${METACLADES[${METACLADE}]}" ]]; then
    condition="${METACLADES[${METACLADE}]}"
    echo "${condition}"
    return
  fi

  # 2) Else if CLADE is set
  if [[ -n "${CLADE}" && -n "${CLADES[${CLADE}]}" ]]; then
    condition="${CLADES[${CLADE}]}"
    echo "${condition}"
    return
  fi

  # 3) Else if MACROCLADE is set
  if [[ -n "${MACROCLADE}" && -n "${MACROCLADES[${MACROCLADE}]}" ]]; then
    condition="${MACROCLADES[${MACROCLADE}]}"
    echo "${condition}"
    return
  fi

  # 4) Fallback: no recognized key => no filter
  echo "TRUE"
}

export -f get_clade_condition


----
Full Path: common/cladistic.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# cladistic.sh
# ------------------------------------------------------------------------------
# Creates filtered observation subsets for a specified clade/metaclade,
# referencing the "expanded_taxa" table instead of the old recursive string method.
#
# Usage pattern:
#   1) Environment variables set by an upstream wrapper or main script:
#      - METACLADE=...
#      - CLADE=...
#      - MACROCLADE=...
#      - DB_CONTAINER, DB_USER, DB_NAME
#      - REGION_TAG, MIN_OBS, ...
#      - MAX_RN (the max number of observations to keep per species)
#      - PRIMARY_ONLY (whether to restrict photos to position=0)
#   2) We load "clade_defns.sh" to define which integer columns to match
#      (e.g. "L50_taxonID" = 47120).
#   3) We create a final table <group>_observations joined to expanded_taxa
#      so only active, recognized taxonIDs appear.
#   4) We then export to CSV with a random subset of photos. 
#      The new requirement: we must include the entire ancestral taxonID set
#      (L5_taxonID, L10_taxonID, ... L70_taxonID) plus expanded_taxa.taxonID,
#      expanded_taxa.rankLevel, and expanded_taxa.name. 
#   5) The environment variable "EXPORT_GROUP" is used to name the final table
#      and output CSV (e.g. "primary_terrestrial_arthropoda").
#
# IMPORTANT NOTE regarding CSV exports:
#   - The script now appends every L{level}_taxonID column and the base columns
#     (taxonID, rankLevel, name from expanded_taxa) to the standard observation
#     columns plus photo fields. This ensures our downstream processes have
#     all needed ancestry info.
#   - The partition-based sampling approach ensures we select up to MAX_RN
#     observations per species (defined by L10_taxonID). Observations with
#     L10_taxonID IS NULL are included in full (i.e. no limit).
#
# ------------------------------------------------------------------------------
# Permission / Ownership Note:
#   If you see "Operation not permitted" when chmod-ing existing CSV files, it
#   usually means the container user (UID 998) cannot change permissions on a
#   file already owned by another user. As long as the directory is world-writable
#   (e.g. drwxrwxrwx) and the file is not locked down, Postgres should still be
#   able to write new CSV data. However, if you see actual "permission denied"
#   errors at export time, confirm the directory and file ownership allow writes.
# ------------------------------------------------------------------------------

# Load shared functions (execute_sql, print_progress, get_obs_columns, etc.)
source "${BASE_DIR}/common/functions.sh"

# Load the new clade definitions (MACROCLADES, CLADES, METACLADES, get_clade_condition)
source "${BASE_DIR}/common/clade_defns.sh"

# 1) Construct the final condition from environment variables
CLADE_CONDITION="$(get_clade_condition)"

print_progress "Creating filtered tables for ${EXPORT_GROUP}"

# We'll build the final table name, e.g. "primary_terrestrial_arthropoda_observations"
TABLE_NAME="${EXPORT_GROUP}_observations"

# We rely on the region-based table built in regional_base.sh:
#   <REGION_TAG>_min${MIN_OBS}_all_taxa_obs
# which is typically "NAfull_min50_all_taxa_obs", etc.
REGIONAL_TABLE="${REGION_TAG}_min${MIN_OBS}_all_taxa_obs"

# 2) The observation columns we'll select from the region table
#    (defined by get_obs_columns in functions.sh)
OBS_COLUMNS="$(get_obs_columns)"

# 3) Drop any old table if it exists
execute_sql "
DROP TABLE IF EXISTS \"${TABLE_NAME}\" CASCADE;
"

# 4) Create new table by joining the region table to expanded_taxa
#    so that we skip any taxon_id not found in expanded_taxa (i.e., inactive or missing).
#    We also filter by e."taxonActive" = TRUE and the clade condition.
send_notification "Joining regional table ${REGIONAL_TABLE} to expanded_taxa"
print_progress "Joining regional table ${REGIONAL_TABLE} to expanded_taxa"

execute_sql "
CREATE TABLE \"${TABLE_NAME}\" AS
SELECT
    ${OBS_COLUMNS},
    -- Include the base expanded_taxa columns (taxonID, rankLevel, name)
    e.\"taxonID\"       AS expanded_taxonID,
    e.\"rankLevel\"     AS expanded_rankLevel,
    e.\"name\"          AS expanded_name,

    -- Include all ancestral taxonID columns (sparsely populated, but required)
    e.\"L5_taxonID\",
    e.\"L10_taxonID\",
    e.\"L11_taxonID\",
    e.\"L12_taxonID\",
    e.\"L13_taxonID\",
    e.\"L15_taxonID\",
    e.\"L20_taxonID\",
    e.\"L24_taxonID\",
    e.\"L25_taxonID\",
    e.\"L26_taxonID\",
    e.\"L27_taxonID\",
    e.\"L30_taxonID\",
    e.\"L32_taxonID\",
    e.\"L33_taxonID\",
    e.\"L33_5_taxonID\",
    e.\"L34_taxonID\",
    e.\"L34_5_taxonID\",
    e.\"L35_taxonID\",
    e.\"L37_taxonID\",
    e.\"L40_taxonID\",
    e.\"L43_taxonID\",
    e.\"L44_taxonID\",
    e.\"L45_taxonID\",
    e.\"L47_taxonID\",
    e.\"L50_taxonID\",
    e.\"L53_taxonID\",
    e.\"L57_taxonID\",
    e.\"L60_taxonID\",
    e.\"L67_taxonID\",
    e.\"L70_taxonID\"
FROM \"${REGIONAL_TABLE}\" obs
JOIN \"expanded_taxa\" e
   ON e.\"taxonID\" = obs.taxon_id
WHERE e.\"taxonActive\" = TRUE
  AND ${CLADE_CONDITION};
"

# 5) Export to CSV with partition-based sampling
send_notification "Exporting filtered observations"
print_progress "Exporting filtered observations"

if [ "${PRIMARY_ONLY}" = true ]; then
    # Photos with position=0, quality_grade='research', up to MAX_RN per species
    execute_sql "
COPY (
  WITH per_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY random()
      ) AS species_rand_idx
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE p.position = 0
      AND o.quality_grade = 'research'
  )
  SELECT *
  FROM per_species
  WHERE
    (\"L10_taxonID\" IS NULL)
    OR (\"L10_taxonID\" IS NOT NULL AND species_rand_idx <= ${MAX_RN})
) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"
else
    # All photos for the final set, restricted to quality_grade='research', up to MAX_RN per species
    execute_sql "
COPY (
  WITH per_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY random()
      ) AS species_rand_idx
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE o.quality_grade = 'research'
  )
  SELECT *
  FROM per_species
  WHERE
    (\"L10_taxonID\" IS NULL)
    OR (\"L10_taxonID\" IS NOT NULL AND species_rand_idx <= ${MAX_RN})
) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"
fi

# 6) Summarize exported data: how many observations, taxa, observers?
print_progress "Creating export statistics"
STATS=$(execute_sql "
WITH export_stats AS (
    SELECT 
        COUNT(DISTINCT observation_uuid) as num_observations,
        COUNT(DISTINCT taxon_id) as num_taxa,
        COUNT(DISTINCT observer_id) as num_observers
    FROM \"${TABLE_NAME}\"
)
SELECT format(
    'Exported Data Statistics:
    Observations: %s
    Unique Taxa: %s
    Unique Observers: %s',
    num_observations, num_taxa, num_observers
)
FROM export_stats;")

echo "${STATS}" >> "${HOST_EXPORT_DIR}/export_summary.txt"

print_progress "Cladistic filtering complete"


----
Full Path: r1/wrapper_amphibia.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=4000
export PRIMARY_ONLY=true
export CLADE="amphibia"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: r1/wrapper_angiospermae.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true
export CLADE="angiospermae"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: r1/wrapper_aves.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=4000
export PRIMARY_ONLY=true
export CLADE="aves"
export EXPORT_GROUP="${CLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: r1/wrapper.sh

#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Database config
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# Export parameters
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=4000
export PRIMARY_ONLY=true
export METACLADE="primary_terrestrial_arthropoda"
export EXPORT_GROUP="${METACLADE}"
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=true # Note: adjust as needed, typically used for successive cladistic exports (from same regional base)

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"

# Paths
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"
log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: ../../../docker/stausee/docker-compose.yml

services:
  ibrida:
    image: postgis/postgis:15-3.3
    user: "998:998"
    shm_size: '16g'
    environment:
      POSTGRES_PASSWORD: ooglyboogly69
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_SHARED_BUFFERS: 8GB
      POSTGRES_WORK_MEM: 2048MB
      POSTGRES_MAINTENANCE_WORK_MEM: 4GB
    volumes:
      - ~/repo/ibridaDB/dbTools:/tool
      - ~/repo/ibridaDB/dbQueries:/query
      - /database/ibridaDB:/var/lib/postgresql/data
      - /datasets/ibrida-data/exports:/exports
      - /datasets/ibrida-data/intake:/metadata
    ports:
      - "5432:5432"
    container_name: ibridaDB

----
Full Path: ../../../docker/stausee/entrypoint.sh

#!/bin/bash
set -e

# Just log and exit - let Docker's default entrypoint handle PostgreSQL
echo "Entrypoint script executed at $(date)"

