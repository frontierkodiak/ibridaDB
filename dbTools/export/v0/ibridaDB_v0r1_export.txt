<codebase_context>

<dirtree: /home/caleb/repo/ibridaDB/dbTools/export/v0>
|-- common (1485 lines)
|   |-- clade_defns.sh (153)
|   |-- clade_helpers.sh (321)
|   |-- cladistic.sh (283)
|   |-- functions.sh (62)
|   |-- main.sh (204)
|   |-- region_defns.sh (70)
|   \-- regional_base.sh (392)
\-- r1 (373)
    |-- wrapper_amphibia_all_exc_nonrg_sp.sh (92)
    |-- wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh (94)
    |-- wrapper_pta_all_exc_nonrg_sp.sh (92)
    \-- wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh (95)
</dirtree: /home/caleb/repo/ibridaDB/dbTools/export/v0>

<file: common/main.sh>
#!/bin/bash
#
# main.sh
#
# Orchestrates the export pipeline by:
#   1) Validating environment variables
#   2) Always calling regional_base.sh (which handles creating/reusing
#      the region/clade-specific ancestor tables as needed).
#   3) Calling cladistic.sh to produce the final <EXPORT_GROUP>_observations table
#   4) Writing a unified export summary (environment variables + final stats)
#   5) Optionally copying the wrapper script for reproducibility
#
# NOTE:
#  - We no longer do skip/existence checks here. Instead, regional_base.sh
#    performs partial skip logic for its tables (_all_sp, _all_sp_and_ancestors_*, etc.).
#  - We have removed references to ANCESTOR_ROOT_RANKLEVEL, since our new multi-root
#    approach does not require it.
#
# This script expects the following environment variables to be set by the wrapper:
#   DB_USER           -> PostgreSQL user (e.g. "postgres")
#   VERSION_VALUE     -> Database version identifier (e.g. "v0")
#   RELEASE_VALUE     -> Release identifier (e.g. "r1")
#   ORIGIN_VALUE      -> (Optional) For logging context
#   DB_NAME           -> Name of the database (e.g. "ibrida-v0-r1")
#   REGION_TAG        -> Region bounding box key (e.g. "NAfull")
#   MIN_OBS           -> Minimum observations required for a species to be included
#   MAX_RN            -> Max random number of research-grade rows per species in final CSV
#   DB_CONTAINER      -> Docker container name for exec (e.g. "ibridaDB")
#   HOST_EXPORT_BASE_PATH -> Host system directory for exports
#   CONTAINER_EXPORT_BASE_PATH -> Container path that maps to HOST_EXPORT_BASE_PATH
#   EXPORT_SUBDIR     -> Subdirectory for the export (e.g. "v0/r1/primary_only_50min_4000max")
#   EXPORT_GROUP      -> Name of the final group (used in final table naming)
#
# Additionally, you may define:
#   WRAPPER_PATH      -> Path to the wrapper script for reproducibility (copied into the output dir if present).
#   INCLUDE_OUT_OF_REGION_OBS -> Whether to keep out-of-region observations for a region-based species.
#   RG_FILTER_MODE    -> One of: ONLY_RESEARCH, ALL, ALL_EXCLUDE_SPECIES_NON_RESEARCH, etc.
#   PRIMARY_ONLY      -> If true, only the primary (position=0) photo is included.
#   SKIP_REGIONAL_BASE-> If true, we skip regeneration of base tables if they exist.
#   INCLUDE_ELEVATION_EXPORT -> If "true", we include the 'elevation_meters' column (provided the DB has it, e.g. not "r0").
#
# All these environment variables are typically set in the release-specific wrapper (e.g. r1/wrapper_amphibia_all_exc_nonrg_sp.sh).
#

set -e

# ------------------------------------------------------------------------------
# 0) Source common functions
# ------------------------------------------------------------------------------
# We'll assume the caller sets BASE_DIR to the root of export/v0
# so that we can find common/functions.sh easily.
source "${BASE_DIR}/common/functions.sh"

# ------------------------------------------------------------------------------
# 1) Validate Required Environment Variables
# ------------------------------------------------------------------------------
required_vars=(
    "DB_USER" "VERSION_VALUE" "RELEASE_VALUE" "ORIGIN_VALUE"
    "DB_NAME" "REGION_TAG" "MIN_OBS" "MAX_RN"
    "DB_CONTAINER" "HOST_EXPORT_BASE_PATH" "CONTAINER_EXPORT_BASE_PATH"
    "EXPORT_SUBDIR" "EXPORT_GROUP"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var:-}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

# Some environment variables are optional but relevant:
# - SKIP_REGIONAL_BASE, INCLUDE_OUT_OF_REGION_OBS, RG_FILTER_MODE, MIN_OCCURRENCES_PER_RANK,
#   INCLUDE_MINOR_RANKS_IN_ANCESTORS, PRIMARY_ONLY, etc.
# We'll let them default if not set.

# ------------------------------------------------------------------------------
# 2) Create Export Directory Structure
# ------------------------------------------------------------------------------
print_progress "Creating export directory structure"
EXPORT_DIR="${CONTAINER_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
HOST_EXPORT_DIR="${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
ensure_directory "${HOST_EXPORT_DIR}"

# ------------------------------------------------------------------------------
# 3) Create PostgreSQL Extension & Role if needed (once per container, safe to run again)
# ------------------------------------------------------------------------------
execute_sql "
DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS dblink;
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'exportuser') THEN
        CREATE ROLE exportuser;
    END IF;
END \$\$;"

# ------------------------------------------------------------------------------
# Optional Logging for Elevation Setting
# ------------------------------------------------------------------------------
if [ "${INCLUDE_ELEVATION_EXPORT:-true}" = "true" ]; then
    print_progress "INCLUDE_ELEVATION_EXPORT=true => Elevation data (elevation_meters) will be included if present"
else
    print_progress "INCLUDE_ELEVATION_EXPORT=false => Elevation data will NOT be included"
fi

# ------------------------------------------------------------------------------
# Timing: We'll measure how long each major phase takes
# ------------------------------------------------------------------------------
overall_start=$(date +%s)

# ------------------------------------------------------------------------------
# 4) Always Invoke regional_base.sh
# ------------------------------------------------------------------------------
# The script 'regional_base.sh' is responsible for building or reusing
# region/clade-specific base tables. If SKIP_REGIONAL_BASE=true and the table
# exists, it is reused. Otherwise, it is created fresh.
regional_start=$(date +%s)
print_progress "Invoking ancestor-aware regional_base.sh"
source "${BASE_DIR}/common/regional_base.sh"
print_progress "regional_base.sh completed"
regional_end=$(date +%s)
regional_secs=$(( regional_end - regional_start ))

# ------------------------------------------------------------------------------
# 5) Apply Cladistic Filtering => Produces <EXPORT_GROUP>_observations
# ------------------------------------------------------------------------------
cladistic_start=$(date +%s)
print_progress "Applying cladistic filters via cladistic.sh"
source "${BASE_DIR}/common/cladistic.sh"
print_progress "Cladistic filtering complete"
cladistic_end=$(date +%s)
cladistic_secs=$(( cladistic_end - cladistic_start ))

# ------------------------------------------------------------------------------
# 6) Single Unified Export Summary
# ------------------------------------------------------------------------------
# We'll store environment variables, row counts, timing, etc. in a single file.
stats_start=$(date +%s)
print_progress "Creating unified export summary"

STATS=$(execute_sql "
WITH export_stats AS (
    SELECT
        COUNT(DISTINCT observation_uuid) AS num_observations,
        COUNT(DISTINCT taxon_id) AS num_taxa,
        COUNT(DISTINCT observer_id) AS num_observers
    FROM \"${EXPORT_GROUP}_observations\"
)
SELECT format(
    'Observations: %s\nUnique Taxa: %s\nUnique Observers: %s',
    num_observations, num_taxa, num_observers
)
FROM export_stats;")

SUMMARY_FILE="${HOST_EXPORT_DIR}/${EXPORT_GROUP}_export_summary.txt"
{
  echo "Export Summary"
  echo "Version: ${VERSION_VALUE}"
  echo "Release: ${RELEASE_VALUE}"
  echo "Origin: ${ORIGIN_VALUE}"
  echo "Region: ${REGION_TAG}"
  echo "Minimum Observations (species): ${MIN_OBS}"
  echo "Maximum Random Number (MAX_RN): ${MAX_RN}"
  echo "Export Group: ${EXPORT_GROUP}"
  echo "Date: $(date)"
  echo "SKIP_REGIONAL_BASE: ${SKIP_REGIONAL_BASE}"
  echo "INCLUDE_OUT_OF_REGION_OBS: ${INCLUDE_OUT_OF_REGION_OBS}"
  echo "INCLUDE_MINOR_RANKS_IN_ANCESTORS: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
  echo "RG_FILTER_MODE: ${RG_FILTER_MODE}"
  echo "MIN_OCCURRENCES_PER_RANK: ${MIN_OCCURRENCES_PER_RANK}"
  echo "INCLUDE_ELEVATION_EXPORT: ${INCLUDE_ELEVATION_EXPORT}"
  echo ""
  echo "Final Table Stats:"
  echo "${STATS}"
  echo ""
  echo "Timing:"
  echo " - Regional Base: ${regional_secs} seconds"
} > "${SUMMARY_FILE}"

stats_end=$(date +%s)
stats_secs=$(( stats_end - stats_start ))
print_progress "Stats/summary step took ${stats_secs} seconds"

# ------------------------------------------------------------------------------
# 7) Optionally Copy the Wrapper Script for Reproducibility
# ------------------------------------------------------------------------------
if [ -n "${WRAPPER_PATH:-}" ] && [ -f "${WRAPPER_PATH}" ]; then
    cp "${WRAPPER_PATH}" "${HOST_EXPORT_DIR}/"
fi

# ------------------------------------------------------------------------------
# 8) Wrap Up
# ------------------------------------------------------------------------------
overall_end=$(date +%s)
overall_secs=$(( overall_end - overall_start ))
print_progress "Export process complete (total time: ${overall_secs} seconds)"

{
  echo " - Cladistic: ${cladistic_secs} seconds"
  echo " - Summary/Stats Step: ${stats_secs} seconds"
  echo " - Overall: ${overall_secs} seconds"
} >> "${SUMMARY_FILE}"

send_notification "Export for ${EXPORT_GROUP} complete. Summary at ${SUMMARY_FILE}"
</file: common/main.sh>

<file: common/regional_base.sh>
#!/bin/bash
# ------------------------------------------------------------------------------
# regional_base.sh
# ------------------------------------------------------------------------------
# Generates region-specific species tables and associated ancestor sets,
# factoring in the user's clade/metaclade and the major/minor rank mode.
#
# Steps:
#   1) Parse environment variables and region coordinates.
#   2) Build or reuse the <REGION_TAG>_min<MIN_OBS>_all_sp table (region + MIN_OBS only).
#   3) Parse clade condition (single or multi-root). If multi-root, check overlap.
#   4) Build or reuse <REGION_TAG>_min<MIN_OBS>_all_sp_and_ancestors_<cladeID>_<mode>
#   5) Build or reuse <REGION_TAG>_min<MIN_OBS>_sp_and_ancestors_obs_<cladeID>_<mode>
#   6) Output final info/summary
#
# Requires:
#   - environment variables: DB_NAME, DB_CONTAINER, DB_USER, ...
#   - script variables: REGION_TAG, MIN_OBS, SKIP_REGIONAL_BASE,
#     INCLUDE_OUT_OF_REGION_OBS, INCLUDE_MINOR_RANKS_IN_ANCESTORS,
#     etc.
#
# ------------------------------------------------------------------------------

source "${BASE_DIR}/common/functions.sh"
source "${BASE_DIR}/common/clade_defns.sh"
source "${BASE_DIR}/common/clade_helpers.sh"
source "${BASE_DIR}/common/region_defns.sh"

# ---------------------------------------------------------------------------
# 0) Validate Environment + Setup
# ---------------------------------------------------------------------------
: "${REGION_TAG:?Error: REGION_TAG is not set}"
: "${MIN_OBS:?Error: MIN_OBS is not set}"
: "${SKIP_REGIONAL_BASE:?Error: SKIP_REGIONAL_BASE is not set}"
: "${INCLUDE_OUT_OF_REGION_OBS:?Error: INCLUDE_OUT_OF_REGION_OBS is not set}"
: "${INCLUDE_MINOR_RANKS_IN_ANCESTORS:?Error: INCLUDE_MINOR_RANKS_IN_ANCESTORS is not set}"

print_progress "=== regional_base.sh: Starting Ancestor-Aware Regional Base Generation ==="

# Retrieve bounding box for the region
get_region_coordinates || {
  echo "Failed to retrieve bounding box for REGION_TAG=${REGION_TAG}" >&2
  exit 1
}

print_progress "Using bounding box => XMIN=${XMIN}, YMIN=${YMIN}, XMAX=${XMAX}, YMAX=${YMAX}"

# ---------------------------------------------------------------------------
# 1) Build or Reuse <REGION_TAG>_min<MIN_OBS>_all_sp
# ---------------------------------------------------------------------------
ALL_SP_TABLE="${REGION_TAG}_min${MIN_OBS}_all_sp"

check_and_build_all_sp() {
  # Check existence
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ALL_SP_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    # If table exists, check row count
    local row_count
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ALL_SP_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ALL_SP_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_REGIONAL_BASE}" = "true" ]; then
        print_progress "SKIP_REGIONAL_BASE=true => reusing existing _all_sp table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating (or recreating) table \"${ALL_SP_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ALL_SP_TABLE}\" CASCADE;"

  # Build the table with bounding box + rank_level=10 + MIN_OBS filter
  execute_sql "
  CREATE TABLE \"${ALL_SP_TABLE}\" AS
  SELECT s.taxon_id
  FROM observations s
  JOIN taxa t ON t.taxon_id = s.taxon_id
  WHERE t.rank_level = 10
    AND s.quality_grade = 'research'
    AND s.geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
  GROUP BY s.taxon_id
  HAVING COUNT(s.observation_uuid) >= ${MIN_OBS};
  "
}

check_and_build_all_sp

# ---------------------------------------------------------------------------
# 2) Parse Clade Condition & Check Overlap if Multi-root
# ---------------------------------------------------------------------------
CLADE_CONDITION="$(get_clade_condition)"
print_progress "Clade Condition: ${CLADE_CONDITION}"

root_list=( $(parse_clade_expression "${CLADE_CONDITION}") )
root_count="${#root_list[@]}"
print_progress "Found ${root_count} root(s) from the clade condition"

# Decide on a short ID for the clade/metaclade
# (if you want to embed actual environment variables: e.g. $CLADE or $METACLADE
#  or parse the user-supplied string from the condition. We'll do a naive approach.)
if [ -n "${METACLADE}" ]; then
  CLADE_ID="${METACLADE}"
elif [ -n "${CLADE}" ]; then
  CLADE_ID="${CLADE}"
elif [ -n "${MACROCLADE}" ]; then
  CLADE_ID="${MACROCLADE}"
else
  # fallback if user didn't set anything
  CLADE_ID="universal"
fi

# Clean up the clade_id so it doesn't contain spaces or special chars
CLADE_ID="${CLADE_ID// /_}"

# If multi-root => check overlap
if [ "${root_count}" -gt 1 ]; then
  print_progress "Multiple roots => checking independence"
  check_root_independence "${DB_NAME}" "${root_list[@]}"
  if [ $? -ne 0 ]; then
    echo "ERROR: Overlap detected among metaclade roots. Aborting."
    exit 1
  fi
  print_progress "All roots are mutually independent"
fi

# Decide majorOrMinor string
if [ "${INCLUDE_MINOR_RANKS_IN_ANCESTORS}" = "true" ]; then
  RANK_MODE="inclMinor"
else
  RANK_MODE="majorOnly"
fi

# Build final table names
ANCESTORS_TABLE="${REGION_TAG}_min${MIN_OBS}_all_sp_and_ancestors_${CLADE_ID}_${RANK_MODE}"
ANCESTORS_OBS_TABLE="${REGION_TAG}_min${MIN_OBS}_sp_and_ancestors_obs_${CLADE_ID}_${RANK_MODE}"

# ---------------------------------------------------------------------------
# 3) Build or Reuse <REGION_TAG>_min<MIN_OBS>_all_sp_and_ancestors_<cladeID>_<mode>
# ---------------------------------------------------------------------------
check_and_build_ancestors() {
  # 1) Check if the table already exists and skip if user wants SKIP_REGIONAL_BASE
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ANCESTORS_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    local row_count
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ANCESTORS_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ANCESTORS_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_REGIONAL_BASE}" = "true" ]; then
        print_progress "Skipping creation of ancestors table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating table \"${ANCESTORS_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ANCESTORS_TABLE}\" CASCADE;"
  execute_sql "
  CREATE TABLE \"${ANCESTORS_TABLE}\" (
    taxon_id integer PRIMARY KEY
  );
  "

  local insert_ancestors_for_root
  insert_ancestors_for_root() {
    local root_pair="$1"  # e.g. "50=47158"
    local rank_part="${root_pair%%=*}"
    local root_taxid="${root_pair##*=}"

    local col_name="L${rank_part}_taxonID"

    # Decide boundary (majorOnly vs. inclMinor)
    local boundary_rank="$rank_part"
    if [ "${INCLUDE_MINOR_RANKS_IN_ANCESTORS}" = "false" ]; then
      boundary_rank="$(get_major_rank_floor "${rank_part}")"
    fi

    execute_sql "
    ----------------------------------------------------------------
    -- 1) Gather species from <ALL_SP_TABLE> that belong to this root
    ----------------------------------------------------------------
    DROP TABLE IF EXISTS temp_${root_taxid}_sp_list CASCADE;
    CREATE TEMP TABLE temp_${root_taxid}_sp_list AS
    SELECT s.taxon_id
    FROM \"${ALL_SP_TABLE}\" s
    JOIN expanded_taxa e ON e.\"taxonID\" = s.taxon_id
    WHERE e.\"${col_name}\" = ${root_taxid};

    ----------------------------------------------------------------
    -- 2) Unroll each species's ancestor IDs (L5..L70) and filter by rank
    ----------------------------------------------------------------
    DROP TABLE IF EXISTS temp_${root_taxid}_all_ancestors CASCADE;

    WITH unravel AS (
      -- 'unravel' yields each row's potential ancestor columns
      SELECT
        e.\"taxonID\"        AS sp_id,
        e.\"L5_taxonID\"     AS L5_id,
        e.\"L10_taxonID\"    AS L10_id,
        e.\"L11_taxonID\"    AS L11_id,
        e.\"L12_taxonID\"    AS L12_id,
        e.\"L13_taxonID\"    AS L13_id,
        e.\"L15_taxonID\"    AS L15_id,
        e.\"L20_taxonID\"    AS L20_id,
        e.\"L24_taxonID\"    AS L24_id,
        e.\"L25_taxonID\"    AS L25_id,
        e.\"L26_taxonID\"    AS L26_id,
        e.\"L27_taxonID\"    AS L27_id,
        e.\"L30_taxonID\"    AS L30_id,
        e.\"L32_taxonID\"    AS L32_id,
        e.\"L33_taxonID\"    AS L33_id,
        e.\"L33_5_taxonID\"  AS L33_5_id,
        e.\"L34_taxonID\"    AS L34_id,
        e.\"L34_5_taxonID\"  AS L34_5_id,
        e.\"L35_taxonID\"    AS L35_id,
        e.\"L37_taxonID\"    AS L37_id,
        e.\"L40_taxonID\"    AS L40_id,
        e.\"L43_taxonID\"    AS L43_id,
        e.\"L44_taxonID\"    AS L44_id,
        e.\"L45_taxonID\"    AS L45_id,
        e.\"L47_taxonID\"    AS L47_id,
        e.\"L50_taxonID\"    AS L50_id,
        e.\"L53_taxonID\"    AS L53_id,
        e.\"L57_taxonID\"    AS L57_id,
        e.\"L60_taxonID\"    AS L60_id,
        e.\"L67_taxonID\"    AS L67_id,
        e.\"L70_taxonID\"    AS L70_id
      FROM expanded_taxa e
      JOIN temp_${root_taxid}_sp_list sp
         ON e.\"taxonID\" = sp.taxon_id
    ),
    all_ancestors AS (
      -- We'll produce rows for the species' own ID (sp_id)
      -- plus each potential ancestor ID, then filter by rankLevel < boundary_rank.
      SELECT sp_id AS taxon_id
      FROM unravel

      UNION ALL

      SELECT x.\"taxonID\" AS taxon_id
      FROM unravel u
      CROSS JOIN LATERAL (VALUES
        (u.L5_id),(u.L10_id),(u.L11_id),(u.L12_id),(u.L13_id),(u.L15_id),
        (u.L20_id),(u.L24_id),(u.L25_id),(u.L26_id),(u.L27_id),(u.L30_id),
        (u.L32_id),(u.L33_id),(u.L33_5_id),(u.L34_id),(u.L34_5_id),(u.L35_id),
        (u.L37_id),(u.L40_id),(u.L43_id),(u.L44_id),(u.L45_id),(u.L47_id),
        (u.L50_id),(u.L53_id),(u.L57_id),(u.L60_id),(u.L67_id),(u.L70_id)
      ) anc(ancestor_id)
      JOIN expanded_taxa x ON x.\"taxonID\" = anc.ancestor_id
      WHERE x.\"rankLevel\" < ${boundary_rank}
    )
    SELECT DISTINCT taxon_id
    INTO TEMP temp_${root_taxid}_all_ancestors
    FROM all_ancestors
    WHERE taxon_id IS NOT NULL;

    ----------------------------------------------------------------
    -- 3) Insert into the final ancestors table
    ----------------------------------------------------------------
    INSERT INTO \"${ANCESTORS_TABLE}\"(taxon_id)
    SELECT DISTINCT taxon_id
    FROM temp_${root_taxid}_all_ancestors;
    "
  }

  # Decide single vs. multi-root
  if [ "${root_count}" -eq 0 ]; then
    print_progress "No recognized root => no ancestors inserted. (Might be 'TRUE' clade?)"
  elif [ "${root_count}" -eq 1 ]; then
    print_progress "Single root => straightforward insertion"
    insert_ancestors_for_root "${root_list[0]}"
  else
    print_progress "Multi-root => union each root's ancestor set"
    for root_entry in "${root_list[@]}"; do
      insert_ancestors_for_root "${root_entry}"
    done
  fi
}

check_and_build_ancestors

# ---------------------------------------------------------------------------
# 4) Build or Reuse <REGION_TAG>_min<MIN_OBS>_sp_and_ancestors_obs_<cladeID>_<mode>
# ---------------------------------------------------------------------------
check_and_build_ancestors_obs() {
  local table_exists
  table_exists="$(execute_sql "
    SELECT 1 FROM pg_catalog.pg_tables
    WHERE schemaname='public'
      AND tablename='${ANCESTORS_OBS_TABLE}'
    LIMIT 1;
  ")"

  if [[ "${table_exists}" =~ 1 ]]; then
    local row_count
    row_count="$(execute_sql "
      SELECT count(*) FROM \"${ANCESTORS_OBS_TABLE}\";
    ")"
    local numeric_count
    numeric_count="$(echo "${row_count}" | awk '/[0-9]/{print $1}' | head -1)"

    if [[ -n "${numeric_count}" && "${numeric_count}" -gt 0 ]]; then
      print_progress "Table ${ANCESTORS_OBS_TABLE} exists with ${numeric_count} rows"
      if [ "${SKIP_REGIONAL_BASE}" = "true" ]; then
        print_progress "Skipping creation of ancestors_obs table"
        return 0
      else
        print_progress "Not skipping => dropping and recreating"
      fi
    fi
  fi

  print_progress "Creating table \"${ANCESTORS_OBS_TABLE}\""
  execute_sql "DROP TABLE IF EXISTS \"${ANCESTORS_OBS_TABLE}\" CASCADE;"

  local OBS_COLUMNS
  OBS_COLUMNS="$(get_obs_columns)"

  # ---------------------------------------------------------------------------
  # ADDED FEATURE: Always store an `in_region` boolean for each observation
  # ---------------------------------------------------------------------------
  # If INCLUDE_OUT_OF_REGION_OBS=true, we do NOT filter by bounding box in the
  # WHERE clause, but we compute a boolean:
  #    COALESCE(ST_Within(geom, ST_MakeEnvelope(...)), false) AS in_region
  #
  # If INCLUDE_OUT_OF_REGION_OBS=false, we do filter by bounding box
  #    AND geom && ST_MakeEnvelope(...)
  # and simply store in_region=TRUE for all rows.

  local BBOX="ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)"

  if [ "${INCLUDE_OUT_OF_REGION_OBS}" = "true" ]; then
    execute_sql "
    CREATE TABLE \"${ANCESTORS_OBS_TABLE}\" AS
    SELECT
      ${OBS_COLUMNS},
      COALESCE(ST_Within(geom, ${BBOX}), false) AS in_region
    FROM observations
    WHERE taxon_id IN (
      SELECT taxon_id
      FROM \"${ANCESTORS_TABLE}\"
    );
    "
  else
    execute_sql "
    CREATE TABLE \"${ANCESTORS_OBS_TABLE}\" AS
    SELECT
      ${OBS_COLUMNS},
      true AS in_region
    FROM observations
    WHERE taxon_id IN (
      SELECT taxon_id
      FROM \"${ANCESTORS_TABLE}\"
    )
    AND geom && ${BBOX};
    "
  fi
}

check_and_build_ancestors_obs

export ANCESTORS_OBS_TABLE="${ANCESTORS_OBS_TABLE}" # for cladistic.sh

print_progress "=== regional_base.sh: Completed building base tables for ${REGION_TAG}, minObs=${MIN_OBS}, clade=${CLADE_ID}, mode=${RANK_MODE} ==="
</file: common/regional_base.sh>

<file: common/functions.sh>
#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

get_obs_columns() {
    # Start with standard columns
    local cols="observation_uuid, observer_id, latitude, longitude"
    
    # Add elevation_meters if export is enabled.
    if [ "${INCLUDE_ELEVATION_EXPORT:-true}" = "true" ] && [ "${RELEASE_VALUE:-r1}" != "r0" ]; then
        cols="${cols}, elevation_meters"
    fi
    
    # Then add the remaining columns
    cols="${cols}, positional_accuracy, taxon_id, quality_grade, observed_on"
    
    # If anomaly_score exists for any release value other than r0, add it.
    if [[ "${RELEASE_VALUE}" != "r0" ]]; then
        cols="${cols}, anomaly_score"
    fi
    
    echo "$cols"
}


# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    mkdir -p "${dir}"
    chmod -R 777 "${dir}"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f get_obs_columns
export -f ensure_directory
export -f send_notification
</file: common/functions.sh>

<file: common/region_defns.sh>
#!/bin/bash
# ------------------------------------------------------------------------------
# region_defns.sh
# ------------------------------------------------------------------------------
# This file defines the bounding box coordinates for each supported region.
#
# Usage:
#   source region_defns.sh
#   Then set REGION_TAG in your environment, and use get_region_coordinates()
#   to populate XMIN, XMAX, YMIN, YMAX environment variables.
# ------------------------------------------------------------------------------

declare -A REGION_COORDINATES

# North America
REGION_COORDINATES["NAfull"]="(-169.453125 12.211180 -23.554688 84.897147)"

# Europe
REGION_COORDINATES["EURwest"]="(-12.128906 40.245992 12.480469 60.586967)"
REGION_COORDINATES["EURnorth"]="(-25.927734 54.673831 45.966797 71.357067)"
REGION_COORDINATES["EUReast"]="(10.722656 41.771312 39.550781 59.977005)"
REGION_COORDINATES["EURfull"]="(-30.761719 33.284620 43.593750 72.262310)"

# Mediterranean
REGION_COORDINATES["MED"]="(-16.259766 29.916852 36.474609 46.316584)"

# Australia
REGION_COORDINATES["AUSfull"]="(111.269531 -47.989922 181.230469 -9.622414)"

# Asia
REGION_COORDINATES["ASIAse"]="(82.441406 -11.523088 153.457031 28.613459)"
REGION_COORDINATES["ASIAeast"]="(462.304688 23.241346 550.195313 78.630006)"
REGION_COORDINATES["ASIAcentral"]="(408.515625 36.031332 467.753906 76.142958)"
REGION_COORDINATES["ASIAsouth"]="(420.468750 1.581830 455.097656 39.232253)"
REGION_COORDINATES["ASIAsw"]="(386.718750 12.897489 423.281250 48.922499)"
REGION_COORDINATES["ASIA_nw"]="(393.046875 46.800059 473.203125 81.621352)"

# South America
REGION_COORDINATES["SAfull"]="(271.230469 -57.040730 330.644531 15.114553)"

# Africa
REGION_COORDINATES["AFRfull"]="(339.082031 -37.718590 421.699219 39.232253)"

# ------------------------------------------------------------------------------
# get_region_coordinates()
# ------------------------------------------------------------------------------
# Sets XMIN, YMIN, XMAX, YMAX variables from the region definition for REGION_TAG.
# If REGION_TAG is not recognized, prints an error and returns 1.
#
# Usage:
#   export REGION_TAG="XYZ"
#   source region_defns.sh
#   get_region_coordinates  # => sets XMIN, YMIN, XMAX, YMAX
# ------------------------------------------------------------------------------
function get_region_coordinates() {
    local coords="${REGION_COORDINATES[$REGION_TAG]}"
    if [ -z "$coords" ]; then
        echo "ERROR: Unknown REGION_TAG: $REGION_TAG" >&2
        return 1
    fi
    
    # Parse the coordinate quadruple from parentheses
    read XMIN YMIN XMAX YMAX <<< "${coords//[()]/}"

    # Export them for use by the caller
    export XMIN YMIN XMAX YMAX
}

export -f get_region_coordinates
</file: common/region_defns.sh>

<file: common/clade_defns.sh>
#!/bin/bash
# ------------------------------------------------------------------------------
# clade_defns.sh
# ------------------------------------------------------------------------------
# This file defines the integer-based filtering expressions for macroclades,
# clades, and metaclades, referencing columns in "expanded_taxa".
#
# Usage:
#   source clade_defns.sh
#   Then pick a macroclade (MACROCLADE="..."), or a clade (CLADE="..."),
#   or a metaclade (METACLADE="...") in your environment, and the
#   cladistic.sh script will build a condition from one of the arrays below.
#
# Example:
#   MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'
#   CLADES["insecta"]='("L50_taxonID" = 47158)'
#   METACLADES["primary_terrestrial_arthropoda"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'
#
# Be sure to substitute the correct taxonIDs for your local database!
# ------------------------------------------------------------------------------
#
# Sections in this file:
#   1) Macroclade Definitions
#   2) Clade Definitions
#   3) Metaclade Definitions
#   4) get_clade_condition() helper
#
# NOTE: We do NOT remove any existing definitions or comments.

# ---[ 1) Macroclade Definitions ]---------------------------------------------
# Typically for kingdom-level (L70) or phylum-level (L60) anchors.

declare -A MACROCLADES

# 1) Arthropoda => phylum at L60 = 47120
MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'

# 2) Chordata => phylum at L60 = 2
MACROCLADES["chordata"]='("L60_taxonID" = 2)'

# 3) Plantae => kingdom at L70 = 47126
MACROCLADES["plantae"]='("L70_taxonID" = 47126)'

# 4) Fungi => kingdom at L70 = 47170
MACROCLADES["fungi"]='("L70_taxonID" = 47170)'

# (Optional) If you consider Actinopterygii, Mammalia, Reptilia, etc.
# to be "macroclades," you may define them here instead of in CLADES.
# For instance:
#   MACROCLADES["mammalia"]='("L50_taxonID" = 40151)'


# ---[ 2) Clade Definitions ]--------------------------------------------------
# Typically for class-level (L50), order-level (L40), or narrower taxonomic groups.
# single-root, so functionally equivalent to METACLADES.

declare -A CLADES

# -- Plant Clades (Subphylum and Class levels) --
# -- Plant Subphylum (L57) --
CLADES["angiospermae"]='("L57_taxonID" = 47125)' # flowering plants

# -- Plant Classes (L50) --
CLADES["liliopsida"]='("L50_taxonID" = 47163)'    # monocots
CLADES["magnoliopsida"]='("L50_taxonID" = 47124)' # dicots

# -- Class-level (L50) Examples --
CLADES["actinopterygii"]='("L50_taxonID" = 47178)'
CLADES["amphibia"]='("L50_taxonID" = 20978)'
CLADES["arachnida"]='("L50_taxonID" = 47119)'
CLADES["aves"]='("L50_taxonID" = 3)'
CLADES["insecta"]='("L50_taxonID" = 47158)'
CLADES["mammalia"]='("L50_taxonID" = 40151)'
CLADES["reptilia"]='("L50_taxonID" = 26036)'

# -- Order-level (L40) Examples --
CLADES["testudines"]='("L40_taxonID" = 39532)'
CLADES["crocodylia"]='("L40_taxonID" = 26039)'
CLADES["coleoptera"]='("L40_taxonID" = 47208)'
CLADES["lepidoptera"]='("L40_taxonID" = 47157)'
CLADES["hymenoptera"]='("L40_taxonID" = 47201)'
CLADES["hemiptera"]='("L40_taxonID" = 47744)'
CLADES["orthoptera"]='("L40_taxonID" = 47651)'
CLADES["odonata"]='("L40_taxonID" = 47792)'
CLADES["diptera"]='("L40_taxonID" = 47822)'

# -- Additional Named Groups (Suborders, Clade Subsets, etc.) --
# Pterygota => The DB shows two taxonIDs (184884, 418641) plus
# another entry with L40_taxonID=48796. We combine them with OR:
CLADES["pterygota"]='("taxonID" = 184884 OR "taxonID" = 418641 OR "L40_taxonID" = 48796)'

# Phasmatodea => Not found in your query results. If/when you know its ID,
# you can fill it in here:
# CLADES["phasmatodea"]='("L40_taxonID" = ???)'

# Subclades within Hymenoptera (all share L40_taxonID=47201).
# Typically, referencing the top-level order is "hymenoptera"
# while these might be more specific anchor taxa:
CLADES["aculeata"]='("taxonID" = 326777)'
CLADES["apoidea"]='("taxonID" = 47222)'
CLADES["formicidae"]='("taxonID" = 47336)'
CLADES["vespoidea"]='("taxonID" = 48740)'
CLADES["vespidae"]='("taxonID" = 52747)'


# ---[ 3) Metaclade Definitions ]----------------------------------------------
# Multi-root or cross-macroclade definitions. Compose bigger groups using OR.

declare -A METACLADES

# Example 1: primary_terrestrial_arthropoda (pta) => Insecta OR Arachnida.
METACLADES["pta"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'

# Example 2: flying_vertebrates => Birds (aves) OR Bats (chiroptera)
# METACLADES["flying_vertebrates"]='("L50_taxonID" = 3 OR "L40_taxonID" = 7721)'

# Example 3: nonavian_reptiles => reptilia minus birds.
# METACLADES["nonavian_reptiles"]='("L50_taxonID" = 26036 AND "L50_taxonID" != 3)'


# ---[ 4) get_clade_condition() Helper ]-----------------------------------------
# Picks the correct expression given environment variables (METACLADE, CLADE,
# MACROCLADE). This is used by cladistic.sh to filter rows.

function get_clade_condition() {
  local condition

  # 1) If METACLADE is set (and found in METACLADES), return that
  if [[ -n "${METACLADE}" && -n "${METACLADES[${METACLADE}]}" ]]; then
    condition="${METACLADES[${METACLADE}]}"
    echo "${condition}"
    return
  fi

  # 2) Else if CLADE is set
  if [[ -n "${CLADE}" && -n "${CLADES[${CLADE}]}" ]]; then
    condition="${CLADES[${CLADE}]}"
    echo "${condition}"
    return
  fi

  # 3) Else if MACROCLADE is set
  if [[ -n "${MACROCLADE}" && -n "${MACROCLADES[${MACROCLADE}]}" ]]; then
    condition="${MACROCLADES[${MACROCLADE}]}"
    echo "${condition}"
    return
  fi

  # 4) Fallback: no recognized key => no filter
  echo "TRUE"
}

export -f get_clade_condition
</file: common/clade_defns.sh>

<file: common/cladistic.sh>
#!/bin/bash
# ------------------------------------------------------------------------------
# cladistic.sh
# ------------------------------------------------------------------------------
# Creates a final observation subset for a user-specified clade/metaclade,
# referencing the "expanded_taxa" table. The input table for this script is
# typically provided in ANCESTORS_OBS_TABLE (set by regional_base.sh). This
# table contains:
#   - All observations of species that passed the MIN_OBS threshold
#     in the specified region bounding box, plus all their ancestral
#     taxonIDs, up to the chosen root rank(s).
#   - If INCLUDE_OUT_OF_REGION_OBS=true, out-of-region rows for those
#     same species are also included (with in_region=false).
#   - If INCLUDE_MINOR_RANKS_IN_ANCESTORS=false, only major decade ranks
#     are included; otherwise, minor ranks are included.
#
# Steps in cladistic.sh:
#   1) Validate environment & drop any pre-existing final table <EXPORT_GROUP>_observations
#   2) Construct a filtering WHERE clause for research/non-research quality using RG_FILTER_MODE
#   3) Create the final <EXPORT_GROUP>_observations table by joining to expanded_taxa
#   4) Optionally wipe partial ranks (L20, L30, L40) if MIN_OCCURRENCES_PER_RANK is set
#   5) Export to CSV via a partition-based random sampling approach:
#       - "capped_research_species" subquery for each species' research-grade rows
#       - "everything_else" subquery for all other rows
#       - Union them, writing to <EXPORT_GROUP>_photos.csv
#   6) Print progress messages, handle debug logs for column lists
#
# Environment Variables:
#   ANCESTORS_OBS_TABLE -> Name of the region/clade-based observations table
#   EXPORT_GROUP        -> Suffix for the final table name & CSV
#   RG_FILTER_MODE      -> One of: [ONLY_RESEARCH, ALL, ALL_EXCLUDE_SPECIES_NON_RESEARCH, etc.]
#   MIN_OCCURRENCES_PER_RANK -> If >= 1, triggers partial-rank wiping for L20/L30/L40
#   MAX_RN             -> Max research-grade rows per species
#   PRIMARY_ONLY        -> If true, only photos with position=0 are included
#   INCLUDE_ELEVATION_EXPORT -> If true & release != r0, includes 'elevation_meters'
#
#   DB_CONTAINER, DB_USER, DB_NAME, BASE_DIR, etc. must also be set.
#
# NOTE: Because we have thoroughly documented the final columns in docs/schemas.md,
# we now rely on get_obs_columns() for the main observation columns, appending
# the expanded_taxa columns and photo columns carefully to preserve the final layout.
# ------------------------------------------------------------------------------

set -e

# 1) Source common functions & validate ANCESTORS_OBS_TABLE
source "${BASE_DIR}/common/functions.sh"

if [ -z "${ANCESTORS_OBS_TABLE:-}" ]; then
  echo "ERROR: cladistic.sh requires ANCESTORS_OBS_TABLE to be set."
  exit 1
fi

print_progress "cladistic.sh: Using ancestor-based table = ${ANCESTORS_OBS_TABLE}"

TABLE_NAME="${EXPORT_GROUP}_observations"
execute_sql "DROP TABLE IF EXISTS \"${TABLE_NAME}\" CASCADE;"

# ------------------------------------------------------------------------------
# 2) Construct RG filter condition & possibly rewrite L10_taxonID if wiping
# ------------------------------------------------------------------------------
rg_where_condition="TRUE"
rg_l10_col="e.\"L10_taxonID\""

case "${RG_FILTER_MODE:-ALL}" in
  "ONLY_RESEARCH")
    rg_where_condition="o.quality_grade='research'"
    ;;
  "ALL")
    rg_where_condition="TRUE"
    ;;
  "ALL_EXCLUDE_SPECIES_NON_RESEARCH")
    rg_where_condition="NOT (o.quality_grade!='research' AND e.\"L10_taxonID\" IS NOT NULL)"
    ;;
  "ONLY_NONRESEARCH")
    rg_where_condition="o.quality_grade!='research'"
    ;;
  "ONLY_NONRESEARCH_EXCLUDE_SPECIES")
    rg_where_condition="(o.quality_grade!='research' AND e.\"L10_taxonID\" IS NULL)"
    ;;
  "ONLY_NONRESEARCH_WIPE_SPECIES_LABEL")
    rg_where_condition="o.quality_grade!='research'"
    rg_l10_col="NULL::integer"
    ;;
  *)
    rg_where_condition="TRUE"
    ;;
esac

# ------------------------------------------------------------------------------
# 3) Create <EXPORT_GROUP>_observations by joining ANCESTORS_OBS_TABLE + expanded_taxa
# ------------------------------------------------------------------------------
# Observations columns come from get_obs_columns(), which includes optional
# elevation_meters & anomaly_score if present in this release.
OBS_COLUMNS="$(get_obs_columns)"

# We'll define the expanded_taxa columns we need:
EXPANDED_TAXA_COLS="
    e.\"taxonID\"       AS expanded_taxonID,
    e.\"rankLevel\"     AS expanded_rankLevel,
    e.\"name\"          AS expanded_name,
    e.\"L5_taxonID\",
    ${rg_l10_col}       AS \"L10_taxonID\",
    e.\"L11_taxonID\",
    e.\"L12_taxonID\",
    e.\"L13_taxonID\",
    e.\"L15_taxonID\",
    e.\"L20_taxonID\",
    e.\"L24_taxonID\",
    e.\"L25_taxonID\",
    e.\"L26_taxonID\",
    e.\"L27_taxonID\",
    e.\"L30_taxonID\",
    e.\"L32_taxonID\",
    e.\"L33_taxonID\",
    e.\"L33_5_taxonID\",
    e.\"L34_taxonID\",
    e.\"L34_5_taxonID\",
    e.\"L35_taxonID\",
    e.\"L37_taxonID\",
    e.\"L40_taxonID\",
    e.\"L43_taxonID\",
    e.\"L44_taxonID\",
    e.\"L45_taxonID\",
    e.\"L47_taxonID\",
    e.\"L50_taxonID\",
    e.\"L53_taxonID\",
    e.\"L57_taxonID\",
    e.\"L60_taxonID\",
    e.\"L67_taxonID\",
    e.\"L70_taxonID\"
"

# Join & filter
execute_sql "
CREATE TABLE \"${TABLE_NAME}\" AS
SELECT
    ${OBS_COLUMNS},
    o.in_region,
    ${EXPANDED_TAXA_COLS}
FROM \"${ANCESTORS_OBS_TABLE}\" o
JOIN expanded_taxa e ON e.\"taxonID\" = o.taxon_id
WHERE e.\"taxonActive\" = TRUE
  AND (${rg_where_condition});
"

# ------------------------------------------------------------------------------
# 4) (Optional) Partial-Rank Wiping for L20, L30, L40
# ------------------------------------------------------------------------------
if [ -z "${MIN_OCCURRENCES_PER_RANK:-}" ] || [ "${MIN_OCCURRENCES_PER_RANK}" = "-1" ]; then
  print_progress "Skipping partial-rank wipe (MIN_OCCURRENCES_PER_RANK not set or -1)."
else
  print_progress "Applying partial-rank wipe with threshold = ${MIN_OCCURRENCES_PER_RANK}"

  for rc in L20_taxonID L30_taxonID L40_taxonID; do
    print_progress "Wiping low-occurrence ${rc} if usage < ${MIN_OCCURRENCES_PER_RANK}"
    execute_sql "
    WITH usage_ct AS (
      SELECT \"${rc}\" as tid, COUNT(*) as c
      FROM \"${TABLE_NAME}\"
      WHERE \"${rc}\" IS NOT NULL
      GROUP BY 1
    )
    UPDATE \"${TABLE_NAME}\"
    SET \"${rc}\" = NULL
    FROM usage_ct
    WHERE usage_ct.tid = \"${TABLE_NAME}\".\"${rc}\"
      AND usage_ct.c < ${MIN_OCCURRENCES_PER_RANK};
    "
  done
fi

# ------------------------------------------------------------------------------
# 5) Export Final CSV using partition-based sampling for research-grade species
# ------------------------------------------------------------------------------
print_progress "cladistic.sh: Exporting final CSV with partition-based sampling"

# We'll define a photo filter to include only position=0 if PRIMARY_ONLY=true
pos_condition="TRUE"
if [ "${PRIMARY_ONLY:-false}" = "true" ]; then
  pos_condition="p.position=0"
fi

# For clarity, let's define the final set of columns in our CSV union
# We'll re-use get_obs_columns for the observation portion, and re-list
# the expanded & photo columns to keep final control. Then we add 'rn'.
CSV_OBS_COLS="$(get_obs_columns), in_region,
expanded_taxonID, expanded_rankLevel, expanded_name,
L5_taxonID, L10_taxonID, L11_taxonID, L12_taxonID, L13_taxonID, L15_taxonID,
L20_taxonID, L24_taxonID, L25_taxonID, L26_taxonID, L27_taxonID,
L30_taxonID, L32_taxonID, L33_taxonID, L33_5_taxonID, L34_taxonID, L34_5_taxonID,
L35_taxonID, L37_taxonID, L40_taxonID, L43_taxonID, L44_taxonID, L45_taxonID,
L47_taxonID, L50_taxonID, L53_taxonID, L57_taxonID, L60_taxonID, L67_taxonID, L70_taxonID
"

# Photo columns
CSV_PHOTO_COLS="photo_uuid, photo_id, extension, license, width, height, position"

# We'll use debug queries to log these column lists:
debug_sql_obs="
SELECT 'DEBUG: Final CSV obs columns => ' ||
       array_to_string(array['$(echo $CSV_OBS_COLS | xargs)'], ', ');
"
debug_sql_photo="
SELECT 'DEBUG: Final CSV photo columns => ' ||
       array_to_string(array['$(echo $CSV_PHOTO_COLS | xargs)'], ', ');
"

execute_sql "$debug_sql_obs"
execute_sql "$debug_sql_photo"

# If MAX_RN is not set, default to 3000
if [ -z "${MAX_RN:-}" ]; then
  echo "Warning: MAX_RN not set, defaulting to 3000"
  MAX_RN=3000
fi

# Now produce the CSV with two subqueries:
#   1) capped_research_species -> research-grade, species-level rows, partition-limited
#   2) everything_else -> non-research or non-species-level
EXPORT_FILE="${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv"

execute_sql "
COPY (
  WITH
  capped_research_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY
          CASE WHEN o.in_region THEN 0 ELSE 1 END,
          random()
      ) AS rn
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE $pos_condition
      AND o.quality_grade='research'
      AND o.\"L10_taxonID\" IS NOT NULL
  ),
  everything_else AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      NULL::bigint AS rn
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE $pos_condition
      AND NOT (o.quality_grade='research' AND o.\"L10_taxonID\" IS NOT NULL)
  )
  SELECT
    ${CSV_OBS_COLS},
    ${CSV_PHOTO_COLS},
    rn
  FROM capped_research_species
  WHERE rn <= ${MAX_RN}

  UNION ALL

  SELECT
    ${CSV_OBS_COLS},
    ${CSV_PHOTO_COLS},
    rn
  FROM everything_else
) TO '${EXPORT_FILE}' WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"

print_progress "cladistic.sh: CSV export complete"
print_progress "Exported final CSV to ${EXPORT_FILE}"
</file: common/cladistic.sh>

<file: common/clade_helpers.sh>
#!/bin/bash
# ------------------------------------------------------------------------------
# clade_helpers.sh
# ------------------------------------------------------------------------------
# This file contains helper functions for multi-root/metaclade logic,
# rank boundary calculations, and advanced taxon-ancestry checks.
#
# Proposed usage:
#   1) "parse_clade_expression()" to parse user-provided condition strings
#      (e.g. "L50_taxonID=123 OR L40_taxonID=9999") into structured data.
#   2) "check_root_independence()" to verify that each root is truly disjoint
#      (none is an ancestor of another).
#   3) "get_major_rank_floor()" to compute the next-lower major-rank boundary
#      if user does not want to include minor ranks. Typically used if the root
#      is e.g. 57 => 50. If user includes minor ranks, we skip the rounding.
#
# NOTE: We do not forcibly integrate with existing "get_clade_condition()"
# in clade_defns.sh. Instead, you can call parse_clade_expression() if you
# want to do deeper multi-root logic.
#
# Implementation details:
#   - We store a reference map from "L<number>_taxonID" to the numeric rank
#     (e.g. "L50_taxonID" => 50). If the user requests minor ranks, we do not
#     round them down to the multiple of 10.
#   - We rely on "expanded_taxa" for ancestry checks. The "check_root_independence()"
#     function is conceptual: it gathers each root's entire ancestry (e.g. ~30
#     columns from L5..L70) and ensures no overlap among root sets.
#
# ------------------------------------------------------------------------------
#
# Exports:
#   - parse_clade_expression()
#   - check_root_independence()
#   - get_major_rank_floor()
#

# -------------------------------------------------------------
# A) Internal reference: Maps "L50_taxonID" => 50, "L40_taxonID" => 40, etc.
# -------------------------------------------------------------
declare -A RANKLEVEL_MAP=(
  ["L5_taxonID"]="5"
  ["L10_taxonID"]="10"
  ["L11_taxonID"]="11"
  ["L12_taxonID"]="12"
  ["L13_taxonID"]="13"
  ["L15_taxonID"]="15"
  ["L20_taxonID"]="20"
  ["L24_taxonID"]="24"
  ["L25_taxonID"]="25"
  ["L26_taxonID"]="26"
  ["L27_taxonID"]="27"
  ["L30_taxonID"]="30"
  ["L32_taxonID"]="32"
  ["L33_taxonID"]="33"
  ["L33_5_taxonID"]="33.5"
  ["L34_taxonID"]="34"
  ["L34_5_taxonID"]="34.5"
  ["L35_taxonID"]="35"
  ["L37_taxonID"]="37"
  ["L40_taxonID"]="40"
  ["L43_taxonID"]="43"
  ["L44_taxonID"]="44"
  ["L45_taxonID"]="45"
  ["L47_taxonID"]="47"
  ["L50_taxonID"]="50"
  ["L53_taxonID"]="53"
  ["L57_taxonID"]="57"
  ["L60_taxonID"]="60"
  ["L67_taxonID"]="67"
  ["L70_taxonID"]="70"
  # stateofmatter => 100, if we had that in expanded_taxa
)

# --------------------------------------------------------------------------
# parse_clade_expression()
# --------------------------------------------------------------------------
# Parses a SQL-like expression containing L{XX}_taxonID conditions into an array 
# of "rank=taxonID" pairs.
#
# Expected usage:
#   - We typically pass the result of get_clade_condition(), which looks like:
#     ("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)
#   - The caller captures the results in an array:
#     roots=( $(parse_clade_expression "$clade_condition") )
#
# Processing steps:
#   1) Removes parentheses and double quotes
#   2) Splits on " OR " to handle multiple conditions
#   3) For each condition:
#      - Splits on '=' to get the LHS and RHS
#      - Extracts the rank number from L{XX}_taxonID pattern
#      - Pairs the rank with the taxonID
#
# Return format:
#   Space-separated strings in the form "rank=taxonID", e.g.:
#   "50=47158" "50=47119"
#
# Examples:
#   Input:  "L50_taxonID" = 47158
#   Output: 50=47158
#
#   Input:  ("L50_taxonID" = 47158 OR "L40_taxonID" = 9999)
#   Output: 50=47158 40=9999
#
# Notes:
#   - Case-insensitive: l50_taxonid and L50_taxonID are equivalent
#   - Spaces around '=' are optional
#   - Ignores any conditions not matching L{XX}_taxonID pattern
#   - Requires numeric taxonID values
# --------------------------------------------------------------------------
function parse_clade_expression() {
  local expr="$1"

  # 1) Remove parentheses and double quotes
  local cleaned_expr
  cleaned_expr="$(echo "$expr" | tr -d '()"')"
  echo "DEBUG [2]: After removing parentheses/quotes: '$cleaned_expr'" >&2

  # 2) Split on " OR " properly using sed
  local or_parts
  or_parts="$(echo "$cleaned_expr" | sed 's/ OR /\n/g')"
  
  local results=()
  
  while IFS= read -r part; do
    # Trim spaces and split on =
    local lhs rhs
    IFS='=' read -r lhs rhs <<< "$(echo "$part" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')"
    
    # Remove any remaining spaces
    lhs="$(echo "$lhs" | sed 's/[[:space:]]//g')"
    rhs="$(echo "$rhs" | sed 's/[[:space:]]//g')"
    
    # Extract the numeric part from LXX_taxonID
    if [[ $lhs =~ L([0-9]+)_taxonID ]]; then
      local rank="${BASH_REMATCH[1]}"
      results+=( "${rank}=${rhs}" )
    fi
  done <<< "$or_parts"

  echo "${results[@]}"
}

function check_root_independence() {
  # --------------------------------------------------------------------------
  # check_root_independence()
  #
  # PURPOSE:
  #   Ensures that each root in a multi-root scenario is truly independent,
  #   i.e., no root is an ancestor or descendant of another when viewed at
  #   or below the highest rank boundary. For example, if you have two roots
  #   at rank=50 (Insecta, Arachnida), they do share a phylum at rank=60
  #   (Arthropoda), but that is above their rank boundary, so it should NOT
  #   trigger a conflict.
  #
  # IMPLEMENTATION STEPS:
  #   1) Parse the root array (each item = "rank=taxonID", e.g. "50=47158").
  #   2) Find the globalMaxRank = max(r_i for each root).
  #   3) For each root, fetch its single row from expanded_taxa (which includes
  #      columns L5..L70). Then cross-join or left-join each potential ancestor
  #      ID to get that ancestor's rankLevel from expanded_taxa.
  #      Keep only those whose rankLevel <= globalMaxRank.
  #   4) Build a set of taxonIDs for that root (space-separated).
  #   5) Compare each pair of root sets for intersection. If they share a taxonID
  #      that is rankLevel <= globalMaxRank, we treat it as an overlap => return 1.
  #
  #   If no overlap is found among the rank <= globalMaxRank ancestors, return 0.
  #
  # USAGE:
  #   check_root_independence <db_name> <rootArray...>
  #   e.g. check_root_independence "myDB" "50=47158" "50=47119"
  #
  # RETURNS:
  #   0 if no overlap found, 1 if overlap is detected or root is not found.
  # --------------------------------------------------------------------------

  local dbName="$1"
  shift
  local roots=("$@")  # e.g. ("50=47158" "50=47119")

  # If there's 0 or 1 root, there's nothing to compare => trivially independent
  if [ "${#roots[@]}" -le 1 ]; then
    return 0
  fi

  # 1) Determine the global max rank among all root definitions
  local globalMaxRank=0
  for r in "${roots[@]}"; do
    local rr="${r%%=*}"
    if (( rr > globalMaxRank )); then
      globalMaxRank="$rr"
    fi
  done

  declare -A rootSets  # will map index => "list of ancestor taxonIDs"

  for i in "${!roots[@]}"; do
    local pair="${roots[$i]}"
    local rank="${pair%%=*}"
    local tid="${pair##*=}"

    # We'll do an expanded cross-lateral approach to gather the root's entire
    # L5..L70 columns, then retrieve each ancestor's rankLevel, ignoring any
    # with rankLevel > globalMaxRank.
    #
    # Because we only do ONE row for the root (plus ~30 columns), a single
    # CROSS JOIN to expanded_taxa for each ancestor ID is feasible.

    local sql="
COPY (
  WITH one_root AS (
    SELECT
      e.\"taxonID\" AS sp_id,
      e.\"L5_taxonID\", e.\"L10_taxonID\", e.\"L11_taxonID\", e.\"L12_taxonID\",
      e.\"L13_taxonID\", e.\"L15_taxonID\", e.\"L20_taxonID\", e.\"L24_taxonID\",
      e.\"L25_taxonID\", e.\"L26_taxonID\", e.\"L27_taxonID\", e.\"L30_taxonID\",
      e.\"L32_taxonID\", e.\"L33_taxonID\", e.\"L33_5_taxonID\", e.\"L34_taxonID\",
      e.\"L34_5_taxonID\", e.\"L35_taxonID\", e.\"L37_taxonID\", e.\"L40_taxonID\",
      e.\"L43_taxonID\", e.\"L44_taxonID\", e.\"L45_taxonID\", e.\"L47_taxonID\",
      e.\"L50_taxonID\", e.\"L53_taxonID\", e.\"L57_taxonID\", e.\"L60_taxonID\",
      e.\"L67_taxonID\", e.\"L70_taxonID\"
    FROM expanded_taxa e
    WHERE e.\"taxonID\" = ${tid}
  ),
  potential_ancestors AS (
    SELECT sp_id as taxon_id FROM one_root
    UNION ALL
    SELECT anc.\"taxonID\"
    FROM one_root o
    CROSS JOIN LATERAL (VALUES
      (o.\"L5_taxonID\"),(o.\"L10_taxonID\"),(o.\"L11_taxonID\"),(o.\"L12_taxonID\"),
      (o.\"L13_taxonID\"),(o.\"L15_taxonID\"),(o.\"L20_taxonID\"),(o.\"L24_taxonID\"),
      (o.\"L25_taxonID\"),(o.\"L26_taxonID\"),(o.\"L27_taxonID\"),(o.\"L30_taxonID\"),
      (o.\"L32_taxonID\"),(o.\"L33_taxonID\"),(o.\"L33_5_taxonID\"),(o.\"L34_taxonID\"),
      (o.\"L34_5_taxonID\"),(o.\"L35_taxonID\"),(o.\"L37_taxonID\"),(o.\"L40_taxonID\"),
      (o.\"L43_taxonID\"),(o.\"L44_taxonID\"),(o.\"L45_taxonID\"),(o.\"L47_taxonID\"),
      (o.\"L50_taxonID\"),(o.\"L53_taxonID\"),(o.\"L57_taxonID\"),(o.\"L60_taxonID\"),
      (o.\"L67_taxonID\"),(o.\"L70_taxonID\")
    ) x(ancestor_id)
    JOIN expanded_taxa anc ON anc.\"taxonID\" = x.ancestor_id
    WHERE anc.\"rankLevel\" <= ${globalMaxRank}
  )
  SELECT array_agg(potential_ancestors.taxon_id) AS allowed_ancestors
  FROM potential_ancestors
) TO STDOUT WITH CSV HEADER;
"
    local query_result
    query_result="$(execute_sql "$sql")"

    # If the query returns only a header line, it might indicate no row found
    # for that root. We can check for 'allowed_ancestors' in the last line.
    local data_line
    data_line="$(echo "$query_result" | tail -n1)"
    if [[ "$data_line" == *"allowed_ancestors"* ]]; then
      echo "ERROR: check_root_independence: No row found or no ancestors for taxonID=${tid}" >&2
      return 1
    fi

    # data_line might look like: {47158,47157,47120,...}
    # We'll remove braces and parse
    local trimmed="$(echo "$data_line" | tr -d '{}')"
    # e.g. 47158,47157,47120
    # We'll split on commas
    IFS=',' read -ra ancestors <<< "$trimmed"

    # Now store them in space-separated form
    rootSets["$i"]="${ancestors[*]}"
  done

  # 3) Compare each pair of sets for intersection
  for ((i=0; i<${#roots[@]}; i++)); do
    for ((j=i+1; j<${#roots[@]}; j++)); do
      local set1=" ${rootSets[$i]} "
      for t2 in ${rootSets[$j]}; do
        # If the token t2 appears in set1 => overlap
        # (We assume space-bounded match to avoid partial string hits)
        if [[ "$set1" =~ " $t2 " ]]; then
          echo "ERROR: Overlap detected between root #$i (${roots[$i]}) \
and root #$j (${roots[$j]}) on taxonID=${t2}" >&2
          return 1
        fi
      done
    done
  done

  return 0
}

# -------------------------------------------------------------
# D) get_major_rank_floor()
# -------------------------------------------------------------
# This function returns the next-lower major rank multiple of 10 if we want
# to exclude minor ranks. For instance:
#   if input=57 => output=50
#   if input=50 => output=40
#   if input=70 => output=60
#
# If the user wants minor ranks, we might skip or do partial rounding logic.
# For now, we do a straightforward approach:
#
function get_major_rank_floor() {
  local input_rank="$1"
  # We'll do a naive loop:
  # possible major ranks = [70,60,50,40,30,20,10,5]
  # or we can do math: floor((input_rank/10))*10 => but that fails for e.g. 57 => 50 is fine
  # Actually that might be enough, but let's handle if it's exactly a multiple of 10 => we subtract 10 again
  # e.g. 50 => 40, because we want "strictly less than the root rank".
  # If input=57 => floor(57/10)*10=50 => good
  # If input=50 => floor(50/10)*10=50 => but we want 40 => so let's do -10 if exactly multiple

  local base=$(( input_rank/10*10 ))
  if (( $(echo "$input_rank == $base" | bc) == 1 )); then
    # means input is multiple of 10
    base=$(( base-10 ))
  fi
  echo "$base"
}

export -f parse_clade_expression
export -f check_root_independence
export -f get_major_rank_floor
</file: common/clade_helpers.sh>

<file: r1/wrapper_pta_all_exc_nonrg_sp.sh>
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=3000
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"
</file: r1/wrapper_pta_all_exc_nonrg_sp.sh>

<file: r1/wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh>
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2750
export PRIMARY_ONLY=true

export METACLADE="pta" # primary_terrestrial_arthropoda
export EXPORT_GROUP="pta_all_exc_nonrg_sp_full_ancestor_search"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# NOTE elevation not ready yet, exclude for now
export INCLUDE_ELEVATION_EXPORT=false

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=true

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"
</file: r1/wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh>

<file: r1/wrapper_amphibia_all_exc_nonrg_sp.sh>
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_all_exc_nonrg_sp_inc_out_of_region"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"
</file: r1/wrapper_amphibia_all_exc_nonrg_sp.sh>

<file: r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh>
#!/bin/bash

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"
echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=50
export MAX_RN=2500
export PRIMARY_ONLY=true

export CLADE="amphibia"
export EXPORT_GROUP="amphibia_all_exc_nonrg_sp_oor_elev"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false  # typically used for successive cladistic exports

# ---[ NEW ENV VARS ]---
# Whether to include out-of-region observations in the final dataset
export INCLUDE_OUT_OF_REGION_OBS=true
# Include elevation_meters in the final dataset?
export INCLUDE_ELEVATION_EXPORT=true

# Whether to keep research-grade only, non-research, etc.
# For now, we default to ALL; future steps will integrate it
export RG_FILTER_MODE="ALL_EXCLUDE_SPECIES_NON_RESEARCH"

export MIN_OCCURRENCES_PER_RANK=50
export INCLUDE_MINOR_RANKS_IN_ANCESTORS=false

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"
log_message "Min Occurrences per Rank: ${MIN_OCCURRENCES_PER_RANK}"
log_message "Include Minor Ranks in Ancestors: ${INCLUDE_MINOR_RANKS_IN_ANCESTORS}"
# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# ---------------------------------------------------------------------------
# Source common functions
# ---------------------------------------------------------------------------
source "${BASE_DIR}/common/functions.sh"

# ---------------------------------------------------------------------------
# Execute main script
# ---------------------------------------------------------------------------
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"
"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"
</file: r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh>

<file: ../../../docs/schemas.md>
# ibridaDB Schemas Reference

This document provides a detailed reference for all of the schemas used in ibridaDB. It covers:

1. The core tables imported directly from iNaturalist.
2. The **expanded_taxa** table generated from the iNaturalist `taxa` table.
3. The final export table produced by the export pipeline.
4. Supplementary information on data types, indexing, and version-specific differences.
5. An appendix with sample SQL output (\d results) for quick reference.

This reference is intended to help developers and maintainers quickly understand the structure, data types, and intended usage of each table without needing to manually inspect the database.

---

## Table of Contents

1. [Core iNaturalist Tables](#core-inaturalist-tables)  
   1.1. [Observations](#observations)  
   1.2. [Photos](#photos)  
   1.3. [Observers](#observers)  
   1.4. [Taxa](#taxa)

2. [Expanded Taxa Table](#expanded-taxa-table)  
   2.1. [Purpose and Generation](#purpose-and-generation)  
   2.2. [Schema Details](#schema-details)  
   2.3. [Indexing and Performance Considerations](#indexing-and-performance-considerations)  
   2.4. [Rank-Level Mapping](#rank-level-mapping)

3. [Final Export Table Schema](#final-export-table-schema)  
   3.1. [Overview](#overview)  
   3.2. [Explicit Column List and Descriptions](#explicit-column-list-and-descriptions)  
   3.3. [Conditional Columns: elevation_meters and anomaly_score](#conditional-columns-elevation_meters-and-anomaly_score)  
   3.4. [Example Row / CSV Layout](#example-row--csv-layout)

4. [Supplementary Information](#supplementary-information)  
   4.1. [Data Types and Precision](#data-types-and-precision)  
   4.2. [Indices and Their Purposes](#indices-and-their-purposes)  
   4.3. [Version-Specific Schema Differences](#version-specific-schema-differences)

5. [Appendix: SQL Dumps and \d Outputs](#appendix-sql-dumps-and-d-output)

---

## 1. Core iNaturalist Tables

These tables are imported directly from iNaturalist open data dumps.

### Observations

**Description:**  
Contains each observation record with geospatial and temporal data.

**Key Columns:**

| Column               | Type              | Description |
|----------------------|-------------------|-------------|
| observation_uuid     | uuid              | Unique identifier for each observation. This identifier is used to link photos and can be found on iNaturalist.org. |
| observer_id          | integer           | Identifier for the observer who recorded the observation. |
| latitude             | numeric(15,10)    | Latitude of the observation. High precision (up to 10 digits after the decimal) ensures accuracy. |
| longitude            | numeric(15,10)    | Longitude of the observation. |
| positional_accuracy  | integer           | Uncertainty in meters for the location. |
| taxon_id             | integer           | Identifier linking the observation to a taxon. |
| quality_grade        | varchar(255)      | Observation quality, e.g., "research", "casual", or "needs_id". |
| observed_on          | date              | Date when the observation was made. |
| anomaly_score        | numeric(15,6)     | A computed metric for anomaly detection; available in releases r1 and later. |
| geom                 | geometry          | Geospatial column computed from latitude and longitude. |
| origin               | varchar(255)      | Metadata field populated during ingestion. |
| version              | varchar(255)      | Database structure version. |
| release              | varchar(255)      | Data release identifier. |
| **elevation_meters** | **numeric(10,2)** | *Optional:* Elevation value in meters (if elevation processing is enabled). |

### Photos

**Description:**  
Contains metadata for photos associated with observations.

**Key Columns:**

| Column           | Type           | Description |
|------------------|----------------|-------------|
| photo_uuid       | uuid           | Unique identifier for each photo. |
| photo_id         | integer        | iNaturalist photo ID. |
| observation_uuid | uuid           | Identifier linking the photo to an observation. |
| observer_id      | integer        | Identifier of the observer who took the photo. |
| extension        | varchar(5)     | Image file format (e.g., "jpeg"). |
| license          | varchar(255)   | Licensing information (e.g., Creative Commons). |
| width            | smallint       | Photo width in pixels. |
| height           | smallint       | Photo height in pixels. |
| position         | smallint       | Indicates the order of photos for an observation (position 0 indicates primary photo). |
| origin           | varchar(255)   | Metadata field. |
| version          | varchar(255)   | Database structure version. |
| release          | varchar(255)   | Data release identifier. |

### Observers

**Description:**  
Contains information about the users (observers) who record observations.

**Key Columns:**

| Column      | Type         | Description |
|-------------|--------------|-------------|
| observer_id | integer      | Unique identifier for each observer. |
| login       | varchar(255) | Unique login/username. |
| name        | varchar(255) | Observer's personal name (if provided). |
| origin      | varchar(255) | Metadata field. |
| version     | varchar(255) | Database structure version. |
| release     | varchar(255) | Data release identifier. |

### Taxa

**Description:**  
Contains the taxonomy as provided by iNaturalist.

**Key Columns:**

| Column     | Type              | Description |
|------------|-------------------|-------------|
| taxon_id   | integer           | Unique taxon identifier. |
| ancestry   | varchar(255)      | Encoded ancestral hierarchy (delimited by backslashes). |
| rank_level | double precision  | Numeric level indicating taxonomic rank. |
| rank       | varchar(255)      | Taxonomic rank (e.g., "species", "genus"). |
| name       | varchar(255)      | Scientific name of the taxon. |
| active     | boolean           | Indicates if the taxon is active in the taxonomy. |
| origin     | varchar(255)      | Metadata field. |
| version    | varchar(255)      | Database structure version. |
| release    | varchar(255)      | Data release identifier. |

---

## 2. Expanded Taxa Table

### Purpose and Generation

The **expanded_taxa** table is generated from the iNaturalist `taxa` table by the `expand_taxa.sh` script. Its purpose is to unpack the single-column ancestry string into discrete columns (e.g., `L5_taxonID`, `L5_name`, `L5_commonName`, etc.) so that clade-based filtering and ancestor lookups can be performed efficiently without resorting to recursive string parsing.

### Schema Details

**Core Columns:**

| Column      | Type              | Description |
|-------------|-------------------|-------------|
| taxonID     | integer           | Primary key; unique taxon identifier. |
| rankLevel   | double precision  | Numeric indicator of the taxonomic rank. |
| rank        | varchar(255)      | Taxonomic rank label. |
| name        | varchar(255)      | Scientific name of the taxon. |
| taxonActive | boolean           | Indicates whether the taxon is active. |

**Expanded Columns:**

For each rank level in the set `{5, 10, 11, 12, 13, 15, 20, 24, 25, 26, 27, 30, 32, 33, 33.5, 34, 34.5, 35, 37, 40, 43, 44, 45, 47, 50, 53, 57, 60, 67, 70}`, the following columns are added:

- `L{level}_taxonID` (integer)
- `L{level}_name` (varchar(255))
- `L{level}_commonName` (varchar(255))

For example, for rank level 10:
- `L10_taxonID`
- `L10_name`
- `L10_commonName`

### Indexing and Performance Considerations

Indexes are created on the most frequently queried expanded columns (typically on `L10_taxonID`, `L20_taxonID`, , `L70_taxonID`) as well as on the base columns (`taxonID`, `rankLevel`, and `name`). These indexes help to optimize the clade filtering and ancestor lookups performed by the export pipeline.

### Rank-Level Mapping

A supplemental mapping (provided in `clade_helpers.sh`) maps the column prefixes to human-readable rank names. For example:

| Prefix | Rank         |
|--------|--------------|
| L5     | subspecies   |
| L10    | species      |
| L20    | genus        |
| L40    | order        |
| L50    | class        |
| L70    | kingdom      |

A complete mapping is maintained in the code to facilitate any dynamic filtering or display of taxonomic information.

---

## 3. Final Export Table Schema

### Overview

The final export table is generated by the export pipeline (primarily via `cladistic.sh`) and is used for downstream applications such as training specimen identification models. This table is created by joining observations with photo metadata and taxonomic data from `expanded_taxa`. It includes additional computed columns for quality filtering and sampling.

### Explicit Column List and Descriptions

The final export table (named `<EXPORT_GROUP>_observations`) contains the following columns:

#### From the Observations Table

| Column               | Type              | Description |
|----------------------|-------------------|-------------|
| observation_uuid     | uuid              | Unique observation identifier. |
| observer_id          | integer           | Observer identifier. |
| latitude             | numeric(15,10)    | Latitude of the observation. |
| longitude            | numeric(15,10)    | Longitude of the observation. |
| **elevation_meters** | **numeric(10,2)** | *Optional:* Elevation in meters (included if `INCLUDE_ELEVATION_EXPORT=true`). |
| positional_accuracy  | integer           | Location uncertainty in meters. |
| taxon_id             | integer           | Identifier linking to the taxon. |
| quality_grade        | varchar(255)      | Quality grade (e.g., "research"). |
| observed_on          | date              | Date of observation. |
| anomaly_score        | numeric(15,6)     | Anomaly score (only available in r1 and later). |
| in_region            | boolean           | Computed flag indicating if the observation lies within the region bounding box. |
| expanded_taxonID     | integer           | Taxon ID from the expanded_taxa table. |
| expanded_rankLevel   | double precision  | Rank level from expanded_taxa. |
| expanded_name        | varchar(255)      | Taxon name from expanded_taxa. |
| L5_taxonID  L70_taxonID | integer       | A series of columns representing the taxonomic ancestry at various rank levels (e.g., L5_taxonID, L10_taxonID, , L70_taxonID). |

#### From the Photos Table

| Column       | Type           | Description |
|--------------|----------------|-------------|
| photo_uuid   | uuid           | Unique photo identifier. |
| photo_id     | integer        | Photo identifier (from iNaturalist). |
| extension    | varchar(5)     | Image file format (e.g., "jpeg"). |
| license      | varchar(255)   | Licensing information. |
| width        | smallint       | Photo width (in pixels). |
| height       | smallint       | Photo height (in pixels). |
| position     | smallint       | Photo order indicator (primary photo has position 0). |

#### Additional Computed Column

| Column | Type    | Description |
|--------|---------|-------------|
| rn     | bigint  | Row number (per species partition based on `L10_taxonID`) used to cap the number of research-grade observations per species (controlled by `MAX_RN`). |

### Conditional Columns: elevation_meters and anomaly_score

- **elevation_meters:**  
  This column is included in the final export if the environment variable `INCLUDE_ELEVATION_EXPORT` is set to true and if the current release is not `"r0"`. It is positioned immediately after the `longitude` column.

- **anomaly_score:**  
  Present only in releases where it has been added (e.g., `r1` onward).

### Example Row / CSV Layout

An exported CSV row (tab-delimited) might be structured as follows:

```
observation_uuid    observer_id    latitude    longitude    elevation_meters    positional_accuracy    taxon_id    quality_grade    observed_on    anomaly_score    in_region    expanded_taxonID    expanded_rankLevel    expanded_name    L5_taxonID    L10_taxonID    ...    L70_taxonID    photo_uuid    photo_id    extension    license    width    height    position    rn
```

Each observation row is linked to one or more photo rows; the export process uses a partition-based random sampling (per species) so that only a maximum of `MAX_RN` research-grade observations per species are included.

---

## 4. Supplementary Information

### Data Types and Precision

- **Latitude and Longitude:** Stored as `numeric(15,10)`, which provides high precision (up to 10 digits after the decimal) ensuring accurate geolocation.
- **elevation_meters:** Stored as `numeric(10,2)`, capturing elevation with two decimal places.
- **anomaly_score:** Stored as `numeric(15,6)` for precise anomaly measurements.
- Standard PostgreSQL data types are used for other columns as specified.

### Indices and Their Purposes

- **Core Tables:**  
  Primary keys and indexes are created on identifiers (e.g., `observation_uuid`, `photo_uuid`, `taxon_id`) and frequently queried columns.
- **Observations Geometry:**  
  A GIST index is created on the `geom` column for fast spatial queries.
- **Expanded_Taxa:**  
  Additional indexes are created on key expanded ancestry columns (e.g., `L10_taxonID`, `L20_taxonID`, , `L70_taxonID`) to optimize clade-based filtering.

### Version-Specific Schema Differences

- **Releases prior to r1:**  
  May not include `anomaly_score` and `elevation_meters`.
- **Current and Future Releases:**  
  Include these columns. Future schema changes will be documented here as needed.

---

## 5. Appendix: SQL Dumps and \d Outputs

Below are example outputs from PostgreSQLs `\d` command for key tables. These serve as a quick reference for the column names and types.

### Observations Table

```sql
-- \d observations
       Column        |          Type          
---------------------+------------------------
 observation_uuid    | uuid                  
 observer_id         | integer               
 latitude            | numeric(15,10)        
 longitude           | numeric(15,10)        
 positional_accuracy | integer               
 taxon_id            | integer               
 quality_grade       | varchar(255)          
 observed_on         | date                  
 anomaly_score       | numeric(15,6)         
 geom                | geometry              
 origin              | varchar(255)          
 version             | varchar(255)          
 release             | varchar(255)
 elevation_meters    | numeric(10,2)         -- Present if enabled
```

### Photos Table

```sql
-- \d photos
      Column      |          Type          
------------------+------------------------
 photo_uuid       | uuid                   
 photo_id         | integer                
 observation_uuid | uuid                   
 observer_id      | integer                
 extension        | varchar(5)             
 license          | varchar(255)           
 width            | smallint               
 height           | smallint               
 position         | smallint               
 origin           | varchar(255)           
 version          | varchar(255)           
 release          | varchar(255)
```

### Observers Table

```sql
-- \d observers
   Column    |          Type          
-------------+------------------------
 observer_id | integer                
 login       | varchar(255)           
 name        | varchar(255)           
 origin      | varchar(255)           
 version     | varchar(255)           
 release     | varchar(255)
```

### Taxa Table

```sql
-- \d taxa
   Column   |          Type          
------------+------------------------
 taxon_id   | integer                
 ancestry   | varchar(255)           
 rank_level | double precision       
 rank       | varchar(255)           
 name       | varchar(255)           
 active     | boolean                
 origin     | varchar(255)           
 version    | varchar(255)           
 release    | varchar(255)
```

### Expanded_Taxa Table

```sql
-- \d "expanded_taxa"
      Column      |          Type          
------------------+------------------------
 taxonID          | integer    (PK)
 rankLevel        | double precision       
 rank             | varchar(255)           
 name             | varchar(255)           
 taxonActive      | boolean                
 L5_taxonID       | integer                
 L5_name          | varchar(255)           
 L5_commonName    | varchar(255)           
 L10_taxonID      | integer                
 L10_name         | varchar(255)           
 L10_commonName   | varchar(255)           
 ...              | ...                    
 L70_taxonID      | integer                
```

### Final Export Table (Example)

Assuming the export group is named `amphibia_all_exc_nonrg_sp_oor_elev`, an example output is:

```sql
-- \d "amphibia_all_exc_nonrg_sp_oor_elev_observations"
       Column              |          Type          
-----------------------------+------------------------
 observation_uuid            | uuid                  
 observer_id                 | integer               
 latitude                    | numeric(15,10)        
 longitude                   | numeric(15,10)        
 elevation_meters            | numeric(10,2)         -- Only if enabled
 positional_accuracy         | integer               
 taxon_id                    | integer               
 quality_grade               | varchar(255)          
 observed_on                 | date                  
 anomaly_score               | numeric(15,6)         -- Only for r1 and later
 in_region                   | boolean               
 expanded_taxonID            | integer               
 expanded_rankLevel          | double precision       
 expanded_name               | varchar(255)           
 L5_taxonID                  | integer               
 L10_taxonID                 | integer               
 ...                         | ...                   
 L70_taxonID                 | integer               
 photo_uuid                  | uuid                  
 photo_id                    | integer               
 extension                   | varchar(5)            
 license                     | varchar(255)          
 width                       | smallint              
 height                      | smallint              
 position                    | smallint              
 rn                          | bigint                -- For internal sampling
```

---

## Final Notes

- This document serves as the definitive reference for all table schemas within ibridaDB (other than intermediate tables).  
- It is essential for developers working on downstream processing, migration, or debugging tasks.  
- As the system evolves (new releases, additional columns, or modifications to processing logic), please update this document to maintain an accurate reference.
  - NOTE: 'regional base' tables are not documented here but quite likely should be. Necessary for debugging and understanding advances features like ancestor-aware (ancestor search), out-of-region (oor) observations of in-region taxa, etc.
</file: ../../../docs/schemas.md>

<file: ../../../docs/export.md>
# ibridaDB Export Reference (v1)

This document describes how to configure and run an **ibridaDB** export job using our current export pipeline. The export process is driven by a set of **environment variables** that control which observations are included, how they are filtered, and where the outputs are written. These variables are typically set in a releasespecific wrapper script (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`).

> **Note:** This document has been updated to include new functionality such as ancestorbased filtering, partial rank wiping, the `in_region` flag with outofregion observation handling, and an optional inclusion of elevation data (via the `elevation_meters` column). See also the [Ingest Documentation](../ingest/INGEST.md) for details on how elevation data is integrated into the database.

---

## Table of Contents

- [ibridaDB Export Reference (v1)](#ibridadb-export-reference-v1)
  - [Table of Contents](#table-of-contents)
  - [Introduction \& Pipeline Overview](#introduction--pipeline-overview)
  - [Quick Start](#quick-start)
  - [Environment Variables](#environment-variables)
    - [Database Configuration](#database-configuration)
    - [Export Parameters](#export-parameters)
    - [Additional Flags and Advanced Settings](#additional-flags-and-advanced-settings)
    - [Paths](#paths)
  - [Export Flow \& Scripts](#export-flow--scripts)
  - [Output Files](#output-files)
  - [Future Work and Notes](#future-work-and-notes)

---

## Introduction & Pipeline Overview

The **ibridaDB Export Pipeline** extracts curated subsets of observations from a spatially enabled PostgreSQL/PostGIS database. It is designed to support advanced filtering based on:

- **Geographic Region:** Defined by a bounding box corresponding to a region tag (e.g., `"NAfull"`).
- **Species Observations:** A minimum number of research-grade observations per species (`MIN_OBS`) is required to be included.
- **Taxonomic Clade:** Filtering by a clade, metaclade, or macroclade is performed based on definitions in `clade_defns.sh`.
- **Quality & Ancestor Filtering:** The pipeline supports research-grade filtering (`RG_FILTER_MODE`), optional partial-rank wiping (using `MIN_OCCURRENCES_PER_RANK` and `INCLUDE_MINOR_RANKS_IN_ANCESTORS`), and computes an `in_region` boolean for each observation.
- **Elevation Data (Optional):** When enabled (via `INCLUDE_ELEVATION_EXPORT`), the final exported CSV will include an `elevation_meters` column immediately after `longitude`.

The export process follows these broad stages:

1. **Wrapper Script:** Sets all required environment variables.
2. **Main Export Script (`common/main.sh`):**
   - Validates variables and creates the export directory.
   - Ensures necessary PostgreSQL extensions and roles exist.
   - Calls **regional_base.sh** to generate region-based tables.
   - Invokes **cladistic.sh** to filter by clade and produce the final export table.
   - Generates a summary file with environment details and final statistics.
3. **Regional Base Generation (`common/regional_base.sh`):**
   - Sets the regions bounding box.
   - Creates tables that capture species meeting the `MIN_OBS` threshold and computes an `in_region` flag.
   - Builds ancestor tables based on clade definitions.
4. **Cladistic Filtering & CSV Export (`common/cladistic.sh`):**
   - Joins the regional base observations to taxonomic data from `expanded_taxa`.
   - Applies research-grade filters, partial-rank wiping, and random sampling (using `MAX_RN`) per species.
   - Exports a final CSV that explicitly lists observation and photo columns (including `elevation_meters` if enabled).

A high-level diagram of the export flow is shown below:

```mermaid
flowchart TB
    A["Wrapper Script<br/>(e.g., r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh)"] --> B["Main Export Script<br/>(common/main.sh)"]
    B --> C{"SKIP_REGIONAL_BASE?"}
    C -- "true" --> D["Reuse Existing Regional Base Tables"]
    C -- "false" --> E["regional_base.sh<br/>(Create/Update Regional Base)"]
    D --> F["cladistic.sh<br/>(Filter Observations & Export CSV)"]
    E --> F["cladistic.sh<br/>(Filter Observations & Export CSV)"]
    F --> G["Export Summary & Log Generation"]
```

---

## Quick Start

1. **Clone or navigate** to the `dbTools/export/v0` directory.
2. **Configure a wrapper script** (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`) to set your export parameters. For example:
   - Set region-specific parameters: `REGION_TAG`, `MIN_OBS`, `MAX_RN`, and `PRIMARY_ONLY`.
   - Specify the clade filter: `CLADE` or `METACLADE`.
   - Enable out-of-region observation handling: `INCLUDE_OUT_OF_REGION_OBS`.
   - Optionally, enable elevation export: `INCLUDE_ELEVATION_EXPORT=true` (if your database includes elevation data).
3. **Run the wrapper script** to initiate the export process:
   ```bash
   ./r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
   ```
4. **Review the output**:
   - A CSV file named `<EXPORT_GROUP>_photos.csv` will be saved to the export directory.
   - A summary file (`<EXPORT_GROUP>_export_summary.txt`) will be generated containing export parameters, statistics, and timing information.

---

## Environment Variables

The export pipeline is configured via several environment variables, which are typically set in your wrapper script.

### Database Configuration

- **`DB_USER`**  
  *Description:* PostgreSQL user (e.g., `"postgres"`).

- **`VERSION_VALUE`**  
  *Description:* Database version identifier (e.g., `"v0"`).

- **`RELEASE_VALUE`**  
  *Description:* Data release identifier (e.g., `"r1"`).  
  *Note:* For older releases (e.g., `"r0"`), certain features (such as `anomaly_score` and `elevation_meters`) may be absent.

- **`ORIGIN_VALUE`**  
  *Description:* Data provenance (e.g., `"iNat-Dec2024"`).

- **`DB_NAME`**  
  *Description:* Name of the database (e.g., `"ibrida-v0-r1"`).

- **`DB_CONTAINER`**  
  *Description:* Name of the Docker container running PostgreSQL (e.g., `"ibridaDB"`).

### Export Parameters

- **`REGION_TAG`**  
  *Description:* A key that defines the regions bounding box (e.g., `"NAfull"`).  
  *Usage:* Used in `regional_base.sh` to set geographic coordinates.

- **`MIN_OBS`**  
  *Description:* Minimum number of research-grade observations required per species for inclusion.  
  *Default:* `50`.

- **`MAX_RN`**  
  *Description:* Maximum number of research-grade observations to sample per species in the final CSV.  
  *Default:* `2500` (or your desired value).

- **`PRIMARY_ONLY`**  
  *Description:* If `true`, only the primary photo (position=0) is included; if `false`, all photos are exported.

- **`CLADE` / `METACLADE` / `MACROCLADE`**  
  *Description:* Defines the taxonomic filter. For example, `CLADE="amphibia"` or `METACLADE="pta"` (primary terrestrial arthropods).

- **`EXPORT_GROUP`**  
  *Description:* A label for the final export; used to name the final observations table and CSV file (e.g., `"amphibia_all_exc_nonrg_sp_oor_elev"`).

### Additional Flags and Advanced Settings

- **`PROCESS_OTHER`**  
  *Description:* A generic flag for additional processing (default: `false`).

- **`SKIP_REGIONAL_BASE`**  
  *Description:* If `true`, the export pipeline will reuse existing regional base tables rather than recreating them.

- **`INCLUDE_OUT_OF_REGION_OBS`**  
  *Description:* If `true`, once a species is selected by `MIN_OBS`, all observations for that species (globally) are included; otherwise, only those within the bounding box are used.  
  *Note:* An `in_region` boolean is computed for each observation.

- **`RG_FILTER_MODE`**  
  *Description:* Controls how research-grade versus non-research observations are filtered.  
  *Possible values:* `ONLY_RESEARCH`, `ALL`, `ALL_EXCLUDE_SPECIES_NON_RESEARCH`, `ONLY_NONRESEARCH`, etc.

- **`MIN_OCCURRENCES_PER_RANK`**  
  *Description:* Minimum occurrences required per rank (e.g., L20, L30, L40) before that rank is retained.  
  *Usage:* Used to optionally wipe out low-occurrence partial rank labels.

- **`INCLUDE_MINOR_RANKS_IN_ANCESTORS`**  
  *Description:* If `true`, includes minor ranks in the ancestor search; otherwise, only major ranks are considered.

- **`INCLUDE_ELEVATION_EXPORT`**  
  *Description:* If `true` (the default for new releases), the final export will include the `elevation_meters` column (placed immediately after `longitude`).  
  *Note:* If the underlying database is older (e.g., release `"r0"`), set this to `false`.

### Paths

- **`HOST_EXPORT_BASE_PATH`**  
  *Description:* Host filesystem path where export files will be written (e.g., `"/datasets/ibrida-data/exports"`).

- **`CONTAINER_EXPORT_BASE_PATH`**  
  *Description:* Container path corresponding to `HOST_EXPORT_BASE_PATH` (e.g., `"/exports"`).

- **`EXPORT_SUBDIR`**  
  *Description:* A subdirectory constructed from variables (e.g., `"v0/r1/primary_only_50min_2500max"`).

- **`BASE_DIR`**  
  *Description:* Root directory of the export tools (e.g., `/home/caleb/repo/ibridaDB/dbTools/export/v0`).

---

## Export Flow & Scripts

The export process consists of several key steps:

1. **Wrapper Script:**  
   - A release-specific wrapper (e.g., `r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh`) sets all necessary environment variables (including the new `INCLUDE_ELEVATION_EXPORT` toggle) and then calls the main export script.

2. **Main Export Script (`common/main.sh`):**  
   - Validates required variables and creates the export directory.
   - Installs necessary PostgreSQL extensions (such as `dblink`) and creates roles if needed.
   - Invokes **regional_base.sh** to generate region-specific base tables.
   - Calls **cladistic.sh** to join base tables with the taxonomic hierarchy (from `expanded_taxa`), apply quality and clade filters, and build the final export table.
   - Generates a summary file that documents the export parameters, final observation counts, and timing information.
   - Optionally copies the wrapper script into the output directory for reproducibility.

3. **Regional Base Generation (`common/regional_base.sh`):**  
   - Determines the geographic bounding box using `REGION_TAG` (from `region_defns.sh`).
   - Creates a species table (`<REGION_TAG>_min${MIN_OBS}_all_sp`) of species meeting the minimum observation threshold.
   - Builds an ancestor table (`<REGION_TAG>_min${MIN_OBS}_all_sp_and_ancestors_<cladeID>_<mode>`) by unrolling taxon ancestry for the selected species.
   - Generates a second table (`<REGION_TAG>_min${MIN_OBS}_sp_and_ancestors_obs_<cladeID>_<mode>`) that contains all observations for the selected species along with an `in_region` boolean flag.
   - The `INCLUDE_OUT_OF_REGION_OBS` flag governs whether the observation table is filtered by the bounding box or not.

4. **Cladistic Filtering & CSV Export (`common/cladistic.sh`):**  
   - Joins the observation table with `expanded_taxa` using clade conditions defined in `clade_defns.sh` (and processed by `clade_helpers.sh`).
   - Applies research-grade filtering based on `RG_FILTER_MODE` and uses a partition-based random sampling (controlled by `MAX_RN` and `PRIMARY_ONLY`).
   - Explicitly enumerates columns in the final export, including `elevation_meters` (if `INCLUDE_ELEVATION_EXPORT=true`), ensuring that the column appears immediately after `longitude`.
   - Exports the final dataset as a CSV file with a header and tab-delimited fields.
   - Debug SQL is executed to confirm the final column list used.

---

## Output Files

After a successful export, you will find:

- A CSV file named `<EXPORT_GROUP>_photos.csv` in the export subdirectory (e.g., `/exports/v0/r1/primary_only_50min_2500max`).
- A summary file named `<EXPORT_GROUP>_export_summary.txt` that documents:
  - The values of key environment variables.
  - Final observation, taxa, and observer counts.
  - Timing information for each stage of the export process.
- Optionally, a copy of the export wrapper script (if `WRAPPER_PATH` is set).

---

## Future Work and Notes

- **Enhanced Documentation:**  
  Future revisions will further detail the logic of ancestor searches and the random sampling strategy used for research-grade observations.
  
- **Additional Filtering Options:**  
  We plan to refine the `RG_FILTER_MODE` and allow further customizations (e.g., combining multiple quality filters).

- **Schema Updates:**  
  As new releases are introduced (e.g., additional columns beyond `anomaly_score` or `elevation_meters`), the export documentation will be updated to reflect the schema changes.

- **User Feedback:**  
  Contributions, bug reports, and suggestions for improvements are welcome.

---

Happy Exporting!
</file: ../../../docs/export.md>

<file: ../../../docs/roadmap.md>

**positional_accuracy** usage
- Thought: perhaps 'smudge' the elevation_meters value; weighted average of elevation tiles within the circle whose radius is the positional accuracy.
</file: ../../../docs/roadmap.md>

<file: ../../../docs/README.md>
# ibridaDB

**ibridaDB** is a modular, reproducible database system designed to ingest, process, and export biodiversity observations from the [iNaturalist open data dumps](https://www.inaturalist.org/). It leverages PostgreSQL with PostGIS to efficiently store and query geospatial data and includes specialized pipelines for:

- **Data Ingestion:** Importing CSV dumps, calculating geospatial geometries, and updating metadata.
- **Elevation Integration:** Optionally enriching observations with elevation data derived from MERIT DEM tiles.
- **Data Export:** Filtering observations by region and taxonomic clade, performing advanced ancestor searches, and exporting curated CSV files for downstream model training.

This repository contains all the code, Docker configurations, and documentation required to build, run, and extend ibridaDB.

---

## Table of Contents

- [ibridaDB](#ibridadb)
  - [Table of Contents](#table-of-contents)
  - [Overview](#overview)
  - [Architecture](#architecture)
  - [Directory Structure](#directory-structure)
  - [Ingestion Pipeline](#ingestion-pipeline)
  - [Elevation Data Integration](#elevation-data-integration)
  - [Export Pipeline](#export-pipeline)
  - [Docker Build and Deployment](#docker-build-and-deployment)
  - [Configuration \& Environment Variables](#configuration--environment-variables)
  - [Adding a New Release](#adding-a-new-release)
  - [Release notes](#release-notes)
  - [License](#license)
  - [Final Notes](#final-notes)

---

## Overview

**ibridaDB** automates the process of:
- Reproducing a spatially enabled database from iNaturalist open data dumps.
- Optionally enriching the database with elevation data from MERIT DEM.
- Exporting curated subsets of observations for downstream training of specimen identification models.

The system is versioned both in terms of **database structure** (Version, e.g., "v0") and **data release** (Release, e.g., "r1"). These concepts allow you to reproduce different releases of the database while keeping the underlying schema consistent.

---

## Architecture

The overall workflow of ibridaDB is divided into three main stages:

1. **Ingestion:**  
   - **CSV Import:** Load observations, photos, taxa, and observers from CSV files.
   - **Geometry Calculation:** Compute geospatial geometries (using latitude/longitude) with PostGIS.
   - **Metadata Update:** Set version, release, and origin metadata on each table.
   - **Optional Elevation Processing:** If enabled, create a PostGIS raster table for DEM tiles, load MERIT DEM data, and update each observation with an elevation value.

2. **Export:**  
   - **Regional Base Tables:** Build tables that restrict species by geographic bounding boxes and minimum observation counts.
   - **Cladistic Filtering:** Further subset the observations based on taxonomic clade (or metaclade) conditions and other quality filters.
   - **Final CSV Export:** Generate CSV files with additional information (including photo metadata) and optionally include elevation data.

3. **Dockerized Deployment:**  
   - The system runs in a Docker container using a custom image that extends the official PostGIS image to include the `raster2pgsql` CLI tool (necessary for elevation data processing).

A high-level diagram of the export flow is shown below:

```mermaid
flowchart TB
    A["Wrapper Script<br/>(e.g., r1/wrapper.sh)"] --> B["Main Export Script<br/>(common/main.sh)"]
    B --> C{"Skip Regional Base?"}
    C -- "true" --> D["Reuse Existing Tables"]
    C -- "false" --> E["regional_base.sh<br/>(Create/Update Base Tables)"]
    D --> F["cladistic.sh<br/>(Apply Cladistic Filters & Export CSV)"]
    E --> F["cladistic.sh<br/>(Apply Cladistic Filters & Export CSV)"]
    F --> G["Export Summary Generated"]
```

---

## Directory Structure

The repository is organized as follows:

```
ibridaDB/
 dbTools/
    ingest/
       v0/
           common/
              geom.sh              # Geometry calculations
              vers_origin.sh       # Version and origin metadata updates
              main.sh              # Core ingestion logic
           r0/                      # Parameters for the initial release (r0)
              wrapper.sh
           r1/                      # Parameters for the r1 release
              wrapper.sh
           utils/
               add_release.sh       # Legacy release update script
               elevation/           # Elevation pipeline tools
                   create_elevation_table.sql
                   create_elevation_table.sh
                   load_dem.sh
                   main.sh          # Orchestrates elevation ingestion
                   update_elevation.sh
                   wrapper.sh
    export/
        v0/
            common/
               functions.sh         # Shared export functions (including get_obs_columns())
               clade_defns.sh         # Clade condition definitions
               clade_helpers.sh       # Helpers for multi-root and clade processing
               regional_base.sh       # Regional base table generation
               cladistic.sh           # Cladistic filtering and final CSV export
               main.sh                # Main export orchestration
            r1/                        # Release-specific export wrappers
                wrapper_amphibia_all_exc_nonrg_sp.sh
                wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
                wrapper_pta_all_exc_nonrg_sp.sh
                wrapper_pta_all_exc_nonrg_sp_full_ancestor_search.sh
 docker/
    Dockerfile                       # Custom Docker image build (with raster2pgsql)
    (other Docker-related files, e.g., docker-compose.yml)
 README.md                            # This high-level documentation
```

---

## Ingestion Pipeline

The ingestion pipeline is contained in `dbTools/ingest/v0/` and performs the following:

1. **Database Initialization:**  
   - Uses wrapper scripts (e.g., `r1/wrapper.sh`) to set parameters (DB name, source info, etc.).
   - The main script (`common/main.sh`) creates the database, imports CSV files, sets up tables and indexes, and computes geometries via `geom.sh`.

2. **Metadata Updates:**  
   - Updates the `origin`, `version`, and `release` columns in each table (via `vers_origin.sh`).

3. **Elevation Integration (Optional):**  
   - When `ENABLE_ELEVATION=true` is set in the wrapper, the elevation pipeline in `utils/elevation/` is invoked.
   - This pipeline creates an `elevation_raster` table, loads MERIT DEM tiles (using `raster2pgsql`), and updates `observations.elevation_meters`.

**Quick Start Example for Ingestion:**

```bash
chmod +x dbTools/ingest/v0/common/main.sh dbTools/ingest/v0/common/geom.sh dbTools/ingest/v0/common/vers_origin.sh dbTools/ingest/v0/r1/wrapper.sh
# To ingest a new release with elevation:
ENABLE_ELEVATION=true dbTools/ingest/v0/r1/wrapper.sh
```

---

## Elevation Data Integration

The elevation pipeline (located in `dbTools/ingest/v0/utils/elevation/`) provides the following functionality:

- **Create Elevation Table:**  
  - Runs `create_elevation_table.sh` to ensure the `elevation_raster` table exists.

- **Load DEM Data:**  
  - Uses `load_dem.sh` to extract and load MERIT DEM tiles into the database using `raster2pgsql`.  
  - **Note:** This requires the custom Docker image built with `raster2pgsql` (see Docker Build section below).

- **Update Elevation:**  
  - Runs `update_elevation.sh` to populate `observations.elevation_meters` using spatial joins with the raster data.
  
The pipeline is activated by setting `ENABLE_ELEVATION=true` in your ingest wrapper.

---

## Export Pipeline

The export pipeline (located in `dbTools/export/v0/`) allows you to generate specialized CSV exports from your ibridaDB database. Key features include:

1. **Regional Base Table Generation:**  
   - `regional_base.sh` creates base tables filtering species by a geographic bounding box (defined by `REGION_TAG`) and by a minimum number of research-grade observations (`MIN_OBS`).
   - It supports an option (`INCLUDE_OUT_OF_REGION_OBS`) to include all observations for selected species, along with computing an `in_region` boolean.

2. **Cladistic Filtering:**  
   - `cladistic.sh` further filters the regional observations by taxonomic clade (using `CLADE`, `METACLADE`, or `MACROCLADE` defined in `clade_defns.sh`).
   - The script also supports advanced options such as partial rank wiping (using `MIN_OCCURRENCES_PER_RANK` and `INCLUDE_MINOR_RANKS_IN_ANCESTORS`) and research-grade filtering (using `RG_FILTER_MODE`).

3. **CSV Export:**  
   - The final CSV export includes explicit columns from the observations and photo tables.
   - A new column `elevation_meters` is now included in the export if enabled by the environment variable `INCLUDE_ELEVATION_EXPORT` (set to true by default for new releases).
   - The CSV is produced via a partition-based random sampling method, ensuring that in-region research-grade observations are preferentially selected (controlled by `PRIMARY_ONLY` and `MAX_RN`).

**Quick Start Example for Export:**

```bash
chmod +x dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
# Run export (with elevation enabled) using the wrapper:
dbTools/export/v0/r1/wrapper_amphibia_all_exc_nonrg_sp_oor_elev.sh
```

---

## Docker Build and Deployment

**Custom Docker Image:**

The official PostGIS Docker image does not include the `raster2pgsql` CLI tool. To support elevation processing, we build a custom image that:

- Uses a multi-stage Docker build to install the PostGIS package (which includes `raster2pgsql`) in a builder stage.
- Copies the `raster2pgsql` binary into the final image.
- Is used in our Docker Compose configuration.

**Build and Push Instructions:**

```bash
cd docker
docker build -t frontierkodiak/ibridadb:latest . --no-cache
docker login
docker push frontierkodiak/ibridadb:latest
```

**Docker Compose:**

Our `docker-compose.yml` maps necessary volumes for:
- Exporting data (`/datasets/ibrida-data/exports`)
- Ingesting metadata (`/datasets/ibrida-data/intake`)
- Providing DEM data (`/datasets/dem`)

Refer to the `docker/stausee/docker-compose.yml` file for details.

---

## Configuration & Environment Variables

Both the ingest and export pipelines are configured via a rich set of environment variables. Key ones include:

- **Database Configuration:**  
  - `DB_USER`, `VERSION_VALUE`, `RELEASE_VALUE`, `ORIGIN_VALUE`, `DB_NAME`, `DB_CONTAINER`

- **Ingestion-Specific:**  
  - `SOURCE`, `METADATA_PATH`, `ENABLE_ELEVATION`, `DEM_DIR`, `EPSG`, `TILE_SIZE`

- **Export-Specific:**  
  - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`
  - Taxonomic filters: `CLADE`, `METACLADE`, `MACROCLADE`
  - Advanced export toggles:  
    - `INCLUDE_OUT_OF_REGION_OBS`  
    - `RG_FILTER_MODE`  
    - `MIN_OCCURRENCES_PER_RANK`  
    - `INCLUDE_MINOR_RANKS_IN_ANCESTORS`  
    - **`INCLUDE_ELEVATION_EXPORT`**  controls whether `elevation_meters` is included in the final export.

- **Paths:**  
  - `HOST_EXPORT_BASE_PATH`, `CONTAINER_EXPORT_BASE_PATH`, `EXPORT_SUBDIR`, `BASE_DIR`

For full details, please consult the export and ingest wrapper scripts in their respective directories.

---

## Adding a New Release

To add a new data release:
1. Create a new release directory (e.g., `dbTools/ingest/v0/r2/` and `dbTools/export/v0/r2/`).
2. Copy an existing wrapper script (from r1) into the new directory.
3. Update parameters such as:
   - `SOURCE` (e.g., change from "Dec2024" to "Feb2025")
   - `RELEASE_VALUE` (e.g., from "r1" to "r2")
   - Any new configuration (e.g., enable elevation by setting `ENABLE_ELEVATION=true` in ingest and `INCLUDE_ELEVATION_EXPORT=true` in export).
4. Run the ingestion/export processes using the new wrapper scripts.

## Release notes

Current versions:
- v0r0: June 2024 iNat data release
- v0r1: December 2024 iNat data release (adds anomaly_score column to observations table)
  - (in-place update) added elevation_meters column to observations tables
- v0r2: February 2025 iNat data release (built with elevation data)

---

## License

[Insert License Information Here]

---

## Final Notes

- **Documentation Updates:**  
  This README provides a high-level overview. For detailed configuration of the ingest and export pipelines, please refer to:
  - `dbTools/ingest/v0/INGEST.md` (in progress)  
  - `dbTools/export/v0/export.md`

- **Contributions:**  
  Contributions and suggestions are welcome. Please submit pull requests or open issues if you encounter problems or have ideas for enhancements.

---

Happy Ingesting and Exporting!
</file: ../../../docs/README.md>

<file: ../../../docs/ingest.md>
# ibridaDB Ingestion Documentation

This document provides a detailed overview of the ibridaDB ingestion pipeline. It covers the steps to load iNaturalist data into a spatially enabled PostgreSQL/PostGIS database, how the elevation integration is handled, and how to run the ingestion process using the provided wrapper scripts.

---

## 1. Overview of the Ingestion Flow

The ingestion pipeline for ibridaDB is designed to:
- **Initialize the Database:** Create a new database using a template (typically a PostGIS-enabled template).
- **Import Data:** Load CSV data for observations, photos, taxa, and observers from iNaturalist data dumps.
- **Compute Geometries:** Generate geospatial geometry columns from the latitude and longitude fields.
- **Update Metadata:** Populate additional columns such as `origin`, `version`, and `release` on each table.
- **Integrate Elevation (Optional):**  
  - **Fresh Ingestion Scenario:** If you are initializing a new database and want to include elevation data, the main ingestion script (in `common/main.sh`) can trigger the elevation pipeline automatically when `ENABLE_ELEVATION=true` is set.
  - **Existing Database Scenario:** If your database already exists and lacks elevation data, you can run the elevation wrapper (in `utils/elevation/wrapper.sh`) to update the observations with DEM-derived elevation values.

After the ingestion steps are complete, the database is ready for further processing or for exporting subsets of observations using the export pipeline.

---

## 2. Environment Variables

The ingestion pipeline is controlled by several environment variables. The key ones include:

### Database and General Settings
- **`DB_USER`**  
  PostgreSQL user (typically `"postgres"`).

- **`DB_TEMPLATE`**  
  Template database name (e.g., `"template_postgis"`) used to create the new database.

- **`NUM_PROCESSES`**  
  Number of parallel processes to use for tasks such as geometry calculations and metadata updates.

- **`BASE_DIR`**  
  Root directory of the ingestion tools (e.g., `/home/caleb/repo/ibridaDB/dbTools/ingest/v0`).

- **`SOURCE`**  
  Source identifier for the iNaturalist data (e.g., `"Dec2024"`, `"Feb2025"`).

- **`METADATA_PATH`**  
  Path to the CSV files containing iNaturalist data (e.g., `/datasets/ibrida-data/intake/Dec2024`).

### Versioning and Release
- **`ORIGIN_VALUE`**  
  Describes the data provenance (e.g., `"iNat-Dec2024"`).

- **`VERSION_VALUE`**  
  Database version identifier (e.g., `"v0"`).

- **`RELEASE_VALUE`**  
  Data release identifier (e.g., `"r1"`).

- **`DB_NAME`**  
  Name of the new database (e.g., `"ibrida-v0-r1"`).

- **`DB_CONTAINER`**  
  Name of the Docker container running PostgreSQL (e.g., `"ibridaDB"`).

### Elevation Integration Settings
- **`ENABLE_ELEVATION`**  
  If set to `"true"`, the ingestion process will invoke the elevation pipeline.  
  - *Fresh Ingestion:* When creating a new database, the main ingestion script calls the elevation pipeline after geometry calculation.  
  - *Existing DB Update:* You may also run the elevation wrapper separately to add or update elevation values.
- **`DEM_DIR`**  
  Path to the directory containing the MERIT DEM `.tar` files (e.g., `"/datasets/dem/merit"`).
- **`EPSG`**  
  EPSG code for the DEM data (default: `"4326"`).
- **`TILE_SIZE`**  
  Tile size for processing DEM data (default: `"100x100"`).

---

## 3. Ingestion Flow Details

### A. Database Initialization and CSV Import

1. **Database Creation:**  
   - The ingestion pipeline starts by dropping any existing database with the target name and creating a fresh database using the specified template.
2. **Table Creation:**  
   - Tables are created using a provided structure SQL file (e.g., `r1/structure.sql`).
3. **Data Import:**  
   - The pipeline imports CSV files for observations, photos, taxa, and observers into the respective tables using PostgreSQLs `COPY` command.
4. **Index Creation:**  
   - Key indexes are created to optimize spatial and text-based queries (including geospatial indexes on the geometry column).

### B. Geometry Calculation

- The script `common/geom.sh` is executed in parallel to compute the `geom` column on the `observations` table from the `latitude` and `longitude` fields.
- A PostGIS GIST index is then created on the new geometry column to speed up spatial queries.

### C. Metadata Update

- The script `common/vers_origin.sh` is run to add and populate the `origin`, `version`, and `release` columns on all tables.
- This process is executed in parallel across tables to speed up the update.

### D. Elevation Integration

The elevation pipeline can be integrated in one of two ways:

#### 1. During Fresh Ingestion
- **Integration via Main Ingestion Script:**  
  When the environment variable `ENABLE_ELEVATION` is set to `"true"`, the main ingestion script (`common/main.sh`) calls the elevation pipeline after geometry calculation. This pipeline performs the following steps:
  - **Create Elevation Table:**  
    The script `utils/elevation/create_elevation_table.sh` ensures that the `elevation_raster` table exists.
  - **Load DEM Data:**  
    The script `utils/elevation/load_dem.sh` extracts DEM tiles from `.tar` archives and loads them into the `elevation_raster` table using `raster2pgsql` (which requires the custom Docker image).
  - **Update Elevation Values:**  
    Finally, `utils/elevation/update_elevation.sh` updates the `observations.elevation_meters` column for each observation based on a spatial join with the DEM data.

#### 2. Updating an Existing Database
- **Separate Elevation Wrapper:**  
  If you have an existing database (e.g., from a previous release) and you need to add or update elevation values, you can run the elevation wrapper script located at `utils/elevation/wrapper.sh`. This script sets the appropriate environment variables and calls the elevation main script to update the database.

---

## 4. Example Wrapper Usage

Below is an example wrapper script for a fresh ingestion (with elevation enabled):

```bash
#!/bin/bash
# Example: dbTools/ingest/v0/r1/wrapper.sh

# Setup logging
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/wrapper_$(date +%Y%m%d_%H%M%S).log"
echo "Starting ingestion at $(date)" > "${LOG_FILE}"

# Export environment variables
export DB_USER="postgres"
export DB_TEMPLATE="template_postgis"
export NUM_PROCESSES=16
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"
export SOURCE="Feb2025"
export METADATA_PATH="/datasets/ibrida-data/intake/Feb2025"
export ORIGIN_VALUE="iNat-Feb2025"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r2"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"
export DB_CONTAINER="ibridaDB"
export STRUCTURE_SQL="${BASE_DIR}/r2/structure.sql"

# Enable elevation integration in this ingestion
export ENABLE_ELEVATION=true
export DEM_DIR="/datasets/dem/merit"
export EPSG="4326"
export TILE_SIZE="100x100"

# Execute the main ingestion script
/home/caleb/repo/ibridaDB/dbTools/ingest/v0/common/main.sh

# End of wrapper
echo "Ingestion process complete at $(date)" >> "${LOG_FILE}"
```

For an existing database update, simply run the elevation wrapper:

```bash
#!/bin/bash
# Example: dbTools/ingest/v0/utils/elevation/wrapper.sh

export DB_NAME="ibrida-v0-r1"
export DB_USER="postgres"
export DB_CONTAINER="ibridaDB"
export DEM_DIR="/datasets/dem/merit"
export NUM_PROCESSES=16
export EPSG="4326"
export TILE_SIZE="100x100"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/ingest/v0"

./utils/elevation/main.sh "$DB_NAME" "$DB_USER" "$DB_CONTAINER" "$DEM_DIR" "$NUM_PROCESSES" "$EPSG" "$TILE_SIZE"
```

---

## 5. Verifying Success

After running the ingestion process, verify that:

- The database is created with all required tables and indexes.
- The CSV files have been successfully imported into the tables.
- The `geom` column on `observations` is correctly computed and indexed.
- If elevation was enabled, the `elevation_raster` table exists and the `observations.elevation_meters` column is populated (for rows with valid DEM coverage).
- Log messages and notifications (if configured) confirm each steps completion.

---

## Final Notes

- This document covers the ingestion side of ibridaDB. For exporting subsets of the data, please see [export.md](../export/export.md) for detailed instructions on the export pipeline.
- Ensure that you use the custom Docker image (with `raster2pgsql` installed) when running ingestion with elevation data.
- For new releases, adjust your wrapper scripts as needed and consider setting up separate directories (e.g., r1, r2) to maintain versioning consistency.

Happy Ingesting!
</file: ../../../docs/ingest.md>

<file: ../../../docker/Dockerfile>
# syntax=docker/dockerfile:1
#
# Dockerfile for extending the official PostGIS image with raster2pgsql.
# The official image (postgis/postgis:15-3.3) does not include the CLI tools
# (like raster2pgsql) by default in its runtime environment.
# This multi-stage build installs the PostGIS package (which provides the CLI tools)
# in a builder stage and then copies the raster2pgsql binary from /usr/bin in the builder
# into /usr/bin in the final image.
#
# Build and tag the image as:
#   docker build -t frontierkodiak/ibridadb:latest ./docker
#
# Then push it to Docker Hub:
#   docker login
#   docker push frontierkodiak/ibridadb:latest
#

##########################
# Stage 1: Builder
##########################
FROM postgis/postgis:15-3.3 AS builder
USER root

# Update apt and install the postgis package (which provides raster2pgsql).
RUN apt-get update && \
    apt-get install -y postgis && \
    rm -rf /var/lib/apt/lists/*

# Confirm location of raster2pgsql.
# On our system, it is installed in /usr/bin.
RUN which raster2pgsql || true

##########################
# Stage 2: Final Image
##########################
FROM postgis/postgis:15-3.3
USER root

# Copy the raster2pgsql binary from the builder stage.
COPY --from=builder /usr/bin/raster2pgsql /usr/bin/raster2pgsql

# Ensure the binary is executable.
RUN chmod +x /usr/bin/raster2pgsql

# Switch to the default postgres user.
USER postgres
</file: ../../../docker/Dockerfile>

<file: ../../../docker/stausee/docker-compose.yml>
services:
  ibrida:
    image: frontierkodiak/ibridadb:latest
    user: "998:998"
    shm_size: '16g'
    environment:
      POSTGRES_PASSWORD: ooglyboogly69
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_SHARED_BUFFERS: 8GB
      POSTGRES_WORK_MEM: 2048MB
      POSTGRES_MAINTENANCE_WORK_MEM: 4GB
    volumes:
      - ~/repo/ibridaDB/dbTools:/tool
      - ~/repo/ibridaDB/dbQueries:/query
      - /database/ibridaDB:/var/lib/postgresql/data
      - /datasets/ibrida-data/exports:/exports
      - /datasets/ibrida-data/intake:/metadata
      - /datasets/dem:/dem
    ports:
      - "5432:5432"
    container_name: ibridaDB
</file: ../../../docker/stausee/docker-compose.yml>

</codebase_context>
