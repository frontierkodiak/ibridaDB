Directory tree, stemming from root "/home/caleb/repo/ibridaDB/dbTools/export/v0":
├── common (843 lines)
│   ├── clade_defns.sh (179)
│   ├── cladistic.sh (242)
│   ├── functions.sh (57)
│   ├── main.sh (175)
│   └── regional_base.sh (190)
├── export.md (205)
└── r1 (89)
    │   └── wrapper.sh (89)
----
----
Full Path: export.md

# ibridaDB Export Reference (v1)

This document describes how to configure and run an **ibridaDB** export job using our **v1** pipeline. The pipeline is driven by a set of **environment variables** that control which data are included, how they are filtered, and where the outputs are written. These variables are typically set in a **wrapper script** (e.g., `r1/wrapper.sh`).

Below is an overview of:

- [ibridaDB Export Reference (v1)](#ibridadb-export-reference-v1)
  - [Introduction \& Pipeline Overview](#introduction--pipeline-overview)
  - [Quick Start](#quick-start)
  - [Environment Variables](#environment-variables)
    - [Database Config](#database-config)
    - [Export Parameters](#export-parameters)
    - [Paths](#paths)
  - [Export Flow \& Scripts](#export-flow--scripts)

A **placeholder** for a mermaid diagram is provided below. You can generate or modify the diagram according to your team’s preferences and paste it in there.  

---

## Introduction & Pipeline Overview

The **ibridaDB** export pipeline is designed to subset observations from a large PostgreSQL/PostGIS database based on:

- **Geographic region** (e.g., bounding box for North America).  
- **Minimum number of research-grade observations** required for each species (`MIN_OBS`).  
- **Taxonomic clade** (class, order, or custom “metaclade”).  
- **Export parameters** such as maximum number of photos per species, whether to include only the primary photo or all photos, etc.

The pipeline executes in the following broad stages:

1. **Wrapper Script** (e.g., `r1/wrapper.sh`) sets environment variables to configure the export.  
2. **Main Script** (`common/main.sh`) orchestrates creation of “regional base” tables if needed, then calls the **cladistic** filtering step.  
3. **Regional Base** (`common/regional_base.sh`): Creates two key tables:
   - `<REGION_TAG>_min${MIN_OBS}_all_taxa`, storing species that meet the threshold in the bounding box.  
   - `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs`, storing all observations for those species, possibly including out-of-region if configured.  
4. **Cladistic** (`common/cladistic.sh`): Filters by the chosen clade/metaclade, creating a final table `<EXPORT_GROUP>_observations`. It then exports a CSV using partition-based random sampling.  
5. **Summary & Logging**: `main.sh` writes a single `*_export_summary.txt` that includes relevant environment variables, final observation counts, and other metrics. It also optionally copies the wrapper script for reproducibility.

```mermaid

flowchart TB
    A["Wrapper Script<br/>(r1/wrapper.sh)"] --> B["Main Script<br/>(common/main.sh)"]
    B --> C{"Check<br/>SKIP_REGIONAL_BASE?"}
    C -- "true && table exists" --> E["Reuse existing tables"]
    C -- "false" --> D["regional_base.sh<br/>Create tables"]
    E --> F["cladistic.sh<br/>Filter by clade"]
    D --> F["cladistic.sh<br/>Filter by clade"]
    F --> G["Export CSV + final table"]
    G --> H["main.sh<br/>Write summary<br/>+ copy wrapper"]


```

---

## Quick Start

1. **Clone or navigate** to the `dbTools/export/v0` directory.  
2. **Create/modify** a wrapper script (e.g., `r1/wrapper.sh`) to set your parameters:
   - `REGION_TAG`, `MIN_OBS`, `MAX_RN`, `PRIMARY_ONLY`, etc.  
   - `CLADE` or `METACLADE` if focusing on a particular subset of taxa.  
   - Optionally `INCLUDE_OUT_OF_REGION_OBS` and `RG_FILTER_MODE`.  
3. **Run** the wrapper script. The pipeline will:
   - Create region-based tables (if not skipping).  
   - Join them to `expanded_taxa` for a final set of observations.  
   - Write a CSV with photo metadata.  
   - Dump a summary file describing the final dataset.

---

## Environment Variables

Below are the most commonly used variables. **All** variables are read in the wrapper, then passed to `main.sh` (and subsequently to `regional_base.sh` or `cladistic.sh`).

### Database Config

- **`DB_USER`**  
  - **Description**: PostgreSQL user for executing SQL.  
  - **Default**: `"postgres"`  

- **`VERSION_VALUE`**  
  - **Description**: Data version identifier (e.g. `"v0"`).  
  - **Usage**: Combined with `RELEASE_VALUE` to build `DB_NAME`. Also included in logs and table references.

- **`RELEASE_VALUE`**  
  - **Description**: Additional label for the data release (e.g., `"r1"`).  
  - **Usage**: Combined with `VERSION_VALUE` to create `DB_NAME`. Controls logic in some scripts (e.g., whether to include anomaly_score).

- **`ORIGIN_VALUE`**  
  - **Description**: Describes data provenance (e.g., `"iNat-Dec2024"`).  
  - **Usage**: Logged in summary contexts.

- **`DB_NAME`**  
  - **Description**: Full name of the database. Typically `"ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"`.

---

### Export Parameters

- **`REGION_TAG`**  
  - **Description**: Specifies a broad region whose bounding box is defined in `regional_base.sh`.  
  - **Examples**: `"NAfull"`, `"EURfull"`.  
  - **Usage**: `regional_base.sh` calls `set_region_coordinates()` to set `$XMIN,$YMIN,$XMAX,$YMAX`.

- **`MIN_OBS`**  
  - **Description**: Minimum number of **research-grade** observations required for a species to be included in the region-based tables.  
  - **Usage**: In `regional_base.sh`, we gather species with at least `MIN_OBS` research-grade observations inside the bounding box.  
  - **Default**: `50`.

- **`MAX_RN`**  
  - **Description**: Maximum number of research-grade observations to be sampled per species in the final CSV.  
  - **Usage**: In `cladistic.sh`, we do a partition-based random sampling. Observations at species rank beyond `MAX_RN` are excluded in the final CSV.  
  - **Default**: `4000`.

- **`PRIMARY_ONLY`**  
  - **Description**: If `true`, only export the primary (position=0) photo for each observation; if `false`, include all photos.  
  - **Usage**: In `cladistic.sh`, the final `COPY` statement filters `p.position=0` if `PRIMARY_ONLY=true`.  

- **`CLADE`** / **`METACLADE`** / **`MACROCLADE`**  
  - **Description**: Used to define the clade or group of interest.  
  - **Usage**: In `clade_defns.sh`, we have integer-based conditions for major taxonomic ranks (e.g., `L50_taxonID=3` for birds). If `METACLADE` is set, it overrides `CLADE` or `MACROCLADE`.  
  - **Example**: `METACLADE="terrestrial_arthropods"` or `CLADE="amphibia"`.

- **`EXPORT_GROUP`**  
  - **Description**: The final label for the exported dataset; used to name `<EXPORT_GROUP>_observations` and the CSV.  
  - **Usage**: Also appended to summary logs, e.g., `amphibia_export_summary.txt`.

- **`PROCESS_OTHER`**  
  - **Description**: Generic boolean-like flag (default `false`).  
  - **Usage**: Not heavily used but can gate extra steps if desired.

- **`SKIP_REGIONAL_BASE`**  
  - **Description**: If `true`, skip creating region-based tables if they already exist and are non-empty.  
  - **Usage**: `main.sh` checks for `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs`; if present and non-empty, it is reused.  
  - **Default**: `false`.

- **`INCLUDE_OUT_OF_REGION_OBS`**  
  - **Description**: If `true`, once a species is selected by `MIN_OBS` inside the bounding box, we include **all** observations of that species globally. If `false`, re-apply bounding box in final table.  
  - **Usage**: Set in `regional_base.sh`. Defaults to `true` in most wrappers to increase data coverage.

- **`RG_FILTER_MODE`**  
  - **Description**: Controls how research-grade vs. non-research observations are ultimately included in the final `<EXPORT_GROUP>_observations`.  
  - **Possible Values**:
    1. `ONLY_RESEARCH` — Only research-grade observations (`quality_grade='research'`).
    2. `ALL` — Include all observations, regardless of quality_grade.
    3. `ALL_EXCLUDE_SPECIES_NON_RESEARCH` — Include everything except non-research-grade at the species level (`L10_taxonID` non-null).
    4. `ONLY_NONRESEARCH` — Include only non-research-grade (`quality_grade!='research'`).
    5. `ONLY_NONRESEARCH_EXCLUDE_SPECIES` — Include only non-research-grade, but also exclude species-level assignments (`quality_grade!='research' AND L10_taxonID IS NULL`).
    6. `ONLY_NONRESEARCH_WIPE_SPECIES_LABEL` — Include only non-research-grade, but keep any records with a species-level guess. The `L10_taxonID` is forcibly set to `NULL` so the model does not see a species label.  
  - **Usage**: `cladistic.sh` uses a SQL CASE block to apply the correct filter logic. If the mode is unrecognized, it defaults to `ALL`. The final CSV export currently retains a separate step that filters photos by research-grade for sampling (e.g., `o.quality_grade='research'`), so you may wish to unify that logic if desired.

---

### Paths

- **`DB_CONTAINER`**  
  - **Description**: Docker container name running PostgreSQL (often `"ibridaDB"`).  
  - **Usage**: Scripts run `docker exec ${DB_CONTAINER} psql ...`.

- **`HOST_EXPORT_BASE_PATH`**  
  - **Description**: Host filesystem path to store exports.  
  - **Default**: `"/datasets/ibrida-data/exports"`.

- **`CONTAINER_EXPORT_BASE_PATH`**  
  - **Description**: Container path mapping to `HOST_EXPORT_BASE_PATH`.  
  - **Default**: `"/exports"`.

- **`EXPORT_SUBDIR`**  
  - **Description**: A subdirectory typically combining `VERSION_VALUE`, `RELEASE_VALUE`, and other parameters (e.g., `"v0/r1/primary_only_50min_4000max"`).  
  - **Usage**: `main.sh` assembles final output paths from `CONTAINER_EXPORT_BASE_PATH/$EXPORT_SUBDIR` and `HOST_EXPORT_BASE_PATH/$EXPORT_SUBDIR`.

- **`BASE_DIR`**  
  - **Description**: Path to the export scripts inside the container.  
  - **Usage**: Set in the wrapper to locate `common/functions.sh`, `common/regional_base.sh`, `common/cladistic.sh`, etc.

---

## Export Flow & Scripts

Below is the **script-by-script** overview:

1. **`wrapper.sh`**  
   - You define all environment variables needed for your particular export (see above).  
   - Sets `WRAPPER_PATH="$0"` so the pipeline can copy the wrapper into the output directory for reproducibility.  
   - Calls `main.sh`.

2. **`main.sh`**  
   - Validates environment variables.  
   - Creates the export directory (`EXPORT_SUBDIR`).  
   - **Optional**: If `SKIP_REGIONAL_BASE=true`, checks whether region-based tables already exist.  
   - Sources `regional_base.sh` if needed.  
   - Calls `cladistic.sh` to do the final clade-based filtering and CSV export.  
   - Gathers final stats (e.g., #observations, #taxa) from `<EXPORT_GROUP>_observations`.  
   - Writes a single summary file (`${EXPORT_GROUP}_export_summary.txt`).  
   - Optionally copies the wrapper script into the export folder.

3. **`regional_base.sh`**  
   - Sets bounding box coordinates for the region (`REGION_TAG`).  
   - Creates `<REGION_TAG>_min${MIN_OBS}_all_taxa`, selecting species with at least `MIN_OBS` research-grade obs.  
   - Creates `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs`, either global or region-limited based on `INCLUDE_OUT_OF_REGION_OBS`.

4. **`cladistic.sh`**  
   - Loads `clade_defns.sh` to interpret `CLADE`, `METACLADE`, or `MACROCLADE`.  
   - Joins `<REGION_TAG>_min${MIN_OBS}_all_taxa_obs` to `expanded_taxa` and filters for active taxa matching the clade condition. Applies `RG_FILTER_MODE` to the final table. Produces `<EXPORT_GROUP>_observations`.  
   - Exports a CSV with photo metadata, applying `PRIMARY_ONLY` and random sampling of up to `MAX_RN` observations per species.

----
Full Path: common/main.sh

#!/bin/bash
#
# main.sh
#
# Orchestrates the export pipeline by:
#  1) Validating environment variables
#  2) Optionally creating the regional base tables (unless SKIP_REGIONAL_BASE=true)
#  3) Calling cladistic.sh to produce <EXPORT_GROUP>_observations
#  4) Writing a unified export summary (environment variables + final stats)
#  5) Optionally copying the wrapper script for reproducibility
#
# CLARIFY: We assume no sensitive env vars need filtering. If you store credentials,
#          you may want to exclude them from the final summary.
#
# ASSUMPTION: The user always sets WRAPPER_PATH="$0" in their wrapper, so we
#             can copy the original wrapper script here.

source "${BASE_DIR}/common/functions.sh"

# Validate required variables
required_vars=(
    "DB_USER" "VERSION_VALUE" "RELEASE_VALUE" "ORIGIN_VALUE"
    "DB_NAME" "REGION_TAG" "MIN_OBS" "MAX_RN"
    "DB_CONTAINER" "HOST_EXPORT_BASE_PATH" "CONTAINER_EXPORT_BASE_PATH"
    "EXPORT_GROUP"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "Error: Required variable $var is not set"
        exit 1
    fi
done

print_progress "Creating export directory structure"
EXPORT_DIR="${CONTAINER_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"
HOST_EXPORT_DIR="${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

ensure_directory "${HOST_EXPORT_DIR}"

# Create PostgreSQL extension and role if needed
execute_sql "
DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS dblink;
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'exportuser') THEN
        CREATE ROLE exportuser;
    END IF;
END \$\$;"

# ------------------------------------------------------------------------------
# Timer Start
# ------------------------------------------------------------------------------
overall_start=$(date +%s)
regional_start=$(date +%s)

# If user wants to skip region creation, check if the table already exists
if [ "${SKIP_REGIONAL_BASE}" = "true" ]; then
    print_progress "SKIP_REGIONAL_BASE=true: Checking existing tables..."

    table_check=$(execute_sql "SELECT 1
      FROM pg_catalog.pg_tables
      WHERE schemaname = 'public'
        AND tablename = '${REGION_TAG}_min${MIN_OBS}_all_taxa_obs'
      LIMIT 1;")

    if [[ "$table_check" =~ "1" ]]; then
        print_progress "Table found, checking row count..."
        row_count=$(execute_sql "SELECT count(*) as cnt FROM \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\";")
        numeric=$(echo "$row_count" | awk '/[0-9]/{print $1}' | head -1)

        if [[ -n "$numeric" && "$numeric" -gt 0 ]]; then
            print_progress "Table has $numeric rows; skipping creation of regional base."
            send_notification "Skipped creating regional base; table is non-empty."
        else
            print_progress "Table is empty; re-creating..."
            source "${BASE_DIR}/common/regional_base.sh"
            send_notification "${REGION_TAG} regional base tables created"
        fi
    else
        print_progress "Table not found; re-creating the regional base."
        source "${BASE_DIR}/common/regional_base.sh"
        send_notification "${REGION_TAG} regional base tables created"
    fi
else
    print_progress "Creating regional base tables"
    source "${BASE_DIR}/common/regional_base.sh"
    send_notification "${REGION_TAG} regional base tables created"
fi

regional_end=$(date +%s)
regional_secs=$(( regional_end - regional_start ))
print_progress "Regional base step took ${regional_secs} seconds"

# ------------------------------------------------------------------------------
# Timer for cladistic step
# ------------------------------------------------------------------------------
cladistic_start=$(date +%s)

print_progress "Applying cladistic filters"
source "${BASE_DIR}/common/cladistic.sh"
send_notification "${EXPORT_GROUP} cladistic filtering complete"

cladistic_end=$(date +%s)
cladistic_secs=$(( cladistic_end - cladistic_start ))
print_progress "Cladistic filtering step took ${cladistic_secs} seconds"

# ------------------------------------------------------------------------------
# Single unified export summary
# ------------------------------------------------------------------------------
stats_start=$(date +%s)
print_progress "Creating unified export summary"

# 1) Gather final table stats from <EXPORT_GROUP>_observations
STATS=$(execute_sql "
WITH export_stats AS (
    SELECT 
        COUNT(DISTINCT observation_uuid) AS num_observations,
        COUNT(DISTINCT taxon_id) AS num_taxa,
        COUNT(DISTINCT observer_id) AS num_observers
    FROM \"${EXPORT_GROUP}_observations\"
)
SELECT format(
    'Observations: %s\nUnique Taxa: %s\nUnique Observers: %s',
    num_observations, num_taxa, num_observers
)
FROM export_stats;")

# 2) Write summary file
SUMMARY_FILE="${HOST_EXPORT_DIR}/${EXPORT_GROUP}_export_summary.txt"
{
  echo "Export Summary"
  echo "Version: ${VERSION_VALUE}"
  echo "Release: ${RELEASE_VALUE}"
  echo "Region: ${REGION_TAG}"
  echo "Minimum Observations (species): ${MIN_OBS}"
  echo "Maximum Random Number (MAX_RN): ${MAX_RN}"
  echo "Export Group: ${EXPORT_GROUP}"
  echo "Date: $(date)"
  echo "SKIP_REGIONAL_BASE: ${SKIP_REGIONAL_BASE}"
  echo "INCLUDE_OUT_OF_REGION_OBS: ${INCLUDE_OUT_OF_REGION_OBS}"
  echo "RG_FILTER_MODE: ${RG_FILTER_MODE}"
  echo ""
  echo "Final Table Stats:"
  echo "${STATS}"
  echo ""
  echo "Timing:"
  echo " - Regional Base: ${regional_secs} seconds"
  echo " - Cladistic: ${cladistic_secs} seconds"
  # We'll fill the final stats calc timing below
} > "${SUMMARY_FILE}"

stats_end=$(date +%s)
stats_secs=$(( stats_end - stats_start ))
print_progress "Stats/summary step took ${stats_secs} seconds"

# 3) Optionally copy the wrapper script for reproducibility
if [ -n "${WRAPPER_PATH}" ] && [ -f "${WRAPPER_PATH}" ]; then
    cp "${WRAPPER_PATH}" "${HOST_EXPORT_DIR}/"
fi

# ------------------------------------------------------------------------------
# Done
# ------------------------------------------------------------------------------
overall_end=$(date +%s)
overall_secs=$(( overall_end - overall_start ))
print_progress "Export process complete (total time: ${overall_secs} seconds)"

# We append final overall time to the summary
{
  echo " - Summary/Stats Step: ${stats_secs} seconds"
  echo " - Overall: ${overall_secs} seconds"
} >> "${SUMMARY_FILE}"

send_notification "Export for ${EXPORT_GROUP} is complete. Summary at ${SUMMARY_FILE}"

----
Full Path: common/regional_base.sh

#!/bin/bash
# --------------------------------------------------------------------------------
# regional_base.sh
# --------------------------------------------------------------------------------
# This script creates two tables for a given region (REGION_TAG) and MIN_OBS:
#   1) <REGION_TAG>_min${MIN_OBS}_all_taxa:
#      The set of species (rank_level=10) that have at least MIN_OBS
#      research-grade observations within the bounding box.
#
#   2) <REGION_TAG>_min${MIN_OBS}_all_taxa_obs:
#      All observations whose taxon_id is in (1). If INCLUDE_OUT_OF_REGION_OBS=true,
#      we do NOT re-apply the bounding-box filter, so out-of-region observations
#      are included. If false, we restrict again by bounding box.
#
# This is a partial step toward the "ancestor-aware" approach. For now, we only
# pick species that pass the threshold, ignoring higher or lower ranks.
#
# Usage:
#   - Sourced by main.sh
#   - Relies on environment variables:
#       REGION_TAG, MIN_OBS, INCLUDE_OUT_OF_REGION_OBS
#       DB_CONTAINER, DB_USER, DB_NAME, etc.
#
# CHANGES:
#   * Removed debug_counts table creation to streamline performance.
#   * No changes to final summary; that is now unified in main.sh.
# --------------------------------------------------------------------------------

# Function sets the bounding-box coords for $REGION_TAG
set_region_coordinates() {
  case "$REGION_TAG" in
    "NAfull")
      XMIN=-169.453125
      YMIN=12.211180
      XMAX=-23.554688
      YMAX=84.897147
      ;;
    "EURwest")
      XMIN=-12.128906
      YMIN=40.245992
      XMAX=12.480469
      YMAX=60.586967
      ;;
    "EURnorth")
      XMIN=-25.927734
      YMIN=54.673831
      XMAX=45.966797
      YMAX=71.357067
      ;;
    "EUReast")
      XMIN=10.722656
      YMIN=41.771312
      XMAX=39.550781
      YMAX=59.977005
      ;;
    "EURfull")
      XMIN=-30.761719
      YMIN=33.284620
      XMAX=43.593750
      YMAX=72.262310
      ;;
    "MED")
      XMIN=-16.259766
      YMIN=29.916852
      XMAX=36.474609
      YMAX=46.316584
      ;;
    "AUSfull")
      XMIN=111.269531
      YMIN=-47.989922
      XMAX=181.230469
      YMAX=-9.622414
      ;;
    "ASIAse")
      XMIN=82.441406
      YMIN=-11.523088
      XMAX=153.457031
      YMAX=28.613459
      ;;
    "ASIAeast")
      XMIN=462.304688
      YMIN=23.241346
      XMAX=550.195313
      YMAX=78.630006
      ;;
    "ASIAcentral")
      XMIN=408.515625
      YMIN=36.031332
      XMAX=467.753906
      YMAX=76.142958
      ;;
    "ASIAsouth")
      XMIN=420.468750
      YMIN=1.581830
      XMAX=455.097656
      YMAX=39.232253
      ;;
    "ASIAsw")
      XMIN=386.718750
      YMIN=12.897489
      XMAX=423.281250
      YMAX=48.922499
      ;;
    "ASIA_nw")
      XMIN=393.046875
      YMIN=46.800059
      XMAX=473.203125
      YMAX=81.621352
      ;;
    "SAfull")
      XMIN=271.230469
      YMIN=-57.040730
      XMAX=330.644531
      YMAX=15.114553
      ;;
    "AFRfull")
      XMIN=339.082031
      YMIN=-37.718590
      XMAX=421.699219
      YMAX=39.232253
      ;;
    *)
      echo "Unknown REGION_TAG: $REGION_TAG"
      exit 1
      ;;
  esac
}

# 1) Set region coordinates
set_region_coordinates

print_progress "Dropping existing tables"
execute_sql "
DROP TABLE IF EXISTS \"${REGION_TAG}_min${MIN_OBS}_all_taxa\" CASCADE;
DROP TABLE IF EXISTS \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\" CASCADE;
"

# --------------------------------------------------
# Removed debug_counts block:
#   print_progress "Creating debug_counts for region ${REGION_TAG}"
#   ...
# --------------------------------------------------

# 2) Create <REGION_TAG>_min${MIN_OBS}_all_taxa
#    We only include species (rank_level=10) that have >= MIN_OBS
#    research-grade obs in region.
print_progress "Creating table \"${REGION_TAG}_min${MIN_OBS}_all_taxa\""
execute_sql "
CREATE TABLE \"${REGION_TAG}_min${MIN_OBS}_all_taxa\" AS
SELECT s.taxon_id
FROM observations s
JOIN taxa t ON t.taxon_id = s.taxon_id
WHERE t.rank_level = 10
  AND s.quality_grade = 'research'
  AND s.geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326)
GROUP BY s.taxon_id
HAVING COUNT(s.observation_uuid) >= ${MIN_OBS};
"

# 3) Create <REGION_TAG>_min${MIN_OBS}_all_taxa_obs
#    If INCLUDE_OUT_OF_REGION_OBS=true, we do not filter again by bounding box.
#    Otherwise, we re-check s.geom against the region.
print_progress "Creating table \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\""
OBS_COLUMNS=$(get_obs_columns)
echo "Using columns: ${OBS_COLUMNS}"

if [ "${INCLUDE_OUT_OF_REGION_OBS}" = "true" ]; then
    execute_sql "
    CREATE TABLE \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\" AS
    SELECT ${OBS_COLUMNS}
    FROM observations
    WHERE taxon_id IN (
        SELECT taxon_id
        FROM \"${REGION_TAG}_min${MIN_OBS}_all_taxa\"
    );
    "
else
    execute_sql "
    CREATE TABLE \"${REGION_TAG}_min${MIN_OBS}_all_taxa_obs\" AS
    SELECT ${OBS_COLUMNS}
    FROM observations
    WHERE taxon_id IN (
        SELECT taxon_id
        FROM \"${REGION_TAG}_min${MIN_OBS}_all_taxa\"
    )
    AND geom && ST_MakeEnvelope(${XMIN}, ${YMIN}, ${XMAX}, ${YMAX}, 4326);
    "
fi

print_progress "Regional base tables created"

----
Full Path: common/functions.sh

#!/bin/bash

# Common functions used across export scripts

# Function to execute SQL commands
execute_sql() {
    local sql="$1"
    docker exec ${DB_CONTAINER} psql -U ${DB_USER} -d "${DB_NAME}" -c "$sql"
}

# Function to print progress
print_progress() {
    echo "======================================"
    echo "$1"
    echo "======================================"
}

get_obs_columns() {
    # Start with standard columns
    local cols="observation_uuid, observer_id, latitude, longitude, positional_accuracy, taxon_id, quality_grade, observed_on"
    
    # TEMPORARY HOTFIX: Commenting out version tracking columns until bulk update is complete
    # Add version tracking columns
    # cols="${cols}, origin, version, release"
    
    # Check if anomaly_score exists in this release
    if [[ "${RELEASE_VALUE}" == "r1" ]]; then
        cols="${cols}, anomaly_score"
    fi
    
    echo "$cols"
}

# Function to ensure directory exists with proper permissions
ensure_directory() {
    local dir="$1"
    mkdir -p "${dir}"
    chmod -R 777 "${dir}"
}

# Function to send ntfy notification
send_notification() {
    local message="$1"
    # Attempt curl with:
    # - max time of 5 seconds (-m 5)
    # - silent mode (-s)
    # - show errors but don't include in output (-S)
    # Redirect stderr to /dev/null to suppress error messages
    curl -m 5 -sS -d "$message" polliserve:8089/ibridaDB 2>/dev/null || true
}

# Export the functions
export -f execute_sql
export -f print_progress
export -f get_obs_columns
export -f ensure_directory
export -f send_notification

----
Full Path: common/clade_defns.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# clade_defns.sh
# ------------------------------------------------------------------------------
# This file defines the integer-based filtering expressions for macroclades,
# clades, and metaclades, referencing columns in "expanded_taxa".
#
# Usage:
#   source clade_defns.sh
#   Then pick a macroclade (MACROCLADE="..."), or a clade (CLADE="..."),
#   or a metaclade (METACLADE="...") in your environment, and the
#   cladistic.sh script will build a condition from one of the arrays below.
#
# Example:
#   MACROCLADES["arthropoda"]='("L60_taxonID" = 47119)'
#   CLADES["insecta"]='("L50_taxonID" = 47120)'
#   METACLADES["primary_terrestrial_arthropoda"]='("L50_taxonID" = 47120 OR "L50_taxonID" = 101885)'
#
# Be sure to substitute the correct taxonIDs for your local database!
# ------------------------------------------------------------------------------

# ---[ Macroclades ]-----------------------------------------------------------
# Typically for kingdom-level (L70) or phylum-level (L60) anchors.

declare -A MACROCLADES

# 1) Arthropoda => phylum at L60 = 47120
MACROCLADES["arthropoda"]='("L60_taxonID" = 47120)'

# 2) Chordata => phylum at L60 = 2
MACROCLADES["chordata"]='("L60_taxonID" = 2)'

# 3) Plantae => kingdom at L70 = 47126
MACROCLADES["plantae"]='("L70_taxonID" = 47126)'

# 4) Fungi => kingdom at L70 = 47170
MACROCLADES["fungi"]='("L70_taxonID" = 47170)'

# (Optional) If you consider Actinopterygii, Mammalia, Reptilia, etc.
# to be "macroclades," you may define them here instead of in CLADES.
# For instance:
#   MACROCLADES["mammalia"]='("L50_taxonID" = 40151)'


# ---[ Clades ]----------------------------------------------------------------
# Typically for class-level (L50), order-level (L40), or narrower taxonomic groups.
declare -A CLADES

# -- Plant Clades (Subphylum and Class levels) --
# -- Plant Subphylum (L57) --
CLADES["angiospermae"]='("L57_taxonID" = 47125)' # flowering plants

# -- Plant Classes (L50) --
CLADES["liliopsida"]='("L50_taxonID" = 47163)'    # monocots
CLADES["magnoliopsida"]='("L50_taxonID" = 47124)'  # dicots

# -- Class-level (L50) Examples --

# 1) Actinopterygii => L50 = 47178
CLADES["actinopterygii"]='("L50_taxonID" = 47178)'

# 2) Amphibia => L50 = 20978
CLADES["amphibia"]='("L50_taxonID" = 20978)'

# 3) Arachnida => L50 = 47119
CLADES["arachnida"]='("L50_taxonID" = 47119)'

# 4) Aves => L50 = 3
CLADES["aves"]='("L50_taxonID" = 3)'

# 5) Insecta => L50 = 47158
CLADES["insecta"]='("L50_taxonID" = 47158)'

# 6) Mammalia => L50 = 40151
CLADES["mammalia"]='("L50_taxonID" = 40151)'

# 7) Reptilia => L50 = 26036
CLADES["reptilia"]='("L50_taxonID" = 26036)'


# -- Order-level (L40) Examples --

# 1) Testudines => L40 = 39532
CLADES["testudines"]='("L40_taxonID" = 39532)'

# 2) Crocodylia => L40 = 26039
CLADES["crocodylia"]='("L40_taxonID" = 26039)'

# 3) Coleoptera => L40 = 47208
CLADES["coleoptera"]='("L40_taxonID" = 47208)'

# 4) Lepidoptera => L40 = 47157
CLADES["lepidoptera"]='("L40_taxonID" = 47157)'

# 5) Hymenoptera => L40 = 47201
CLADES["hymenoptera"]='("L40_taxonID" = 47201)'

# 6) Hemiptera => L40 = 47744
CLADES["hemiptera"]='("L40_taxonID" = 47744)'

# 7) Orthoptera => L40 = 47651
CLADES["orthoptera"]='("L40_taxonID" = 47651)'

# 8) Odonata => L40 = 47792
CLADES["odonata"]='("L40_taxonID" = 47792)'

# 9) Diptera => L40 = 47822
CLADES["diptera"]='("L40_taxonID" = 47822)'


# -- Additional Named Groups (Suborders, Clade Subsets, etc.) --

# Pterygota => The DB shows two taxonIDs (184884, 418641) plus
# another entry with L40_taxonID=48796. We combine them with OR:
CLADES["pterygota"]='("taxonID" = 184884 OR "taxonID" = 418641 OR "L40_taxonID" = 48796)'

# Phasmatodea => Not found in your query results. If/when you know its ID,
# you can fill it in here:
# CLADES["phasmatodea"]='("L40_taxonID" = ???)'

# Subclades within Hymenoptera (all share L40_taxonID=47201).
# Typically, referencing the top-level order is "hymenoptera"
# while these might be more specific anchor taxa:
CLADES["aculeata"]='("taxonID" = 326777)'
CLADES["apoidea"]='("taxonID" = 47222)'
CLADES["formicidae"]='("taxonID" = 47336)'
CLADES["vespoidea"]='("taxonID" = 48740)'
CLADES["vespidae"]='("taxonID" = 52747)'


# ---[ Metaclades ]------------------------------------------------------------
# Multi-root or cross-macroclade definitions. Compose bigger groups using OR/AND.

declare -A METACLADES

# Example 1: terrestrial_arthropods => Insecta OR Arachnida OR others.
# (Using the taxonIDs from the CLADES above.)
METACLADES["terrestrial_arthropods"]='("L50_taxonID" = 47158 OR "L50_taxonID" = 47119)'

# Example 2: flying_vertebrates => Birds (aves) OR Bats (chiroptera)
# Suppose chiroptera => L40=7721 (if that’s valid in your DB).
METACLADES["flying_vertebrates"]='("L50_taxonID" = 3 OR "L40_taxonID" = 7721)'

# Example 3: nonavian_reptiles => reptilia minus birds. You might do:
# METACLADES["nonavian_reptiles"]='("L50_taxonID" = 26036 AND "L50_taxonID" != 3)'


# ---[ Helper Function ]-------------------------------------------------------
# Picks the correct expression given environment variables.
function get_clade_condition() {
  local condition

  # 1) If METACLADE is set (and found in METACLADES), return that
  if [[ -n "${METACLADE}" && -n "${METACLADES[${METACLADE}]}" ]]; then
    condition="${METACLADES[${METACLADE}]}"
    echo "${condition}"
    return
  fi

  # 2) Else if CLADE is set
  if [[ -n "${CLADE}" && -n "${CLADES[${CLADE}]}" ]]; then
    condition="${CLADES[${CLADE}]}"
    echo "${condition}"
    return
  fi

  # 3) Else if MACROCLADE is set
  if [[ -n "${MACROCLADE}" && -n "${MACROCLADES[${MACROCLADE}]}" ]]; then
    condition="${MACROCLADES[${MACROCLADE}]}"
    echo "${condition}"
    return
  fi

  # 4) Fallback: no recognized key => no filter
  echo "TRUE"
}

export -f get_clade_condition


----
Full Path: common/cladistic.sh

#!/bin/bash
# ------------------------------------------------------------------------------
# cladistic.sh
# ------------------------------------------------------------------------------
# Creates filtered observation subsets for a specified clade/metaclade,
# referencing the "expanded_taxa" table instead of the old recursive string method.
#
# Usage pattern:
#   1) Environment variables set by an upstream wrapper or main script:
#      - METACLADE=...
#      - CLADE=...
#      - MACROCLADE=...
#      - DB_CONTAINER, DB_USER, DB_NAME
#      - REGION_TAG, MIN_OBS, ...
#      - MAX_RN (the max number of observations to keep per species)
#      - PRIMARY_ONLY (whether to restrict photos to position=0)
#      - RG_FILTER_MODE (how to handle research-grade vs. non-research)
#   2) We load "clade_defns.sh" to define which integer columns to match
#      (e.g. "L50_taxonID" = 47120).
#   3) We create a final table <group>_observations by joining the region-based
#      table <REGION_TAG>_min${MIN_OBS}_all_taxa_obs to expanded_taxa, applying:
#       - e."taxonActive"=TRUE
#       - clade condition from get_clade_condition()
#       - RG_FILTER_MODE-based filtering or transformations
#   4) We then export to CSV. Our new expansions handle:
#       - a union approach for the final photo export:
#         * subquery #1: "research species" capped at MAX_RN
#         * subquery #2: "everything else" with no limit
#   5) The environment variable "EXPORT_GROUP" is used to name the final table
#      and output CSV (e.g. "primary_terrestrial_arthropoda").
#
# NOTE: The final summary stats are handled by main.sh. We only create the
# <EXPORT_GROUP>_observations table and do the CSV export here.
#
# If you need partial stats or debugging, you can log them here, but do not
# overwrite export_summary.txt. main.sh unifies everything at the end.
#
# ------------------------------------------------------------------------------
# RG_FILTER_MODE Implementation:
#   * ONLY_RESEARCH
#       WHERE quality_grade='research'
#   * ALL
#       no additional filter
#   * ALL_EXCLUDE_SPECIES_NON_RESEARCH
#       exclude non-research if rank=species => 
#         "NOT(quality_grade!='research' AND L10_taxonID IS NOT NULL)"
#   * ONLY_NONRESEARCH
#       "quality_grade!='research'"
#   * ONLY_NONRESEARCH_EXCLUDE_SPECIES
#       "quality_grade!='research' AND L10_taxonID IS NULL"
#   * ONLY_NONRESEARCH_WIPE_SPECIES_LABEL
#       "quality_grade!='research'", but set L10_taxonID=NULL for all those rows
# ------------------------------------------------------------------------------
# Photo Export (Approach B):
#   - We do 2 subqueries and UNION ALL:
#     1) "capped_research_species": research-grade + species-level => partition by L10_taxonID
#        limit MAX_RN
#     2) "everything_else": no sampling limit
#   - If PRIMARY_ONLY=true, we add p.position=0 in both subqueries
#   - The final table <EXPORT_GROUP>_observations already reflects RG_FILTER_MODE,
#     so if it doesn't contain certain rows (e.g. only nonresearch), subquery #1
#     might be empty. That is expected.
#
# Permission / Ownership Note:
#   If you see "Operation not permitted" when chmod-ing existing CSV files, it
#   usually means the container user (UID 998) cannot change permissions on a file
#   owned by another user. If the directory is world-writable and the file is not
#   locked down, Postgres can still write CSV data. If you see "permission denied",
#   confirm directory/file ownership for the container user.
# ------------------------------------------------------------------------------

source "${BASE_DIR}/common/functions.sh"
source "${BASE_DIR}/common/clade_defns.sh"

CLADE_CONDITION="$(get_clade_condition)"
print_progress "Creating filtered tables for ${EXPORT_GROUP}"

TABLE_NAME="${EXPORT_GROUP}_observations"
REGIONAL_TABLE="${REGION_TAG}_min${MIN_OBS}_all_taxa_obs"
OBS_COLUMNS="$(get_obs_columns)"

# Drop old table if it exists
execute_sql "
DROP TABLE IF EXISTS \"${TABLE_NAME}\" CASCADE;
"

send_notification "Joining regional table ${REGIONAL_TABLE} to expanded_taxa"
print_progress "Joining regional table ${REGIONAL_TABLE} to expanded_taxa"

# ------------------------------------------------------------------------------
# 1) Build the WHERE clause and/or column transformations based on RG_FILTER_MODE
# ------------------------------------------------------------------------------
rg_where_condition="TRUE"
rg_l10_col="e.\"L10_taxonID\""  # default is the original L10_taxonID

case "${RG_FILTER_MODE}" in
  "ONLY_RESEARCH")
    rg_where_condition="o.quality_grade='research'"
    ;;
  "ALL")
    rg_where_condition="TRUE"
    ;;
  "ALL_EXCLUDE_SPECIES_NON_RESEARCH")
    rg_where_condition="NOT (o.quality_grade!='research' AND e.\"L10_taxonID\" IS NOT NULL)"
    ;;
  "ONLY_NONRESEARCH")
    rg_where_condition="o.quality_grade!='research'"
    ;;
  "ONLY_NONRESEARCH_EXCLUDE_SPECIES")
    rg_where_condition="(o.quality_grade!='research' AND e.\"L10_taxonID\" IS NULL)"
    ;;
  "ONLY_NONRESEARCH_WIPE_SPECIES_LABEL")
    rg_where_condition="o.quality_grade!='research'"
    rg_l10_col="NULL::integer" 
    # ASSUMPTION: In your DB, L10_taxonID is an integer type. If not, switch to ::bigint
    ;;
  *)
    # Unrecognized => default to ALL
    rg_where_condition="TRUE"
    ;;
esac

# ------------------------------------------------------------------------------
# 2) Create <EXPORT_GROUP>_observations with a SELECT that applies the above logic
# ------------------------------------------------------------------------------
execute_sql "
CREATE TABLE \"${TABLE_NAME}\" AS
SELECT
    ${OBS_COLUMNS},
    e.\"taxonID\"         AS expanded_taxonID,
    e.\"rankLevel\"       AS expanded_rankLevel,
    e.\"name\"            AS expanded_name,

    e.\"L5_taxonID\"      AS \"L5_taxonID\",
    ${rg_l10_col}         AS \"L10_taxonID\",
    e.\"L11_taxonID\",
    e.\"L12_taxonID\",
    e.\"L13_taxonID\",
    e.\"L15_taxonID\",
    e.\"L20_taxonID\",
    e.\"L24_taxonID\",
    e.\"L25_taxonID\",
    e.\"L26_taxonID\",
    e.\"L27_taxonID\",
    e.\"L30_taxonID\",
    e.\"L32_taxonID\",
    e.\"L33_taxonID\",
    e.\"L33_5_taxonID\",
    e.\"L34_taxonID\",
    e.\"L34_5_taxonID\",
    e.\"L35_taxonID\",
    e.\"L37_taxonID\",
    e.\"L40_taxonID\",
    e.\"L43_taxonID\",
    e.\"L44_taxonID\",
    e.\"L45_taxonID\",
    e.\"L47_taxonID\",
    e.\"L50_taxonID\",
    e.\"L53_taxonID\",
    e.\"L57_taxonID\",
    e.\"L60_taxonID\",
    e.\"L67_taxonID\",
    e.\"L70_taxonID\"

FROM \"${REGIONAL_TABLE}\" o
JOIN \"expanded_taxa\" e
   ON e.\"taxonID\" = o.taxon_id

WHERE e.\"taxonActive\" = TRUE
  AND ${CLADE_CONDITION}
  AND (${rg_where_condition});
"

# ------------------------------------------------------------------------------
# 3) Export to CSV with Approach B: union of 2 subqueries
# ------------------------------------------------------------------------------
send_notification "Exporting filtered observations"
print_progress "Exporting filtered observations"

# We'll define a small snippet for the (p.position=0) if PRIMARY_ONLY
pos_condition="TRUE"
if [ "${PRIMARY_ONLY}" = true ]; then
    pos_condition="p.position=0"
fi

# subquery1: "capped_research_species"
#   - Condition: research-grade + L10_taxonID not null
#   - row_number partition by L10_taxonID
#   - we keep row_number<=MAX_RN
# subquery2: "everything_else"
#   - no row_number limit
#   - condition: NOT(research & species)
execute_sql "
COPY (
  WITH
  capped_research_species AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position,
      ROW_NUMBER() OVER (
        PARTITION BY o.\"L10_taxonID\"
        ORDER BY random()
      ) AS rn
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE
      ${pos_condition}
      AND o.quality_grade='research'
      AND o.\"L10_taxonID\" IS NOT NULL
  ),
  everything_else AS (
    SELECT
      o.*,
      p.photo_uuid,
      p.photo_id,
      p.extension,
      p.license,
      p.width,
      p.height,
      p.position
      -- no row_number needed
    FROM \"${TABLE_NAME}\" o
    JOIN photos p ON o.observation_uuid = p.observation_uuid
    WHERE
      ${pos_condition}
      AND NOT (o.quality_grade='research' AND o.\"L10_taxonID\" IS NOT NULL)
  )
  SELECT * FROM capped_research_species
  WHERE rn <= ${MAX_RN}
  UNION ALL
  SELECT * FROM everything_else
) TO '${EXPORT_DIR}/${EXPORT_GROUP}_photos.csv'
WITH (FORMAT CSV, HEADER, DELIMITER E'\t');
"

print_progress "Cladistic filtering complete"

----
Full Path: r1/wrapper.sh

#!/bin/bash
#
# wrapper.sh
#
# A typical user-facing script that sets environment variables and then calls main.sh.
# We define WRAPPER_PATH="$0" so that main.sh can copy this file for reproducibility.

SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LOG_FILE="${SCRIPT_DIR}/$(basename "$0" .sh)_$(date +%Y%m%d_%H%M%S).log"

echo "Starting new run at $(date)" > "${LOG_FILE}"

# Function to log messages to both console and file
log_message() {
    echo "$1" | tee -a "${LOG_FILE}"
}

# Redirect all stdout and stderr to both console and log file
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}")

log_message "Initializing export process with configuration:"

# Provide path to wrapper for reproducibility
export WRAPPER_PATH="$0"

# ---------------------------------------------------------------------------
# Database config
# ---------------------------------------------------------------------------
export DB_USER="postgres"
export VERSION_VALUE="v0"
export RELEASE_VALUE="r1"
export ORIGIN_VALUE="iNat-Dec2024"
export DB_NAME="ibrida-${VERSION_VALUE}-${RELEASE_VALUE}"

log_message "Database: ${DB_NAME}"
log_message "Version: ${VERSION_VALUE}"
log_message "Release: ${RELEASE_VALUE}"

# ---------------------------------------------------------------------------
# Export parameters
# ---------------------------------------------------------------------------
export REGION_TAG="NAfull"
export MIN_OBS=100
export MAX_RN=200
export PRIMARY_ONLY=true

# We could set CLADE or METACLADE here; let's pick something as an example:
export CLADE="amphibia"
export EXPORT_GROUP="${CLADE}"

# Additional flags
export PROCESS_OTHER=false
export SKIP_REGIONAL_BASE=false

# NEW ENV VARS
export INCLUDE_OUT_OF_REGION_OBS=true
export RG_FILTER_MODE="ALL"

log_message "Region: ${REGION_TAG}"
log_message "Min Observations: ${MIN_OBS}"
log_message "Max Random Number: ${MAX_RN}"
log_message "Export Group: ${EXPORT_GROUP}"
log_message "Skip Regional Base Creation: ${SKIP_REGIONAL_BASE}"
log_message "Include Out-of-Region Obs: ${INCLUDE_OUT_OF_REGION_OBS}"
log_message "RG Filter Mode: ${RG_FILTER_MODE}"

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
export DB_CONTAINER="ibridaDB"
export HOST_EXPORT_BASE_PATH="/datasets/ibrida-data/exports"
export CONTAINER_EXPORT_BASE_PATH="/exports"
export EXPORT_SUBDIR="${VERSION_VALUE}/${RELEASE_VALUE}/primary_only_${MIN_OBS}min_${MAX_RN}max"
export BASE_DIR="/home/caleb/repo/ibridaDB/dbTools/export/v0"

log_message "Export Directory: ${HOST_EXPORT_BASE_PATH}/${EXPORT_SUBDIR}"

# Source common functions
source "${BASE_DIR}/common/functions.sh"

# Execute main script
send_notification "Starting ${EXPORT_GROUP} export"
log_message "Executing main script at $(date)"

"${BASE_DIR}/common/main.sh"

log_message "Process completed at $(date)"
send_notification "${EXPORT_GROUP} export completed!"

----
Full Path: ../../../docker/stausee/docker-compose.yml

services:
  ibrida:
    image: postgis/postgis:15-3.3
    user: "998:998"
    shm_size: '16g'
    environment:
      POSTGRES_PASSWORD: ooglyboogly69
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_SHARED_BUFFERS: 8GB
      POSTGRES_WORK_MEM: 2048MB
      POSTGRES_MAINTENANCE_WORK_MEM: 4GB
    volumes:
      - ~/repo/ibridaDB/dbTools:/tool
      - ~/repo/ibridaDB/dbQueries:/query
      - /database/ibridaDB:/var/lib/postgresql/data
      - /datasets/ibrida-data/exports:/exports
      - /datasets/ibrida-data/intake:/metadata
    ports:
      - "5432:5432"
    container_name: ibridaDB

----
Full Path: ../../../docker/stausee/entrypoint.sh

#!/bin/bash
set -e

# Just log and exit - let Docker's default entrypoint handle PostgreSQL
echo "Entrypoint script executed at $(date)"

